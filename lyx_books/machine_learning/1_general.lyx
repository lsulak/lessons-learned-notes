#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrbook
\begin_preamble
% DO NOT ALTER THIS PREAMBLE!!!
%
% This preamble is designed to ensure that the manual prints
% out as advertised. If you mess with this preamble,
% parts of the manual may not print out as expected.  If you
% have problems LaTeXing this file, please contact 
% the documentation team
% email: lyx-docs@lists.lyx.org

% the pages of the TOC are numbered roman
% and a PDF-bookmark for the TOC is added

\pagenumbering{roman}
\let\myTOC\tableofcontents
\renewcommand{\tableofcontents}{%
 \pdfbookmark[1]{\contentsname}{}
 \myTOC

 \pagenumbering{arabic}}

% extra space for tables
\newcommand{\extratablespace}[1]{\noalign{\vskip#1}}
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
\end_preamble
\options bibliography=totoc,index=totoc,BCOR7.5mm,titlepage,captions=tableheading
\use_default_options false
\begin_modules
logicalmkup
theorems-ams
theorems-ams-extended
multicol
shapepar
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "lmodern" "default"
\font_sans "lmss" "default"
\font_typewriter "lmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement h
\paperfontsize 12
\spacing single
\use_hyperref true
\pdf_title "Machine Learning Notes"
\pdf_author "Ladislav Sulak"
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle false
\pdf_quoted_options "linkcolor=black, citecolor=black, urlcolor=blue, filecolor=blue, pdfpagelayout=OneColumn, pdfnewwindow=true, pdfstartview=XYZ, plainpages=false"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\notefontcolor #0000ff
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 1
\tocdepth 1
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 1
\math_indentation default
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle headings
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict true
\end_header

\begin_body

\begin_layout Chapter
General
\end_layout

\begin_layout Section
The Field of AI, CI, and Machine Learning
\end_layout

\begin_layout Subsection
Intelligence
\end_layout

\begin_layout Itemize
Intelligence is the computational part of the ability to achieve goals in
 the world.
 Varying kinds and degrees of intelligence occur in people, many animals
 and some machines.
\end_layout

\begin_layout Itemize
Webster’s New Collegiate Dictionary defines intelligence as:
\end_layout

\begin_deeper
\begin_layout Itemize
The ability to learn or understand or to deal with new or trying situations
 - reason.
 Also, the skilled use of reason.
\end_layout

\begin_layout Itemize
The ability to apply knowledge to manipulate one's environment or to think
 abstractly as measured by objective criteria (as tests).
\end_layout

\end_deeper
\begin_layout Itemize
Davig Fogel's definition:
\end_layout

\begin_deeper
\begin_layout Itemize
The capability of a system to adapt its behavior (to implement decisions)
 to meet its goals in a range of environments.
 It is a property of all purpose-driven decision makers.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Artificial Intelligence
\end_layout

\begin_layout Itemize
John McCarthy (a co-founder of the field)
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{http://www-formal.stanford.edu/jmc/whatisai/whatisai.html}}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
It is the science and engineering of 
\series bold
making intelligent machines
\series default
, especially intelligent computer programs.
 It is related to the similar task of using computers to understand human
 intelligence, but AI does not have to confine itself to methods that are
 biologically observable.
 (however it can be also a hundreds of thousand lines of conditions IF)
\end_layout

\end_deeper
\begin_layout Itemize
TL;DR; 
\series bold
AI is intelligence demonstrated by machines
\series default

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://www.iep.utm.edu/art-inte/}}
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
AI is defined as the study of the computation required for intelligent behavior
 (mostly humans, but also animals and so on) and the attempt to duplicate
 such computation using computers.
 
\end_layout

\begin_layout Itemize
AI is a superset of CI and ML.
 AI is a very broad term, and some subsets (applications) can be interrelated
 together, for example ANN are CI and also ML.
 In literature, various definitions and terms in the field of AI are still
 not used homogeneously
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://blog.leanix.net/en/what-is-the-difference-between-artificial-intellige
nce-and-machine-learning}}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Itemize
Branches of AI (not full list, because some were not yet identified): logical
 AI, search, 
\series bold
pattern recognition
\series default
, representation, inference, common sense knowledge and reasoning, learning
 from experience, planning, epistemology, ontology, heuristics, genetic
 programming.
 
\end_layout

\begin_layout Itemize
Applications of AI (some of them): game playing, speech recognition, understandi
ng natural language, computer vision, expert systems, heuristic classification.
\end_layout

\begin_layout Itemize
Anything typical human can do in less than 1 second of thought, we can possibly
 now or soon automate with AI
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://www.youtube.com/watch?v=21EiKfQYZXc}}
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
AI can be divided into two branches:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
ANI
\series default
 (Artificial Narrow Intelligence) - these have one purpose, such as smart
 reader, self-driving car, web search, and so on.
\end_layout

\begin_layout Itemize

\series bold
AGI
\series default
 (Artificial General Intelligence) - this concept is about building AI that
 can do whatever a human can do, or maybe even more things.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\series bold
Machine Learning
\end_layout

\begin_layout Itemize
Arthur Samuel (1959).
 
\end_layout

\begin_deeper
\begin_layout Itemize
Field of study that gives computer the ability to learn without being explicitly
 programmed.
\end_layout

\end_deeper
\begin_layout Itemize
Tom Mitchell (1998).
 
\end_layout

\begin_deeper
\begin_layout Itemize
A computer program is said to 
\bar under
learn
\bar default
 from experience E with respect to some task T and some performance measure
 P, if its 
\series bold
performance
\series default
 on T, as measured by P, 
\series bold
improves
\series default
 
\series bold
with
\series default
 
\series bold
experience
\series default
 E.
\end_layout

\begin_deeper
\begin_layout Standard
Example: playing checkers.
\end_layout

\begin_layout Itemize
E = the experience of playing many games of checkers
\end_layout

\begin_layout Itemize
T = the task of playing checkers.
\end_layout

\begin_layout Itemize
P = the probability that the program will win the next game.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Evolved from
\series default
 the study of 
\series bold
pattern recognition
\series default
 and 
\series bold
computational learning theory
\series default
 in 
\series bold
AI
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Data-driven predictions or decisions
\series default
 = building a model from sample input.
 Machine learning systems automatically learn programs from data.
\end_layout

\begin_layout Itemize
A dumb algorithm with lots and lots of data beats a clever one with modest
 amounts of it.
 After all, machine learning is all about letting data do the heavy lifting.
\end_layout

\begin_layout Itemize

\series bold
Machine learning tasks by learning style
\end_layout

\begin_deeper
\begin_layout Itemize

\bar under
Supervised learning
\bar default
 - in every example of dataset we have a correct answer about the example.
 In other words, we are given a dataset and already know what our correct
 output should look like, having the idea that there is a relationship between
 the input and the output.
 These problems can be generalized into:
\end_layout

\begin_deeper
\begin_layout Itemize
Semi-supervised learning: the computer is given only an incomplete training
 signal: a training set with some (often most) of the target outputs are
 missing.
 The goal is the same as supervised learning algorithm.
 By adding more examples you add more information about your problem - larger
 sample reflects better the probability distribution the data we labeled
 came from.
\end_layout

\begin_layout Itemize
Active learning: special case of semi-supervised learning.
 Instead of assuming that all of the training examples are given at the
 start, active learning algorithms interactively collect new examples, typically
 by making queries to a human user (or some other information source).
 Often, the queries are based on unlabeled data, which is a scenario that
 combines semi-supervised learning with active learning.
 
\end_layout

\begin_deeper
\begin_layout Standard
The computer can only obtain training labels for a limited set of instances
 (based on a budget), and also has to optimize its choice of objects to
 acquire labels for.
 When used interactively, these can be presented to the user for labeling.
 
\end_layout

\end_deeper
\begin_layout Itemize
Structured prediction: when the desired output value is a complex object,
 such as a parse tree or a labeled graph, then standard methods must be
 extended.
 It is an umbrella term for supervised machine learning techniques that
 involves predicting structured objects, rather than scalar discrete or
 real values.
\end_layout

\begin_layout Itemize
Learning to rank: when the input is a set of objects and the desired output
 is a ranking of those objects, then again the standard methods must be
 extended.
 It is a supervised learning problem.
\end_layout

\end_deeper
\begin_layout Itemize

\bar under
Unsupervised learning
\bar default
 - find a structure from data itself where we do not know the effect of
 the variables.
 In other words, this allows us to approach problems with little or no idea
 what our results should look like.
 No labels are given to the learning algorithm.
 For example clustering algorithms deals with this.
 Or dimensionality reduction algorithms, or outlier detection (here, the
 output is a real number that indicates how 
\begin_inset Formula $x$
\end_inset

 is different from a 
\begin_inset Quotes eld
\end_inset

typical
\begin_inset Quotes erd
\end_inset

 example in the dataset).
\end_layout

\begin_layout Itemize

\bar under
Reinforcement learning
\bar default

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-lea
rning-implementation/}}
\end_layout

\end_inset

 - training data (in form of rewards and punishments) is given only as a
 feedback to the program's actions in a dynamic environment, such as playing
 a game against an opponent.
 The machine 
\begin_inset Quotes eld
\end_inset

lives
\begin_inset Quotes erd
\end_inset

 in an environment and is capable of perceiving the state of that environment
 as a vector of features.
 The machine can execute actions in every state, and different actions bring
 different rewards and could also move the machine to another state of the
 environment.
 The goal of reinforcement learning algorithm is to learn a policy (this
 is a function - similar to a model in supervised learning - that takes
 the feature vector of a state as input and outputs an optimal action to
 execute in that state).
 The action is optimal if it maximizes the 
\series bold
expected average reward
\series default
.
 Reinforcement learning solves a particular kind of problem where decision
 making is sequential and the goal is long-term.
\end_layout

\begin_deeper
\begin_layout Itemize
Here belong algorithm such as Monte Carlo, Q-learning, or SARSA
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://en.wikipedia.org/wiki/Reinforcement_learning}}
\end_layout

\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Another way of
\series bold
 categorizing machine learning tasks
\series default
 is when 
\series bold
we
\series default
 
\series bold
consider
\series default
 the 
\series bold
similarities in their function
\series default
.
 Chapters in this document basically group machine learning algorithms based
 on how they work.
 However it is not perfect either because some algorithms can be in multiple
 categories.
 This was inspired from machinelearningmastery
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/}}
\end_layout

\end_inset

 and Scikit-Learn API reference
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{http://scikit-learn.org/stable/modules/classes.html
\backslash
#api-reference}}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Computational Intelligence
\end_layout

\begin_layout Itemize
Two key concepts and requirements -
\series bold
 adaptation and self-organization
\series default
.
 Algorithms and implementations that enable or facilitate appropriate 
\series bold
actions
\series default
 (intelligent behavior) 
\series bold
in
\series default
 
\series bold
complex and changing environments
\series default
.
\end_layout

\begin_layout Itemize
CI usually refers to the ability of a computer to learn a specific task
 from data or experimental observation.
 Even though it is commonly considered a synonym of 
\series bold
soft computing
\series default
, there is still no commonly accepted definition of computational intelligence
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://en.wikipedia.org/wiki/Computational_intelligence}}.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Generally, CI is a set of nature-inspired computational methodologies and
 approaches to address complex real-world problems.
\end_layout

\begin_layout Itemize
Although Artificial Intelligence and Computational Intelligence seek a similar
 long-term goal: reach general intelligence, which is the intelligence of
 a machine that could perform any intellectual task that a human being can;
 there's a clear difference between them.
 According to Bezdek (1994), 
\series bold
Computational Intelligence is a subset of Artificial Intelligence
\series default
.
\end_layout

\begin_layout Itemize
Main principles:
\end_layout

\begin_deeper
\begin_layout Itemize
Fuzzy logic (uncertainty like in human experiences - thus reflected as the
 behavioral level of the organism; fuzziness is not resolved by observation
 or measurement
\end_layout

\begin_layout Itemize
Neural Networks (so they belong to CI and ML)
\end_layout

\begin_layout Itemize
Evolutionary computation (includes genetic algorithms, genetic programming,
 evolutionary programming, evolution strategies) 
\end_layout

\begin_layout Itemize
Learning theory
\end_layout

\begin_layout Itemize
Probabilistic methods
\end_layout

\end_deeper
\begin_layout Subsection
Soft Computing
\end_layout

\begin_layout Itemize
Same as CI, expanded with Probabilistic Reasoning, Swarm Intelligence, and
 partly Chaos Theory.
\end_layout

\begin_layout Itemize
Soft Computing is thus tolerant of imprecision, uncertainty and partial
 truth.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Knowledge Discovery in Databases
\end_layout

\begin_layout Itemize
KDD is a process of discovering interesting patterns and knowledge from
 large amounts of data.
\end_layout

\begin_layout Itemize
KDD involves the following process stages (iterative steps):
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://www.smartdatacollective.com/difference-between-knowledge-discovery-and
-data-mining/}}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Development and understanding of the application domain.
\end_layout

\begin_layout Enumerate
Creating a target data set.
\end_layout

\begin_layout Enumerate
Data cleaning and preprocessing.
\end_layout

\begin_layout Enumerate
Data reduction and projection.
\end_layout

\begin_layout Enumerate
Matching process objectives (e.g.
 summarization, classification, regression, clustering).
\end_layout

\begin_layout Enumerate
Modeling and exploratory analysis and hypothesis selection: choosing the
 algorithms or data mining, and select the method or methods to be used
 in the search for patterns of data.
\end_layout

\begin_layout Enumerate
Data Mining: the search for patterns of interest in a particular representationa
l form or a set of these representations, including classification rules
 or trees, regression and clustering.
 It can be descriptive (=unsupervised learning) or predictive (=supervised
 learning).
\end_layout

\begin_layout Enumerate
Pattern evaluation - interpreting mined patterns.
\end_layout

\begin_layout Enumerate
Acting on the discovered knowledge: using the knowledge directly, incorporating
 the knowledge in another system for further action, or simply document
 the results.
 It is knowledge presentation - post-processing in a way that is easily
 understandable by users, usually using visualizations.
\end_layout

\end_deeper
\begin_layout Subsection
Data Mining
\end_layout

\begin_layout Itemize
Process of discovering patterns in large datasets involving methods at the
 
\series bold
intersection of machine learning, statistics, 
\series default
and
\series bold
 database systems
\series default
.
\end_layout

\begin_layout Itemize
Data mining is just one process (raw analysis) of more ones in the overall
 
\series bold
KDD
\series default
 (knowledge discovery in databases) - however, a lot of people thinks that
 Data Mining = KDD.
\end_layout

\begin_layout Subsection
Data Analysis Process
\end_layout

\begin_layout Itemize
Process of ordering and organizing 
\series bold
raw 
\series default
data in order to determine useful insights and decisions.
\end_layout

\begin_layout Itemize
This is a superset of Data Mining that involves extracting, cleaning, transformi
ng, modeling and visualization of data with an intention to uncover meaningful
 and useful information that can help in deriving conclusion and take decisions.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://www.educba.com/data-mining-vs-data-analysis/}}
\end_layout

\end_inset

 It requires the intersection of 
\series bold
computer science, machine learning, statistics
\series default
, and 
\series bold
mathematics
\series default
.
\end_layout

\begin_layout Itemize
The process has 5 steps:
\end_layout

\begin_deeper
\begin_layout Enumerate
Define Your Questions
\end_layout

\begin_layout Enumerate
Set Clear Measurement Priorities (what and how to measure)
\end_layout

\begin_layout Enumerate
Collect Data
\end_layout

\begin_layout Enumerate
Analyze Data
\end_layout

\begin_layout Enumerate
Interpret Results
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Survey of major AI application areas
\end_layout

\begin_layout Itemize

\series bold
Computer Vision
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Image classification
\series default
 / 
\series bold
Object recognition
\series default
 (here belongs 
\series bold
Face recognition
\series default
 for instance) - yes or no answers, if 2 images shows the same person for
 example.
\end_layout

\begin_layout Itemize

\series bold
Object detection 
\series default
- positions of found objects.
 For example position (in rectangle box) of a pedestrian or a car.
\end_layout

\begin_layout Itemize

\series bold
Image segmentation
\series default
 - each pixel is classified into a class.
 It tells us, if a given pixel is a part of a car or pedestrian and so on.
 There is an excellent dataset named COCO (from 2018, but may be updated)
 
\begin_inset CommandInset href
LatexCommand href
name "here"
target "http://cocodataset.org/#home"
literal "false"

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{COCO is a large-scale object detection, segmentation, and captioning
 dataset.}
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Tracking
\series default
 - also tracks for example people in a video.
 Basically a object recognition on a video.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Natural Language Processing
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Text classification
\series default
 - given an input email, is it spam or not? Or for example, classify from
 a given product description, which product category does it belongs to.
 Here belongs also 
\series bold
Sentiment classification
\series default
 (automatically rate a review of some product).
\end_layout

\begin_layout Itemize

\series bold
Information retrieval 
\series default
- e.g.
 web search.
\end_layout

\begin_layout Itemize

\series bold
Name entity recognition
\series default
 - find people's name in a sentences, automatic extraction of names of companies
, phones numbers, and so on.
\end_layout

\begin_layout Itemize

\series bold
Machine translation
\series default
 - translation of text from one language into another language.
\end_layout

\begin_layout Itemize

\series bold
Part-of-speech
\series default
 - given an input sentence, which words are verbs, which are nouns, and
 so on.
 This can be important for building sentiment classification system.
 We can thus help AI system to figure out which of the words to pay more
 attention to.
\end_layout

\begin_layout Itemize

\series bold
Parsing
\series default
 - helping a group the words together to make a phrase.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Speech
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Speech recognition 
\series default
- is speech-to-text translation.
\end_layout

\begin_layout Itemize

\series bold
Trigger word/wakeword detection
\end_layout

\begin_layout Itemize

\series bold
Speaker ID 
\series default
- get a identity of a speaker.
\end_layout

\begin_layout Itemize

\series bold
Speech synthesis 
\series default
- speech-to-text, also known as TTS.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Robotics
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Perception
\series default
 - figure out what's in he world around you based on the senses you have
 (cameras, radar, sensors, ...).
\end_layout

\begin_layout Itemize

\series bold
Motion planning
\series default
 - finding a path for the robot to follow.
\end_layout

\begin_layout Itemize

\series bold
Control
\series default
 - sending commands to motors to follow a path.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
General machine learning
\end_layout

\begin_deeper
\begin_layout Itemize
Unstructured data - images, text, audio, ...
\end_layout

\begin_layout Itemize
Structured data - tables, structures, ...
\end_layout

\end_deeper
\begin_layout Subsection
ML-related Job Positions
\end_layout

\begin_layout Itemize

\series bold
Software Engineer
\end_layout

\begin_deeper
\begin_layout Itemize
Writes specialized software for executing a functionality based/with some
 learned model; making software reliable.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Machine Learning Engineer
\end_layout

\begin_deeper
\begin_layout Itemize
Responsible for learning ML models or for building other ML algorithms.
\end_layout

\begin_layout Itemize
Machine Learning Engineers often focus more on producing great data-driven
 products than they do answering operational questions for a company.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Machine Learning Researcher
\end_layout

\begin_deeper
\begin_layout Itemize
Extending state-of-the-art in ML.
 These people may publish papers (for some companies).
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Applied ML scientist
\end_layout

\begin_deeper
\begin_layout Itemize
Stands between ML Researcher and ML Engineer.
 They are responsible for going to the academic literature or the research
 literature and finding the state-of-the-art techniques and finding ways
 to adapt them to a problem they are facing.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Data Engineer
\end_layout

\begin_deeper
\begin_layout Itemize
Organize data and making sure that the data is saved in an easily, accessible,
 secure, and cost effective way.
\end_layout

\begin_layout Itemize
Since you’d be (one of) the first data hires, heavy statistics and machine
 learning expertise is less important than strong software engineering skills.
 As a result, you’ll have great opportunities to shine and grow via trial
 by fire, but there will be less guidance and you may face a greater risk
 of flopping or stagnating.
\end_layout

\begin_layout Itemize
Data Engineers are the link between the management’s big data strategy and
 the data scientists that need to work with data.
\end_layout

\begin_layout Itemize
This can be basically (it can be said, that BA is a 
\begin_inset Quotes eld
\end_inset

beginning position
\begin_inset Quotes erd
\end_inset

 and DM is 
\begin_inset Quotes eld
\end_inset

more skilled
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Business Analyst 
\series default
- using business intelligence to translate the analyzed data into visual
 insights.
 It is also needed to know data mining and AB testing.
 Average salary in 2018 is 
\begin_inset Formula $67.350$
\end_inset

 dollars/year.
\end_layout

\begin_layout Itemize

\series bold
Data Analyst 
\series default
- data preparation and cleaning, entry-level position in data science.
 More math, software tools are used in this role.
 Average salary in 2018 is 
\begin_inset Formula $73.250$
\end_inset

 dollars/year.
\end_layout

\begin_layout Itemize

\series bold
Data Modeler 
\series default
- developing systems and models that can be applied to a company's databases.
 This role can be similar to ML Engineer.
 Average salary in 2018 is 
\begin_inset Formula $81.850$
\end_inset

 dollars/year.
 A more advanced 
\begin_inset Quotes eld
\end_inset

version
\begin_inset Quotes erd
\end_inset

 of this role, that is more about research, is 
\series bold
Data Scientist
\series default
.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Data Scientist
\end_layout

\begin_deeper
\begin_layout Itemize
Examining data and provide insights.
 Make presentations to team/executive.
 However, this position can be very similar to ML Engineer.
\end_layout

\begin_layout Itemize
Data scientist is often used as a blanket title to describe jobs that are
 drastically different! Data science combines several disciplines, including
 statistics, data analysis, machine learning, and computer science.
\end_layout

\begin_layout Itemize
This can be the same name as
\series bold
 Data Analyst 
\series default
role (plus machine learning, but usually more skilled).
 But, if you are data analyst, your job might consist of tasks like pulling
 data out of SQL databases, becoming an Excel or Tableau master, and producing
 basic data visualizations and reporting dashboards.
\end_layout

\begin_layout Itemize
Once you have a handle on your day-to-day responsibilities, a company like
 this can be a great environment to try new things and expand your skillset.
\end_layout

\begin_layout Itemize
They are math experts.
 They use linear algebra and multi-variable calculus to create new insight
 from existing data.
\end_layout

\begin_layout Itemize
Average salary in 2018 is 
\begin_inset Formula $121.350$
\end_inset

 dollars/year.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
AI Product Manager
\end_layout

\begin_deeper
\begin_layout Itemize
Decide what to build, what is feasible and valuable.
\end_layout

\begin_layout Itemize
Similar role is 
\series bold
Data Science Manager
\series default
, but these are both less technical roles.
 They involve presentation of results and communication with internal organs
 or other managers / departments.
 Average salary in 2018 is 
\begin_inset Formula $134.850$
\end_inset

 dollars/year.
\end_layout

\end_deeper
\begin_layout Subsection
Data Science Project Workflow
\end_layout

\begin_layout Enumerate
Identify the question (Data Scientist is responsible for this part, but
 Data Scientist role is responsible for all the roles)
\end_layout

\begin_layout Enumerate
Prepare the data (Data Analyst or Data Modeler)
\end_layout

\begin_layout Enumerate
Analyze the data (Data Modeler)
\end_layout

\begin_layout Enumerate
Visualize the insights (Business Analyst)
\end_layout

\begin_layout Enumerate
Present your findings (Business Analyst or Data Science Manager)
\end_layout

\begin_layout Subsection
Big Data
\end_layout

\begin_layout Standard
The term 
\begin_inset Quotes eld
\end_inset

big data
\begin_inset Quotes erd
\end_inset

 does not refer only to a big volume.
 There are 4 important things: Volume, velocity, variety, and veracity.
\end_layout

\begin_layout Itemize

\series bold
Volume 
\series default
(size) how much data you have.
\end_layout

\begin_layout Itemize

\series bold
Velocity 
\series default
(speed) - how fast data are getting to you.
 How much needs to be processed or is coming into the system.
 This is where the whole concept of streaming data and real-time processing
 comes in to play.
\end_layout

\begin_layout Itemize

\series bold
Variety 
\series default
- how different your data are.
\end_layout

\begin_layout Itemize

\series bold
Veracity
\series default
 (credibility) - how reliable are your data.
 The issue with big data is that they are very unreliable.
 You cannot really trust the data.
\end_layout

\begin_layout Standard
Use Big Data only if you need to! That means, only if you are running into
 scaling issues! Big Data is a very expensive thing.
 For example, a Hadoop cluster for instance needs at least five servers
 to work properly (more is better).
 Also, maintenance and development of top big data tools can be expensive.
\end_layout

\begin_layout Itemize
Batch processing
\end_layout

\begin_layout Itemize
Stream processing
\end_layout

\begin_deeper
\begin_layout Itemize
In stream processing sometimes it is absolutely all right to drop messages,
 other times it is not.
 Sometimes it is fine to process a message multiple times, other times that
 needs to be avoided.
\end_layout

\begin_layout Itemize
That is why there are different strategies of streaming: processing message
 at least once, exactly once, or at most once.
\end_layout

\end_deeper
\begin_layout Paragraph
Hadoop Platforms
\end_layout

\begin_layout Itemize
When people talk about big data, one of the first things come to mind is
 Hadoop.
\end_layout

\begin_layout Itemize
You will see that Hadoop has evolved from a platform into an ecosystem.
 Its design allows a lot of Apache projects and 3rd party tools to benefit
 from Hadoop.
\end_layout

\begin_layout Itemize
Hadoop is a platform for distributed storing and analyzing of very large
 data sets.
 Hadoop has four main modules: Hadoop common, HDFS, MapReduce and YARN.
 The way these modules are woven together is what makes Hadoop so successful.
\end_layout

\begin_layout Itemize
Note: Hadoop is still relevant in 2019 even if you look into serverless
 tools.
\end_layout

\begin_layout Itemize
You use Apache Kafka to ingest data, and store the it in HDFS.
 You do the analytics with Apache Spark and as a backend for the display
 you store data in Apache HBase.
 To have a working system you also need YARN for resource management.
 You also need Zookeeper, a configuration management service to use Kafka
 and HBase.
 Each project is closely connected to the other.
 Spark for instance, can directly access Kafka to consume messages.
 It is able to access HDFS for storing or processing stored data.
 Want to store data from Kafka directly into HDFS without using Spark? No
 problem, there is a project for that.
 Apache Flume has interfaces for Kafka and HDFS.
\end_layout

\begin_layout Itemize

\series bold
Alternatives
\end_layout

\begin_deeper
\begin_layout Itemize
Often times it does not make sense to deploy a Hadoop cluster, because it
 can be overkill.
 Hadoop does not run on a single server.
\end_layout

\begin_layout Itemize
You basically need at least five servers, better six to run a small cluster.
 Because of that.
 the initial platform costs are quite high.
\end_layout

\begin_layout Itemize
One option you have is to use a specialized systems like Cassandra, MongoDB
 or other NoSQL DB’s for storage.
 Or you move to Amazon and use Amazon’s Simple Storage Service, or S3.
\end_layout

\begin_layout Itemize
Guess what the tech behind S3 is.
 Yes, HDFS.
 That’s why AWS also has the equivalent to MapReduce named Elastic MapReduce.
 The great thing about S3 is that you can start very small.
 When your system grows you don’t have to worry about S3’s server scaling.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
HDFS
\series default
 
\series bold
document store
\end_layout

\begin_deeper
\begin_layout Itemize
HDFS works different to a typical file system - HDFS is hardware independent.
 Not only does it span over many disks in a server.
 It also spans over many servers.
\end_layout

\begin_layout Itemize
HDFS will automatically place your files somewhere in the Hadoop server
 collective.
\end_layout

\begin_layout Itemize
It will not only store your file, Hadoop will also replicate it two or three
 times (you can define that).
 Replication means replicas of the file will be distributed to different
 servers.
\end_layout

\begin_layout Itemize
This gives you superior fault tolerance.
 If one server goes down, then your data stays available on a different
 server.
\end_layout

\begin_layout Itemize
Another great thing about HDFS is, that there is no limit how big the files
 can be.
\end_layout

\begin_layout Itemize
HDFS physically stores files different then a normal file system.
 It splits the file into blocks.
 These blocks are then distributed and replicated on the Hadoop cluster.
 The splitting happens automatically.
 In the configuration you can define how big the blocks should be (megabytes
 or gigabyte, it's up to you).
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
MapReduce
\end_layout

\begin_deeper
\begin_layout Itemize
How MapReduce is working is, that it processes data in 2 phases: the map
 phase and then the reduce phase.
\end_layout

\begin_layout Itemize
The map phase - the framework is reading data from HDFS.
 Each dataset is called an input record.
 The whole map and reduce process relies heavily on using key-value pairs.
 That’s what the mappers are for.
 In the map phase input data, for instance a file, gets loaded and transformed
 into key-value pairs.
 When each map phase is done it sends the created key-value pairs to the
 reducers where they are getting sorted by key.
 This means, that an input record for the reduce phase is a list of values
 from the mappers that all have the same key.
\end_layout

\begin_layout Itemize
The reduce phase - actual computation is done and the results are stored.
 The storage target can either be a database or back HDFS or something else.
 Reduce phase is doing the computation of key and its values from map phase
 and outputs the results.
\end_layout

\begin_layout Itemize
The map and reduce phases are parallelised.
 What that means is, that you have multiple map phases (mappers) and reduce
 phases (reducers) that can run in parallel on your cluster machines.
\end_layout

\begin_layout Itemize
How many mappers and reducers can you use in parallel? The number of parallel
 map and reduce processes depends on how many CPU cores you have in your
 cluster.
 Every mapper and every reducer is using one core.
\end_layout

\begin_layout Itemize
MapReduce is awesome for simpler analytics tasks, like counting stuff.
 It just has one flaw: It has only two stages Map and Reduce.
\end_layout

\begin_deeper
\begin_layout Itemize
The problem with MapReduce is that there is no simple way to chain multiple
 map and reduce processes together.
\end_layout

\begin_layout Itemize
At the end of each reduce process the data must be stored somewhere.
 This fact makes it very hard to do complicated analytics processes.
 You would need to chain MapReduce jobs together.
 Chaining jobs with storing and loading intermediate results just makes
 no sense.
\end_layout

\end_deeper
\begin_layout Itemize
Another issue with MapReduce is that it is not capable of streaming analytics.
 Jobs take some time to spin up, do the analytics and shut down.
 Basically minutes of wait time are totally normal.
 This is a big negative point in a more and more real time data processing
 world.
\end_layout

\end_deeper
\begin_layout Paragraph
Apache Spark
\end_layout

\begin_layout Itemize
Spark is a complete in-memory framework.
 Data gets loaded from, for instance HDFS, into the memory of workers.
\end_layout

\begin_layout Itemize
There is no longer a fixed map and reduce stage as it is in MapReduce.
 Your code can be as complex as you want.
\end_layout

\begin_layout Itemize
Once in memory, the input data and the intermediate results stay in memory
 (until the job finishes).
 They do not get written to a drive like with MapReduce.
 This makes Spark the optimal choice for doing complex analytics.
 It allows you for instance to do iterative processes.
 Modifying a dataset multiple times in order to create an output is totally
 easy.
\end_layout

\begin_layout Itemize
Streaming analytics capability is also what makes Spark so great.
 Spark has natively the option to schedule a job to run every X seconds
 or X milliseconds.
 As a result, Spark can deliver you results from streaming data in “real
 time”.
\end_layout

\begin_layout Itemize
Spark vs Hadoop is however a wrong question.
\end_layout

\begin_deeper
\begin_layout Itemize
Hadoop is used to store data in the Hadoop Distributed File System (HDFS).
 It can analyze the stored data with MapReduce and manage resources with
 YARN.
 However, Hadoop is more than just storage, analytics and resource management.
 There’s a whole ecosystem of tools around the Hadoop core.
\end_layout

\begin_layout Itemize
Spark is “just” an analytics framework.
 It has no storage capability.
 Although it has a standalone resource management, you usually don’t use
 that feature.
\end_layout

\begin_layout Itemize
So Spark and Hadoop are not the same thing, and they can even work together.
 As Storage you use HDFS.
 Analytics is done with Apache Spark and YARN is taking care of the resource
 management.
\end_layout

\begin_layout Itemize
From a platform architecture perspective, Hadoop and Spark are usually managed
 on the same cluster.
 This means on each server where a HDFS data node is running, a Spark worker
 thread runs as well.
\end_layout

\begin_layout Itemize
In distributed processing, network transfer between machines is a large
 bottle neck.
 Transferring data within a machine reduces this traffic significantly.
 Spark is able to determine on which data node the needed data is stored.
 This allows a direct load of the data from the local storage into the memory
 of the machine, which significantly reduces network traffic.
\end_layout

\begin_layout Itemize
So, the question is not whether to use Spark or Hadoop.
 The question has to be: Should you use 
\bar under
Spark or MapReduce
\bar default
 alongside Hadoop’s HDFS and YARN.
\end_layout

\begin_layout Itemize
If you are doing simple batch jobs like counting values or doing calculating
 averages, go with MapReduce.
 If you need more complex analytics like machine learning or fast stream
 processing: go with Apache Spark.
\end_layout

\end_deeper
\begin_layout Itemize
Spark jobs can be programmed in a variety of languages.
 That makes creating analytic processes very user-friendly for data scientists.
\end_layout

\begin_layout Itemize
Data locality - you can and you should run Spark workers directly on the
 data nodes of your Hadoop cluster, because processing data locally where
 it is stored is the most efficient thing to do.
 Spark can then natively identify on what data node the needed data is stored.
 This enables Spark to use the worker running on the machine where the data
 is stored to load the data into the memory.
 The downside of this setup is that you need more expensive servers.
 Because Spark processing needs stronger servers with more RAM and CPUs
 than a “pure” Hadoop setup.
\end_layout

\begin_layout Itemize
The machine learning library MLlib is included in Spark so there is often
 no need to import another library.
 But Spark can be integrated with TensorFlow for example.
\end_layout

\begin_layout Itemize
Managing resources - Spark has stand-alone resource manager.
 If Spark is running in an Hadoop environment you don't have to use Spark's
 own standalone resource manager.
 You can configure Spark to use Hadoop's YARN resource management.
 Having a single resource manager instead of two independent ones makes
 it a lot easier to configure the resource management.
\end_layout

\begin_layout Itemize

\series bold
Apache Nifi
\series default
 - tool for creating data pipelines; you can do it directly in UI.
 You can read data from a RestAPI and post it to Kafka, or read data from
 Kafka and put it into a database.
\end_layout

\begin_layout Subsection
Machine Learning in Production
\end_layout

\begin_layout Itemize
Machine learning in production can be done by using stream and batch processing.
 In the batch processing layer you are creating the models, because you
 have all the data available for training.
 In the stream in processing layer you are using created models and you
 are applying them to new data.
\end_layout

\begin_layout Itemize
This is a constant cycle: training, applying, re-training, pushing into
 production, and applying.
 What you don’t want to do is doing this manually.
 You need to figure out a process of automatic retraining and automatic
 pushing to into production of models.
\end_layout

\begin_layout Itemize
Automation is harder than you think! Look at AWS machine learning for instance.
 The process is: build, train, tune deploy.
 Where’s the loop of retraining? You can create models and then use them
 in production.
 But this loop is almost nowhere to be seen.
 In 2019, this is still a very big issue that needs to be solved.
\end_layout

\begin_layout Subsection
Data Warehouse
\end_layout

\begin_layout Subsection
Data Lake
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section
Data Types
\end_layout

\begin_layout Standard
(from here
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{http://www.statisticshowto.com/types-variables/}}
\end_layout

\end_inset

)
\end_layout

\begin_layout Itemize
Qualitative (broader name for variables that can't be counted, output of
 classification models)
\end_layout

\begin_deeper
\begin_layout Itemize

\bar under
Nominal
\bar default
 (a.k.a.
 categorical) are like labels.
 They are mutually exclusive, so no overlap.
 Memohelp - nominal sounds like name.
 There is no order, for instance MALE and FEMALE.
 They are discrete.
\end_layout

\begin_layout Itemize

\bar under
Ordinal
\bar default
 in which the order matters, we can assign scores.
 Examples are BAD, OK and HAPPY.
\end_layout

\begin_layout Itemize

\bar under
Dichotomous
\bar default
 (a.k.a.
 binary variable), that can only have 2 values.
\end_layout

\end_deeper
\begin_layout Itemize
Quantitative (broader name for variables that can be counted, output of
 regression models)
\end_layout

\begin_deeper
\begin_layout Itemize

\bar under
Discrete
\bar default
 can be mapped to limited int, which means that there can be only limited
 amount of possible values.
\end_layout

\begin_layout Itemize

\bar under
Continuous
\bar default
 have unlimited number of values.
\end_layout

\end_deeper
\begin_layout Itemize
Different angle, from experimentation with model
\end_layout

\begin_deeper
\begin_layout Itemize

\bar under
Confounding variable
\bar default
: extra variables that have a hidden effect on your experimental results.
 
\end_layout

\begin_layout Itemize

\bar under
Dependent variable
\bar default
: the outcome of an experiment.
 As you change the independent variable, you watch what happens to the dependent
 variable.
\end_layout

\begin_layout Itemize

\bar under
Independent variable
\bar default
: (also known as predictor variable) a variable that is not affected by
 anything that you, the researcher, does.
 Usually plotted on the x-axis.
 It is usually some input, feature, but it is also known as explanatory
 variable, regressor, and some others
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://en.wikipedia.org/wiki/Dependent_and_independent_variables}}
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize

\bar under
Endogenous variable
\bar default
: similar to dependent variables, they are affected by other variables in
 the system.
 Used almost exclusively in econometrics.
 
\end_layout

\begin_layout Itemize

\bar under
Exogenous variable
\bar default
: variables that affect others in the system.
 
\end_layout

\begin_layout Itemize

\bar under
Extraneous variables
\bar default
 are any variables that you are not intentionally studying in your experiment
 or test.
\end_layout

\begin_layout Itemize

\bar under
Latent variable
\bar default
: a hidden variable that can't be measured or observed directly.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section
Data Distributions 
\end_layout

\begin_layout Standard
(from here
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-
science/}}
\end_layout

\end_inset

 and here
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://medium.com/@laumannfelix/statistics-probability-fundamentals-2-cbb123
9f9605}}
\end_layout

\end_inset

)
\end_layout

\begin_layout Itemize

\bar under
Bernoulli distribution
\bar default
 - it has only two possible outcomes, namely 1 (success) and 0 (failure),
 and a single trial.
\begin_inset Formula 
\begin{equation}
P(x)=\begin{cases}
1-p & x=0\\
p & x=1
\end{cases}\label{eq:bernoulli}
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
for 
\begin_inset Formula $0<p<1$
\end_inset


\end_layout

\begin_layout Itemize
A Bernoulli distribution is a special case of Binomial distribution (see
 below).
 Specifically, when 
\begin_inset Formula $n=1$
\end_inset

 the 
\series bold
Binomial distribution
\series default
 becomes Bernoulli distribution.
 Also, a generalization of Bernoulli distribution is called 
\series bold
Categorical distribution
\series default
 (probability distribution for a set of discrete random variables).
\end_layout

\begin_layout Itemize
The Bernoulli distribution serves as a building block for discrete distributions
 which model Bernoulli trials, such as 
\series bold
binomial distribution 
\series default
and
\series bold
 geometric distribution
\series default
.
\end_layout

\begin_layout Itemize
It is a probability distribution over a two-valued (=binary - true or false
 / 1 or 0) random variable.
\end_layout

\begin_layout Itemize

\series bold
A Bernoulli trial, or Bernoulli experiment
\series default
, is an experiment satisfying two key properties:
\end_layout

\begin_deeper
\begin_layout Itemize
There are exactly two complementary outcomes, success and failure.
\end_layout

\begin_layout Itemize
The probability of success is the same every time the experiment is repeated.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Example
\series default
: For a single coin toss, the probability you win one dollar is 
\begin_inset Formula $p$
\end_inset

.
 The random variable that represents your winnings after one coin toss is
 a Bernoulli random variable.
\end_layout

\begin_layout Itemize
Another examples:
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://brilliant.org/wiki/bernoulli-distribution/}}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
A tennis player either wins or loses a match.
\end_layout

\begin_layout Itemize
A newborn child is either male or female.
\end_layout

\begin_layout Itemize
You either pass or fail an exam.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/bernoulli_distr.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of Bernoulli Distribution of data.
 X-axis has 2 possible scenarios and y-axis is a probability.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\bar under
Binomial distribution
\bar default
 - a distribution where only two outcomes are possible, such as success
 or failure, gain or loss, win or lose and where the 
\series bold
probability of success and failure is same for all the trials
\series default
 is called a Binomial distribution.
\end_layout

\begin_deeper
\begin_layout Itemize
If there are 
\begin_inset Formula $n$
\end_inset

 Bernoulli trials, and each trial has a probability 
\begin_inset Formula $p$
\end_inset

 of success, then the probability of exactly 
\begin_inset Formula $k$
\end_inset

 successes is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Pr(X=k)=\left(\begin{array}{c}
k\\
n
\end{array}\right)p^{k}(1\text{−}p)^{n\text{−}k}\label{eq:binomial_distribution}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
denoting the probability that the random variable 
\begin_inset Formula $X$
\end_inset

 is equal to 
\begin_inset Formula $k$
\end_inset


\end_layout

\begin_layout Itemize
In other words, an 
\series bold
experiment
\series default
 with 
\series bold
only two possible outcomes
\series default
 
\series bold
repeated 
\begin_inset Formula $n$
\end_inset

 number of times
\series default
 is called 
\series bold
binomial
\series default
.
 The parameters of a binomial distribution are 
\begin_inset Formula $n$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the total number of trials and 
\begin_inset Formula $p$
\end_inset

 is the probability of success in each trial.
\end_layout

\begin_layout Itemize
It is useful for analyzing the results of repeated independent trials.
\end_layout

\begin_layout Itemize
A binomial experiment is a series of 
\begin_inset Formula $n$
\end_inset

 Bernoulli trials, whose outcomes are independent of each other.
 A random variable, 
\begin_inset Formula $X$
\end_inset

, is defined as the number of successes in a binomial experiment.
 Finally, a binomial distribution is the probability distribution of 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Example
\series default
: if you toss the coin 5 times, your winnings could be any whole number
 of dollars from zero dollars to five dollars, inclusive.
 The probability that you win five dollars is 
\begin_inset Formula $p^{5}$
\end_inset

, because each coin toss is independent of the others, and for each coin
 toss the probability of heads is 
\begin_inset Formula $p$
\end_inset

.
 Probability of winning exactly three dollars in five tosses would require
 you to toss the coin five times, getting exactly three heads and two tails.
 This can be achieved with probability 
\begin_inset Formula $\binom{5}{3}*p^{3}*(1\text{−}p)^{2}$
\end_inset

.
 And, in general, if there are n Bernoulli trials, then the sum of those
 trials is binomially distributed with parameters n and p.
\end_layout

\begin_layout Itemize
Another example:
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://brilliant.org/wiki/binomial-distribution/}}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider a fair coin.
 Flipping the coin once is a Bernoulli trial, since there are exactly two
 complementary outcomes (flipping a head and flipping a tail), and they
 are both 
\begin_inset Formula $\frac{1}{2}$
\end_inset

 no matter how many times the coin is flipped.
 
\bar under
Note that the fact that the coin is fair is not necessary; flipping a weighted
 coin is still a Bernoulli trial.
\end_layout

\begin_layout Itemize
A binomial experiment might consist of flipping the coin 100 times, with
 the resulting number of heads being represented by the random variable
 
\begin_inset Formula $X$
\end_inset

.
 The binomial distribution of this experiment is the probability distribution
 of 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Properties
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize
Each trial (/attempt) is independent.
\end_layout

\begin_layout Itemize
There are only two possible outcomes in a trial - either a success or a
 failure.
 
\end_layout

\begin_layout Itemize
A total number of 
\begin_inset Formula $n$
\end_inset

 identical trials are conducted.
 
\end_layout

\begin_layout Itemize
The probability of success and failure is same for all trials (trials are
 identical).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(x)=\frac{n!}{(n-x)!x!}p^{x}*(1-p)^{n-x}\label{eq:binomial}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
The difference between Binomial and Bernoulli distributions are
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://math.stackexchange.com/questions/838107/what-is-the-difference-and-rel
ationship-between-the-binomial-and-bernoulli-distr}}
\end_layout

\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
A Bernoulli random variable has two possible outcomes: 0 or 1.
 
\end_layout

\begin_layout Itemize
A Binomial distribution is the 
\series bold
sum of independent and identically distributed
\series default
 Bernoulli random variables.
\end_layout

\begin_layout Itemize
All Bernoulli distributions are Binomial distributions, but most Binomial
 distributions are not Bernoulli distributions.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\bar under
Uniform distribution
\bar default
 - all the 
\begin_inset Formula $n$
\end_inset

 number of possible outcomes of a uniform distribution are equally probable,
 always defined with boundaries 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
A variable 
\begin_inset Formula $x$
\end_inset

 is said to be uniformly distributed if the 
\series bold
the
\series default
 
\series bold
probability density function
\series default
 is:
\begin_inset Formula 
\begin{equation}
f(x)=\begin{cases}
\frac{1}{b-a} & a\leq x\leq b\\
0 & x<a\:or\:x>b
\end{cases}\label{eq:uniform}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And graph of a uniform distribution curve looks like:
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/uniform_distr.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of Uniform distribution of data.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\bar under
Geometric distribution
\end_layout

\begin_deeper
\begin_layout Itemize
The geometric distribution is useful for determining the likelihood of a
 success given a limited number of trials, which is highly applicable to
 the real world in which unlimited (and unrestricted) trials are rare.
\end_layout

\begin_layout Itemize
For a geometric distribution with probability 
\begin_inset Formula $p$
\end_inset

 of success, the probability that exactly 
\begin_inset Formula $k$
\end_inset

 failures occur before the first success is:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Pr(X=k)=(1-p)^{k}p\label{eq:geometric_distribution}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Note that the geometric distribution satisfies the important property of
 being memory-less, meaning that if a success has not yet occurred at some
 given point, the probability distribution of the number of additional failures
 does not depend on the number of failures already observed.
 For instance, suppose a die is being rolled until a 1 is observed.
 If the additional information were provided that the die had already been
 rolled three times without a 1 being observed, the probability distribution
 of the number of further rolls is the same as it would be without the additiona
l information.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://brilliant.org/wiki/geometric-distribution}}
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
An example: A programmer has a 90% chance of finding a bug every time he
 compiles his code, and it takes him two hours to rewrite his code every
 time he discovers a bug.
 What is the probability that he will finish his program by the end of his
 workday?
\end_layout

\begin_deeper
\begin_layout Itemize
Assume that a workday is 8 hours and that the programmer compiles his code
 immediately at the beginning of the day.
\end_layout

\begin_layout Itemize
In this instance, a success is a bug-free compilation, and a failure is
 the discovery of a bug.
 The programmer needs to have 0, 1, 2, or 3 failures, so his probability
 of finishing his program is:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $Pr(X=0)+Pr(X=1)+Pr(X=2)+Pr(X=3)\text{​}=(0.9)^{0}(0.1)+(0.9)^{1}(0.1)+(0.9)^{2}(0.1)+(0.9)^{3}(0.1)=0.344$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
This information is useful for determining whether the programmer should
 spend his day writing the program or performing some other tasks during
 that time.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\bar under
Normal (Gaussian) distribution
\bar default
 - characterized by standard deviation and mean.
 This distribution represents the behavior of most of the situations in
 the universe.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Properties
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize
The mean, median and mode of the distribution coincide.
 
\end_layout

\begin_layout Itemize
The curve of the distribution is bell-shaped and symmetrical about the line
 
\begin_inset Formula $x=μ$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The total area under the curve is 1.
 
\end_layout

\begin_layout Itemize
Exactly half of the values are to the left of the center and the other half
 to the right.
\end_layout

\end_deeper
\begin_layout Itemize
The normal distribution is highly different from Binomial distribution.
 However, if the number of trials 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

, then the shapes will be quite similar.
\end_layout

\begin_layout Itemize
The normal distribution is also a limiting case of Poisson distribution
 with the parameter 
\begin_inset Formula $λ\rightarrow\infty$
\end_inset

.
\end_layout

\begin_layout Itemize
It can be computed in the following way (so there are just 2 parameters:
 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

):
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}\label{eq:gaussian}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/normal_distr.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of 3 Normal distributions of data.
 X-axis has measured variable, and y-axis has density.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Owing largely to the central limit theorem, the normal distributions is
 an appropriate approximation even when the underlying distribution is known
 to be not normal.
 This is convenient because the normal distribution is easy to obtain estimates
 with; the empirical rule states that 68% of the data modeled by a normal
 distribution falls within 1 standard deviation of the mean, 95% within
 2 standard deviations, and 99.7% within 3 standard deviations.
 For obvious reasons, the empirical rule is also occasionally known as the
 68-95-99.7 rule.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://brilliant.org/wiki/normal-distribution}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\bar under
Poisson distribution
\bar default
 - used as an approximation to binomial with 
\begin_inset Formula $p<0,1$
\end_inset

 and 
\begin_inset Formula $n>30$
\end_inset

.
 Normal distribution is used as an approximation to binomial with sufficiently
 large 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This is a discrete probability distribution of the number of events occurring
 in a given time period, given the average number of times the event occurs
 over that time period.
\end_layout

\begin_layout Itemize
The Poisson distribution is applicable only when several conditions hold:
\end_layout

\begin_deeper
\begin_layout Itemize
An event can occur any number of times during a time period.
\end_layout

\begin_layout Itemize
Events occur independently.
 In other words, if an event occurs, it does not affect the probability
 of another event occurring in the same time period.
\end_layout

\begin_layout Itemize
The rate of occurrence is constant; that is, the rate does not change based
 on time.
\end_layout

\begin_layout Itemize
The probability of an event occurring is proportional to the length of the
 time period.
 For example, it should be twice as likely for an event to occur in a 2
 hour time period than it is for an event to occur in a 1 hour period.
\end_layout

\end_deeper
\begin_layout Itemize
Given that a situation follows a Poisson distribution, there is a formula
 which allows one to calculate the probability of observing 
\begin_inset Formula $k$
\end_inset

 events over a time period for any non-negative integer value of 
\begin_inset Formula $k$
\end_inset

.
 Let 
\begin_inset Formula $X$
\end_inset

 be the discrete random variable that represents the number of events observed
 over a given time period.
 Let 
\begin_inset Formula $\lambda$
\end_inset

 be the expected value (average) of 
\begin_inset Formula $X$
\end_inset

.
 If 
\begin_inset Formula $X$
\end_inset

 follows a Poisson distribution, then the probability of observing 
\begin_inset Formula $k$
\end_inset

 events over the time period is:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(X=k)=\frac{λ^{k}e^{-λ}}{k!}\label{eq:poisson_distribution}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $e$
\end_inset

 is Euler's number
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Example
\series default
: suppose you work at a call center, approximately how many calls do you
 get in a day? It can be any number.
 Now, the entire number of calls at a call center in a day is modeled by
 Poisson distribution.
\end_layout

\begin_layout Itemize
Another example: The Poisson distribution is appropriate for modeling the
 number of phone calls an office would receive during the noon hour, if
 they know that they average 4 calls per hour during that time period.
\end_layout

\begin_deeper
\begin_layout Itemize
Although the average is 4 calls, they could theoretically get any number
 of calls during that time period.
\end_layout

\begin_layout Itemize
The events are effectively independent since there is no reason to expect
 a caller to affect the chances of another person calling.
\end_layout

\begin_layout Itemize
The occurrence rate may be assumed to be constant.
\end_layout

\begin_layout Itemize
It is reasonable to assume that (for example) the probability of getting
 a call in the first half hour is the same as the probability of getting
 a call in the final half hour.
\end_layout

\end_deeper
\begin_layout Itemize
Another example: In the World Cup, an average of 2.5 (this is our expected
 value 
\begin_inset Formula $\lambda$
\end_inset

) goals are scored each game.
 Modeling this situation with a Poisson distribution, what is the probability
 that 
\begin_inset Formula $k$
\end_inset

 goals are scored in a game?
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $P(X=0)=\frac{2.5^{0}e^{-2.5}}{0!}\approx0.082$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(X=1)=\frac{2.5^{1}e^{-2.5}}{1!}\approx0.205$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(X=2)=\frac{2.2e^{-2.5}}{2!}\approx0.257$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(X=3)=\frac{2.5^{3}e^{-2.5}}{3!}\approx0.213$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(X=4)=\frac{2.5^{4}e^{-2.5}}{4!}\approx0.133$
\end_inset


\end_layout

\begin_layout Standard
And so on.
 There is no upper limit on the value of 
\begin_inset Formula $k$
\end_inset

 for this formula, though the probability rapidly approaches 0 as 
\begin_inset Formula $k$
\end_inset

 increases (so more far away we are from expected value, then the lower
 probability we receive).
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/poisson_distr.jpg
	scale 150

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of Poisson distribution of data.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\bar under
Exponential distribution
\bar default
 - probability distribution that describes the time between events in a
 Poisson point process, i.e.
 a process in which events occur continuously and independently at a constant
 average rate.
\end_layout

\begin_deeper
\begin_layout Itemize
The exponential distribution is a continuous probability distribution which
 describes the amount of time it takes to obtain a success in a series of
 continuously occurring independent trials.
 It is a continuous analog of the geometric distribution.
\end_layout

\begin_layout Itemize

\series bold
Example
\series default
: interval of time between the calls of call center.
\end_layout

\begin_layout Itemize

\series bold
The probability density function
\series default
:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x)=\begin{cases}
\lambda e^{-\lambda x} & x\geq0\\
0 & x<0
\end{cases}\label{eq:exp_distr}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $λ>0$
\end_inset

 is the parameter of the distribution, often called the rate parameter.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/exp_distr.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of Expoiential distribution of data.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\bar under
Log-normal distribution
\bar default
 is the probability distribution of a random variable whose logarithm follows
 a normal distribution.
 It models phenomena whose relative growth rate is independent of size,
 which is true of most natural phenomena including the size of tissue and
 blood pressure, income distribution, and even the length of chess games.
\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $Z$
\end_inset

 be a standard normal variable, which means the probability distribution
 of 
\begin_inset Formula $Z$
\end_inset

 is normal centered at 0 and with variance 1.
 Then a log-normal distribution is defined as the probability distribution
 of a random variable:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
X=e^{\mu+\sigma Z}\label{eq:log_normal_distribution}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 are the mean and standard deviation of the logarithm of 
\begin_inset Formula $X$
\end_inset

, respectively.
\end_layout

\end_deeper
\begin_layout Itemize
The term "log-normal" comes from the result of taking the logarithm of both
 sides:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $log(X)=μ+σZ$
\end_inset


\end_layout

\begin_layout Standard
As 
\begin_inset Formula $Z$
\end_inset

 is normal, 
\begin_inset Formula $\mu+\sigma Z$
\end_inset

 is also normal (the transformations just scale the distribution, and do
 not affect normality), meaning that the logarithm of 
\begin_inset Formula $X$
\end_inset

 is normally distributed (hence the term log-normal).
\end_layout

\end_deeper
\begin_layout Itemize
For most natural growth processes, the growth rate is independent of size,
 so the log-normal distribution is followed.
 As a result, the log-normal distribution has heavy applications to biology
 and finance, two areas where growth is an important area of study.
 In particular, epidemics and stock prices tend to follow a log-normal distribut
ion.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\bar under
Multivariate normal distribution
\end_layout

\begin_deeper
\begin_layout Itemize
It is a vector in multiple normally distributed variables, such that any
 linear combination of the variables is also normally distributed.
 It is mostly useful in extending the central limit theorem to multiple
 variables, but also has applications to Bayesian inference and thus machine
 learning, where the multivariate normal distribution is used to approximate
 the features of some characteristics.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://brilliant.org/wiki/multivariate-normal-distribution/}}
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
A random vector 
\begin_inset Formula $X=(X_{1},X_{2},...,X_{n})$
\end_inset

 is multivariate normal if any linear combination of the random variables
 
\begin_inset Formula $X_{1},X_{2},...,X_{n}$
\end_inset

 is normally distributed.
 In other words, 
\begin_inset Formula $a_{1}X_{1}+a_{2}X_{2}+...+a_{n}X_{n}$
\end_inset

 has a normal distribution for any constants 
\begin_inset Formula $a_{1},a_{2},...,a_{n}$
\end_inset

.
\end_layout

\begin_layout Itemize
Equivalently, multivariate distributions can be viewed as a linear transformatio
n of a collection of independent standard normal random variables, meaning
 that if 
\begin_inset Formula $z$
\end_inset

 is another random vector whose components are all standard random variables,
 there exists a matrix 
\begin_inset Formula $A$
\end_inset

 and vector 
\begin_inset Formula $\mu$
\end_inset

 such that 
\begin_inset Formula $x=Az+μ$
\end_inset

.
\end_layout

\begin_layout Itemize
The multivariate normal distribution is useful in analyzing the relationship
 between multiple normally distributed variables.
\end_layout

\end_deeper
\begin_layout Section
Basic Terms
\begin_inset CommandInset label
LatexCommand label
name "sec:Basic-Terms"

\end_inset


\end_layout

\begin_layout Description
Classification
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

algorithms: discrete valued output (e.g.
 0 or 1).
 They can be divided into:
\end_layout

\begin_layout Itemize
Binary classification problems - only 2 discrete values.
 
\series bold
Usually 
\series default
- we give label '1'
\series bold
 
\series default
to rare class that we want to detect.
\end_layout

\begin_layout Itemize
Multi-class classification problems - more discrete values than 2.
 This is usually solved by using 
\series bold
one-vs-all
\series default
 (or one-vs-rest) technique, where we have 2 classes (1 specific and all
 others) and then we iterate over all classes.
 So 
\begin_inset Formula $n$
\end_inset

 classes, 
\begin_inset Formula $n$
\end_inset

 binary classification problems, 
\begin_inset Formula $n$
\end_inset

 trained classifiers as a result.
\end_layout

\begin_layout Description
Regression
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

algorithms: predict continuous valued output given an unlabeled example.
 The regression problem is solved by a regression learning algorithm that
 takes a collection of labeled examples as inputs and produces a model that
 can take an unlabeled example as input and output a target.
\end_layout

\begin_layout Description
Image
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

classification: is there a car on a picture? So, we are working with 1 object
 (car).
\end_layout

\begin_layout Description
Classification
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

with
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

localization: localize (an select with bounding box) car where it is located
 on a picture.
 So again a single object (car).
 For example, localization of 4 landmarks of a human face (beginning and
 end of eyes).
 Each landmark is X and Y coordinate, so ground truth is a vector of 9 numbers
 - the first is if it is an object or not, and the other 8 are coordinates
 for all 4 landmarks.
 Then loss function is computed (it is still just 1 number) by summing over
 all such 9 numbers obtained by ground truth minus predicted values (maybe
 using power of 2 on each deviation) .
\end_layout

\begin_layout Description
Detection: it is classification with localization in a given picture, but
 we may have multiple objects.
 All are selected, so we are working with many objects.
 This can be achieved with sliding window, but nowadays we don't use a tradition
al algorithm (computational cost).
 We use 
\begin_inset Quotes eld
\end_inset

convolutional implementation
\begin_inset Quotes erd
\end_inset

, see the next figure.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/convolutional_sliding_window.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Convolutional implementation(s) of sliding window for object detection (2014).
 Instead of going through an input image step by step, with many sliding
 windows, convolutional layer will do this in more optimal way (all at once).
 This still can have a problem with outputting the most accurate bounding
 boxes (shape can be rectangular a bit as well).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Description
Verification
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

vs
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Recognition: 
\end_layout

\begin_layout Itemize
verification: input is image/name/ID, output is whether the input image
 is that claimed person.
 This is 1:1 problem.
\end_layout

\begin_layout Itemize
recognition is much harder than verification: it has database of 
\begin_inset Formula $K$
\end_inset

 people, and just an image on its input.
 Output is ID if the image is any of 
\begin_inset Formula $K$
\end_inset

 people; 
\begin_inset Quotes eld
\end_inset

not recognized
\begin_inset Quotes erd
\end_inset

 otherwise.
 This is 1:K problem (so K verification steps).
 An example:
\end_layout

\begin_deeper
\begin_layout Itemize
Your face verification system is mostly working well.
 But since Kian got his ID card stolen, when he came back to the house that
 evening he couldn't get in!
\end_layout

\begin_layout Itemize
To reduce such shenanigans, you'd like to change your face verification
 system to a face recognition system.
 This way, no one has to carry an ID card anymore.
 An authorized person can just walk up to the house, and the front door
 will unlock for them!
\end_layout

\begin_layout Itemize
No more person's name on the input!
\end_layout

\end_deeper
\begin_layout Description
Fuzzy
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

set
\end_layout

\begin_layout Itemize
It is a generalization of a set.
\end_layout

\begin_layout Itemize
For each element 
\begin_inset Formula $x$
\end_inset

 in a fuzzy set 
\begin_inset Formula $S$
\end_inset

, there is a membership function 
\begin_inset Formula $μ_{S}(x)\,∈\,[0,1]$
\end_inset

 that defines the membership strength of 
\begin_inset Formula $x$
\end_inset

 to the set 
\begin_inset Formula $S$
\end_inset

.
\end_layout

\begin_layout Itemize
We say that 
\begin_inset Formula $x$
\end_inset

 weakly belongs to a fuzzy set 
\begin_inset Formula $S$
\end_inset

 if 
\begin_inset Formula $μ_{S}(x)$
\end_inset

 is close to 
\begin_inset Formula $0$
\end_inset

.
 On the other hand, if 
\begin_inset Formula $μ_{S}(x)$
\end_inset

 is close to 
\begin_inset Formula $1$
\end_inset

, then x has a strong membership in 
\begin_inset Formula $S$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $μ(x)=1$
\end_inset

 for all 
\begin_inset Formula $x∈S$
\end_inset

, then a fuzzy set 
\begin_inset Formula $S$
\end_inset

 becomes equivalent to a normal, non-fuzzy set.
\end_layout

\begin_layout Description
Shallow
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

vs.
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Deep
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

learning:
\end_layout

\begin_layout Itemize
A shallow learning algorithm learns the parameters of the model directly
 from the features of the training examples.
 Most supervised learning algorithms are shallow.
\end_layout

\begin_layout Itemize
The notorious exceptions are neural network learning algorithms, specifically
 those that build neural networks with more than one layer between input
 and output.
 Such neural networks are called deep neural networks.
 In deep neural network learning (or, simply, deep learning), contrary to
 shallow learning, most model parameters are learned not directly from the
 features of the training examples, but from the outputs of the preceding
 layers.
\end_layout

\begin_layout Description
Model-Based
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

vs.
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Instance-Based
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Learning
\end_layout

\begin_layout Itemize
Most supervised learning algorithms are
\series bold
 model-based
\series default
.
 Model-based learning algorithms use the training data to create a model
 that has parameters learned from the training data.
 After the model was built, the training data can be discarded.
\end_layout

\begin_layout Itemize

\series bold
Instance-based
\series default
 learning algorithms use the whole dataset as the model.
 One example of algorithm is k-Nearest Neighbors (kNN).
 In classification, to predict a label for an input example the kNN algorithm
 looks at the close neighborhood of the input example in the space of feature
 vectors and outputs the label that it saw the most often in this close
 neighborhood.
\end_layout

\begin_layout Description
One-Shot
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Learning
\begin_inset CommandInset label
LatexCommand label
name "One-ShotLearning"

\end_inset


\end_layout

\begin_layout Itemize
This is one of the challenges of face recognition.
 For most face recognition applications you need to be able to 
\series bold
recognize
\series default
 a person given just one single example example of that person's face.
\end_layout

\begin_deeper
\begin_layout Itemize
So, we want to build a model that can recognize that two photos of the same
 person represent that same person.
 If we present to the model two photos of two different people, we expect
 the model to recognize that the two people are different.
\end_layout

\begin_layout Itemize
To solve such a problem, we could go a traditional way and build a binary
 classifier that takes two images as input and predict either true (when
 the two pictures represent the same person) or false (when the two pictures
 belong to different people).
 However, in practice, this would result in a neural network twice as big
 as a typical neural network, because each of the two pictures needs its
 own embedding sub-network.
 Training such a network would be challenging not only because of its size,
 but also because the positive examples would be much harder to obtain than
 negative ones (highly imbalanced problem).
\end_layout

\begin_layout Itemize
It’s a common misconception that for one-shot learning we need only one
 example of each entity for training.
 In practice, we need more than one example of each person for the person
 identification model to be accurate.
 It’s called one-shot because of the most frequent application of such a
 model: face-based authentication.
 For example, such a model could be used to unlock your phone.
 If your model is good, then you only need to have one picture of you on
 your phone and it will recognize you, and also it will recognize that someone
 else is not you.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
For example
\series default
: we have let's say 
\begin_inset Formula $K$
\end_inset

 faces of people in our database.
 So we are using softmax in our deep neural network for multi-task classificatio
n.
 If a new person come, we would need to retrain the whole network, which
 is not wise.
 Instead, a similarity function is being used.
 Bigger it is, more different faces are.
 This similarity function can be learned with ANN, more specifically 
\series bold
Siamese network
\series default
 (see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Siamese-Network"
plural "false"
caps "false"
noprefix "false"

\end_inset

), that solves this problem effectively.
\end_layout

\begin_layout Description
Zero-Shot
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Learning
\end_layout

\begin_layout Standard

\series bold
Note
\series default
: This is a relatively new research area, so there are no algorithms that
 proved to have a significant practical utility yet.
\end_layout

\begin_layout Itemize
In zero-shot learning (ZSL) we want to train a model to assign labels to
 objects.
 The most frequent application is to learn to assign labels to images.
 However, contrary to standard classification, 
\series bold
we want the model to be able to predict labels that we didn't have in the
 training data
\series default
.
\end_layout

\begin_layout Itemize
The trick is to use embeddings (for example word embeddings) no just to
 represent the input 
\begin_inset Formula $x$
\end_inset

, but also to represent the output 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Word embeddings
\series default
 have such a property that each dimension of the embedding represents a
 specific feature of the meaning of the word.
 For example, if our word embedding has four dimensions (usually they are
 much wider, between 50 and 300 dimensions), then these four dimensions
 could represent such features of the meaning as animalness, abstractness,
 sourness, and yellowness (yes, sounds funny, but it’s just an example).
 So the word bee would have an embedding like this 
\begin_inset Formula $[1,0,0,1]$
\end_inset

, the word yellow like this 
\begin_inset Formula $[0,1,0,1]$
\end_inset

, the word unicorn like this 
\begin_inset Formula $[1,1,0,0]$
\end_inset

.
 The values for each embedding are obtained using a specific training procedure
 applied to a vast text corpus.
\end_layout

\end_deeper
\begin_layout Itemize
Now, in our classification problem, we can replace the label 
\begin_inset Formula $y_{i}$
\end_inset

 for each example 
\begin_inset Formula $i$
\end_inset

 in our training set with its word embedding and train a multi-label model
 that predicts word embeddings.
 To get the label for a new example 
\begin_inset Formula $x$
\end_inset

, we apply our model 
\begin_inset Formula $f$
\end_inset

 to 
\begin_inset Formula $x$
\end_inset

, get the embedding 
\begin_inset Formula $\hat{y}$
\end_inset

 and then search among all English words those whose embeddings are the
 most similar to 
\begin_inset Formula $\hat{y}$
\end_inset

 using cosine similarity.
\end_layout

\begin_layout Itemize

\series bold
An example
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize
Take a zebra - it is 
\bar under
white
\bar default
, it is a 
\bar under
mammal
\bar default
, and it has 
\bar under
stripes
\bar default
.
\end_layout

\begin_layout Itemize
Take a clownfish - it is 
\bar under
orange
\bar default
, 
\bar under
not a
\bar default
 
\bar under
mammal
\bar default
, and has 
\bar under
stripes
\bar default
.
\end_layout

\begin_layout Itemize
Now take a tiger - it is 
\bar under
orange
\bar default
, it is a 
\bar under
mammal
\bar default
, and it has 
\bar under
stripes
\bar default
.
\end_layout

\begin_layout Itemize
If these 
\bar under
three features
\bar default
 are present in word embeddings, the CNN would learn to detect these same
 features in pictures.
 Even if the label tiger was absent in the training data, but other objects
 including zebras and clownfish were, then the CNN will most likely learn
 the notion of mammalness, orangeness, and stripeness to predict labels
 of those objects.
\end_layout

\begin_layout Itemize
Once we present the picture of a tiger to the model, 
\bar under
those features will be correctly identified from the image
\bar default
 and most likely the closest word embedding from our English dictionary
 to the predicted embedding will be that of tiger.
\end_layout

\end_deeper
\begin_layout Description
Self-learning
\end_layout

\begin_layout Itemize
Historically, there were multiple attempts at solving 
\bar under
semi-supervised learning problem
\bar default
.
 None of them could be called universally acclaimed and frequently used
 in practice.
\end_layout

\begin_layout Itemize
One frequently cited method is called 
\series bold
self-learning
\series default
.
 In self-learning, we use a learning algorithm to build the initial model
 using the labeled examples.
 Then we apply the model to all unlabeled examples and label them using
 the model.
 If the confidence score of prediction for some unlabeled example x is higher
 than some threshold (chosen experimentally), then we add this labeled example
 to our training set, retrain the model and continue like this until a stopping
 criterion is satisfied.
 We could stop, for example, if the accuracy of the model has not been improved
 during the last m iterations.
\end_layout

\begin_layout Description
Grammar
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

induction
\end_layout

\begin_layout Itemize
Also known as grammatical inference.
 It is a process of learning a formal grammar (usually as a collection of
 re-write rules or as finite state machine or automation of some kind) from
 a set of observations, thus constructing a model which accounts for the
 characteristics of the observed objects.
\end_layout

\begin_layout Itemize
More generally, grammatical inference is that branch of machine learning
 where the instance space consists of discrete combinatorial objects such
 as strings, trees and graphs.
\end_layout

\begin_layout Itemize
Finite state machines of various types, and context-free grammars are common.
\end_layout

\begin_layout Itemize
The applications of grammar induction are for example semantic parsing (task
 of converting a natural language utterance to a logical form: a machine-underst
andable representation of its meaning; semantic parsing can thus be understood
 as extracting the precise meaning of an utterance; applications of semantic
 parsing include machine translation, question answering and code generation),
 natural language understanding, example-based translation (this is essentially
 a translation by analogy), morpheme analysis, and so on.
 But also for loss-less data compression.
\end_layout

\begin_layout Description
Speech
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

recognition
\end_layout

\begin_layout Itemize

\series bold
In speech recognition problem
\series default
, you are given an input 
\begin_inset Formula $x$
\end_inset

 (audio clip), and you job is to automatically find a text transcript 
\begin_inset Formula $y$
\end_inset

.
 Audio clip has 
\series bold
time 
\series default
on X-axis, and what microphone does is to measure very small changes in
 
\series bold
air pressure
\series default
 (Y-axis).
 But a human ear doesn't process raw wave forms.
 Human ear has physical structures that measures the 
\series bold
amounts of intensity of different frequencies
\series default
.
 Thus, a common preprocessing step is to run an input audio clip and generate
 a spectogram (time on X-axis and frequencies on Y-axis) that shows intensities
 of different colors that shows the amount of energy (how loud is a sound
 at different frequencies and different times).
 And this spectogram is common preprocessing step for some algorithm, and
 a human ear is doing similar 
\begin_inset Quotes eld
\end_inset

preprocessing step
\begin_inset Quotes erd
\end_inset

.

\series bold
 Using a spectogram and optionally a 1D conv layer is a common pre-processing
 step prior to passing audio data to an RNN, GRU or LSTM.
\end_layout

\begin_layout Itemize

\series bold
Attention models
\series default
 (see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Sequence-Models"
plural "false"
caps "false"
noprefix "false"

\end_inset

) can be used for speech recognition.
 Or an alternative, 
\series bold
CTC models
\series default
 (see below) also work well.
 In such end-to-end learning, there is no more need for phonemes (hand-engineere
d basic units of sound, historically very important, but were replaced by
 ANN with a lot of data - 100k hours in commercial systems for example,
 and maybe 300-3k hours in academia).
\end_layout

\begin_layout Itemize

\series bold
CTC
\series default
 (Connectionist temporal classification) 
\series bold
cost
\series default
 (from 2006) is used for speech recognition problems.
 An idea is to use RNN (in practice usually bidirectional LSTM) with many-to-man
y architecture when the number of input and output size is the same.
 
\end_layout

\begin_deeper
\begin_layout Itemize
But notice that the number of timesteps is very large and in speech recognition,
 usually the 
\series bold
number of input time steps is much bigger than the number of output timesteps
\series default
.
 So, for example, if you have 10 seconds of audio and your features come
 at a 100 hertz.
 So, 100 samples per second then 10 second audio clip would end up with
 a 1k inputs.
 But your output might not have a 1k characters.
 
\end_layout

\begin_layout Itemize
CTC allows to generate output like this 
\begin_inset Quotes eld
\end_inset

ttt_h_eee___[ ]___qqq__ ...
 
\begin_inset Quotes eld
\end_inset

 ('[ ]' denotes space character) for input 
\begin_inset Quotes eld
\end_inset

The quick ...
\begin_inset Quotes erd
\end_inset

.
 Basic rule for CTC cost function is to collapse repeated characters not
 separated by "blank" (underscore '_' character in this example).
 
\end_layout

\end_deeper
\begin_layout Itemize
Trigger word detection system can be done with a smaller amount of data.
 See the next figure for more details.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/trigger_word_detection.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of trigger word detection system.
 RNN can be used on an audio clip (or spectogram).
 This is a supervised learning task, and where some trigger word ends, here
 a sequence of '1' symbols starts.
 When an trigger word starts, then a sequence of '1's ends and starts a
 sequence of '0's.
 
\begin_inset Formula $x^{<t>}$
\end_inset

 are features of the audio (such as spectogram features) at time 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Description
Association
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

rule
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

learning
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

algorithms: association rule learning methods extract rules that best explain
 observed relationships between variables in data.
\end_layout

\begin_layout Description
Training
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

sample: a training sample (or training instance or training example) is
 a 
\series bold
data point x
\series default
 in an available training set that we use for working on a predictive modeling
 task.
 For example, if we are interested in classifying emails, one email in our
 dataset would be one training sample.
\end_layout

\begin_layout Description
Skewed
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

classes: if there are for instance 2 classes, and one is significantly bigger,
 for example we have 
\begin_inset Formula $1\%$
\end_inset

 of examples in the first class, and 
\begin_inset Formula $99\%$
\end_inset

 of examples in the second one.
\end_layout

\begin_layout Description
Target
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

function: in predictive modeling, we are typically interested in modeling
 a particular process; we want to learn or approximate a particular function
 that, for example, let's us distinguish spam from non-spam email.
 The target function f(x) = y is the 
\series bold
true function
\series default
 f that 
\series bold
we
\series default
 
\series bold
want to model
\series default
.
\end_layout

\begin_layout Description
Hypothesis
\end_layout

\begin_layout Itemize
A hypothesis is a certain function (denoted as 
\begin_inset Quotes eld
\end_inset

h
\begin_inset Quotes erd
\end_inset

 on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "img:hypothesis"
plural "false"
caps "false"
noprefix "false"

\end_inset

) that we believe (or hope) 
\series bold
is similar to the true function
\series default
, the target function that we want to model.
 
\end_layout

\begin_layout Itemize
In context of email spam classification, it would be the rule we came up
 with that allows us to separate spam from non-spam emails.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/hypothesis.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Hypothesis
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "img:hypothesis"

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Example 
\series default
of a concrete hypothesis for univariate linear regression: 
\begin_inset Formula $h(x)=50+0.1x$
\end_inset


\end_layout

\begin_layout Itemize
For binary classification problems: the decision boundary is the line that
 separates the area where y = 0 and where y = 1.
 It is created by our hypothesis function.
\end_layout

\begin_layout Description
Model: in machine learning field, the terms hypothesis and model are often
 used 
\series bold
interchangeably
\series default
.
 
\end_layout

\begin_layout Description
Model
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

types: from here
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-g
enerative-and-discriminative-algorithm}}
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Generative models
\series default
 (e.g.
 Gaussian Mixture model, Hidden Markov Models, Naive Bayes, Restricted Boltzmann
 Machine, Generative Adversarial networks) generate all values for a given
 phenomenon.
 Discriminative models infer outputs based on inputs, while generative models
 generate both inputs and outputs, typically given some hidden parameters.
 They are trying to model a probability distribution of data.
 They are trying to find such distribution from which similar data could
 be generated.
 If the observed data are truly sampled from the generative model, then
 fitting the parameters of the generative model to maximize the data likelihood
 is a common method (Maximum Likelihood estimation - just multiplication
 of conditional probabilities and looking for maximization).
 
\series bold
They learn JOINT probability distribution
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Discriminative models
\series default
 (e.g.
 Linear Regression, Logistic Regression, SVM, Perceptron, Neural Networks,
 or Random Forests) work differently, we don't model probability distribution
 of data, but they are trying directly to model something for distinguishing
 classes - direct estimation of probabilities if an item is in one class
 or another.
 They can use regularization (penalization for model complexity, it prevents
 overfitting).
 They are inherently supervised and cannot easily support unsupervised.
 
\series bold
They learn CONDITIONAL probability distribution.
\end_layout

\begin_layout Description
Probabilistic
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Graphic
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Model (PGM): representation of conditional independent relationships between
 the nodes (random variables).
 The most important characteristic of PGMs is the ability of being translated
 from a graph to a joint distribution.
 These models can be discrete or continuous.
\end_layout

\begin_layout Description
Classifier: this is a 
\series bold
special case of a hypothesis
\series default
 (nowadays, often learned by a machine learning algorithm).
 A classifier is a hypothesis or 
\series bold
discrete-valued function
\series default
 that is used to 
\series bold
assign (categorical) class labels
\series default
 
\series bold
to particular data points
\series default
.
 
\end_layout

\begin_layout Itemize
In the email classification example, this classifier could be a hypothesis
 for labeling emails as spam or non-spam.
 
\end_layout

\begin_layout Itemize
However, a hypothesis must not necessarily be synonymous to a classifier.
 In a different application, our hypothesis could be a function for mapping
 study time and educational backgrounds of students to their future SAT
 scores.
\end_layout

\begin_layout Itemize

\series bold
A classier is a function that maps an unlabeled instance to a label using
 internal data structures.

\series default
 
\series bold
An inducer
\series default
 (an induction algorithm), 
\series bold
builds a classifier from a given dataset
\series default
 [6].
 Why inducer? Because learners do 
\series bold
induction
\series default
 - from some particular observations or evidences, create something more
 general - some larger hypothesis (from data to knowledge).
 This is an opposite to 
\series bold
deduction
\series default
, from some general knowledge to some specific (from knowledge to data).
\end_layout

\begin_layout Description
The
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Hadamard
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

product: also known as 
\series bold
Schur product
\series default
, is simply element-wise multiplication of two vectors of the same dimension.
\end_layout

\begin_layout Description
Arg
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Max
\end_layout

\begin_layout Itemize
Given a set of values 
\begin_inset Formula $A=\{a_{1},a_{2},...,a_{n}\}$
\end_inset

, the operator 
\begin_inset Formula $max_{a\in A}f(a)$
\end_inset

 returns the highest value 
\begin_inset Formula $f(a)$
\end_inset

 for all elements in the set 
\begin_inset Formula $A$
\end_inset

 (so, 
\begin_inset Formula $max$
\end_inset

 returns the biggest value of a function result).
\end_layout

\begin_layout Itemize
On the other hand, the operator 
\begin_inset Formula $arg\,max_{a\in A}f(a)$
\end_inset

 returns such element of the set 
\begin_inset Formula $A$
\end_inset

, that maximizes 
\begin_inset Formula $f(a)$
\end_inset

 (so, 
\begin_inset Formula $arg\,max$
\end_inset

 returns the input value to some function, which is maximized because of
 this value; but we want to know what caused it).
\end_layout

\begin_layout Description
Derivative
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

and
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Gradient
\end_layout

\begin_layout Itemize

\series bold
A derivative
\series default
 
\begin_inset Formula $f'$
\end_inset

 of function 
\begin_inset Formula $f$
\end_inset

 is a function or a value that describes how fast 
\begin_inset Formula $f$
\end_inset

 grows (or decreases).
 If the derivative is negative at some point 
\begin_inset Formula $x$
\end_inset

, that the function decreases at this point; if derivative is positive at
 some point, then the function increases there, and if the derivative is
 
\begin_inset Formula $0$
\end_inset

 at 
\begin_inset Formula $x$
\end_inset

, that means that the function's slope at 
\begin_inset Formula $x$
\end_inset

 is horizontal (local or global optima).
\end_layout

\begin_layout Itemize
The process of finding a derivative is called 
\series bold
differentiation
\series default
.
\end_layout

\begin_layout Itemize
Derivatives for basic functions are known, and it a function is not basic,
 we can find its derivative using the 
\series bold
chain rule
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Gradient
\series default
 is the generalization of derivatives for functions that take several inputs
 (or one input in the form of a vector or some other complex structure).
 
\series bold
So gradient of a function is a vector of partial derivatives
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Gradient of function 
\begin_inset Formula $f$
\end_inset


\series default
, denoted as 
\begin_inset Formula $\nabla f$
\end_inset

 is given by the vector 
\begin_inset Formula $[\frac{\partial f}{\partial x^{(1)}},\frac{\partial f}{\partial x^{(2)}}]$
\end_inset

 where 
\begin_inset Formula $\frac{\partial f}{\partial x^{(1)}}$
\end_inset

(similarly for 
\begin_inset Formula $\partial x^{(2)}$
\end_inset

) is partial derivative of function 
\begin_inset Formula $f$
\end_inset

 with respect to 
\begin_inset Formula $x^{(1)}$
\end_inset

.
\end_layout

\begin_layout Description
Bag
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

of
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

words
\end_layout

\begin_layout Itemize
Text converted to a feature vector (this vector is called bag of words)
 by taking dictionary of English words (sorted alphabetically and having
 20k words) and using it.
\end_layout

\begin_layout Itemize
For example, the first feature is equal to 1 if the email message contains
 the word 
\begin_inset Quotes eld
\end_inset

a
\begin_inset Quotes erd
\end_inset

, otherwise this feature is 0.
 Similarly for second word, and the algorithm will continue with the third
 word from dictionary, and so on until 20,000th word.
 Each data sample will thus have dimensionality of 20k with information
 about presence of a given word or not.
\end_layout

\begin_layout Description
Neural
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

style
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

transfer: you want to recreate an image according to some painting for instance.
 For estimating how good is a generated image, we need custom cost function.
 Idea from 2015.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J(G)=\alpha.J_{content}(C,G)+\beta J_{style}(S,G)\label{eq:neural_style_transfer_cost_function}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
where these two cost functions inside measures a similarity between input
 images, and 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 are hyperparameters to specify a relative weighting between content cost
 and style cost.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $J_{content}=\frac{1}{2}||a^{[l](C)}-a^{[l](G)}||^{2}$
\end_inset

 is calculated as a similarity between activation of layer 
\begin_inset Formula $l$
\end_inset

 on the images 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

.
 If these activations are similar, both images have similar content.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $J_{style}$
\end_inset

 is calculated as correlation between activations across different channels
 in layer 
\begin_inset Formula $l$
\end_inset

 (how often they occur together).
\end_layout

\begin_layout Itemize
Following the original NST paper, it can be used the VGG network.
 Specifically, VGG-19 (it is CNN), a 19-layer version of the VGG network.
 This model has already been trained on the very large ImageNet database,
 and thus has learned to recognize a variety of low level features (at the
 earlier layers) and high level features (at the deeper layers).
\end_layout

\begin_layout Itemize
How do you ensure the generated image G matches the content of the image
 C?
\end_layout

\begin_deeper
\begin_layout Itemize
As we saw in lecture, the earlier (shallower) layers of a ConvNet tend to
 detect lower-level features such as edges and simple textures, and the
 later (deeper) layers tend to detect higher-level features such as more
 complex textures as well as object classes.
\end_layout

\begin_layout Itemize
We would like the "generated" image G to have similar content as the input
 image C.
 Suppose you have chosen some layer's activations to represent the content
 of an image.
 In practice, you'll get the most visually pleasing results if you choose
 a layer in the middle of the network--neither too shallow nor too deep.
 
\end_layout

\begin_layout Itemize
So, the content cost takes a hidden layer activation of the neural network,
 and measures how different 
\begin_inset Formula $a^{(C)}$
\end_inset

 and 
\begin_inset Formula $a^{(G)}$
\end_inset

 are.
 When we minimize the content cost later, this will help make sure 
\begin_inset Formula $G$
\end_inset

 has similar content as 
\begin_inset Formula $C$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/neural_style_transfer_unrolling.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Unrolling step for easier computation (not a neccessary step though).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Style matrix
\end_layout

\begin_deeper
\begin_layout Itemize
The style matrix is also called a "Gram matrix." In linear algebra, the Gram
 matrix G of a set of vectors 
\begin_inset Formula $(v_{1},…,v_{n})$
\end_inset

 is the matrix of dot products, whose entries are 
\begin_inset Formula $G_{ij}=v_{i}^{T}v_{j}=np.dot(v_{i},v_{j})$
\end_inset

.
 In other words, 
\begin_inset Formula $G_{ij}$
\end_inset

 compares how similar 
\begin_inset Formula $v_{i}$
\end_inset

 is to 
\begin_inset Formula $v_{j}$
\end_inset

: If they are highly similar, you would expect them to have a large dot
 product, and thus for 
\begin_inset Formula $G_{ij}$
\end_inset

 to be large.
 The value 
\begin_inset Formula $G_{ij}$
\end_inset

 measures how similar the activations of filter 
\begin_inset Formula $i$
\end_inset

 are to the activations of filter 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Itemize
One important part of the gram matrix is that the diagonal elements such
 as 
\begin_inset Formula $G_{ii}$
\end_inset

 also measures how active filter 
\begin_inset Formula $i$
\end_inset

 is.
 For example, suppose filter ii is detecting vertical textures in the image.
 Then 
\begin_inset Formula $G_{ii}$
\end_inset

 measures how common vertical textures are in the image as a whole: If 
\begin_inset Formula $G_{ij}$
\end_inset

 is large, this means that the image has a lot of vertical texture.
\end_layout

\begin_layout Itemize
By capturing the prevalence of different types of features (
\begin_inset Formula $G_{ii}$
\end_inset

), as well as how much different features occur together (
\begin_inset Formula $G_{ij}$
\end_inset

), the Style matrix 
\begin_inset Formula $G$
\end_inset

 measures the style of an image.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/neural_transfer_gram_matrix.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Unrolling step and computation of Gram (Style) Matrix.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The next step is to compute Style cost (computed similarly as content cost).
 The goal will be to minimize the distance between the Gram matrix of the
 "style" image 
\begin_inset Formula $S$
\end_inset

 and that of the "generated" image 
\begin_inset Formula $G$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The style of an image can be represented using the Gram matrix of a hidden
 layer's activations.
 However, we get even better results combining this representation from
 multiple different layers.
 This is in contrast to the content representation, where usually using
 just a single hidden layer is sufficient.
 
\end_layout

\begin_layout Itemize
Minimizing the style cost will cause the image 
\begin_inset Formula $G$
\end_inset

 to follow the style of the image 
\begin_inset Formula $S$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The total cost is a linear combination of the content cost 
\begin_inset Formula $J_{content}(C,G)$
\end_inset

 and the style cost 
\begin_inset Formula $J_{style}(S,G)$
\end_inset

.
 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 are hyperparameters that control the relative weighting between content
 and style.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/neural_style_transfer.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Two examples of neural style transfer between an image and piccasso paintings.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/neural_style_transfer_gradient_descent.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Computation of gradient descent using neural style transfer between an image
 and piccasso paintings.
 As you run gradient descent, you minimize the cost function 
\begin_inset Formula $J(G)$
\end_inset

 slowly through the pixel value so then you get slowly an image that looks
 more and more like your content image rendered in the style of your style
 image.
 So, just to be clear, we initialize the "generated" image as a noisy image
 created from the content_image.
 By initializing the pixels of the generated image to be mostly noise but
 still slightly correlated with the content image, this will help the content
 of the "generated" image more rapidly match the content of the "content"
 image.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/neural_style_transfer_style_matrix.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Computation of style matrix for neural style transfer.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/neural_style_transfer_style_cost_function.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Computation of style cost function for particular layer 
\begin_inset Formula $l$
\end_inset

 for neural style transfer.
 So, the pixel values of generated image G are updated in each iteration
 of the optimization algorithm.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Description
Handling
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

multiple
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

inputs
\end_layout

\begin_layout Itemize
Often in practice, you will work with multimodal data.
 For example, your input could be an image and text and the binary output
 could indicate whether the text describes this image.
 It's hard to adapt shallow learning algorithms to work with multimodal
 data.
\end_layout

\begin_layout Itemize
You could train one shallow model on the image and another one on the text.
 Then you can use a model combination technique using ensemble learning.
 If you cannot divide your problem into two independent sub-problems, you
 can try to vectorize each input (by applying the corresponding feature
 engineering method) and then simply concatenate two feature vectors together
 to form one wider feature vector.
 For example, if your image has features 
\begin_inset Formula $[i(1),\,i(2),\,\,\,\,\,\,i(3)]$
\end_inset

 and your text has features 
\begin_inset Formula $[t(1),\,t(2),\,t(3),\,t(4)]$
\end_inset

 your concatenated feature vector will be 
\begin_inset Formula $[i(1),\,i(2),\,i(3),\,t(1),\,t(2),\,t(3),\,t(4)]$
\end_inset

.
\end_layout

\begin_layout Itemize
With neural networks, you have more flexibility.
 You can build two subnetworks, one for each type of input.
 For example, a CNN subnetwork would read the image while an RNN subnetwork
 would read the text.
 Both subnetworks have as their last layer an embedding: CNN has an embedding
 of the image, while RNN has an embedding of the text.
 You can now concatenate two embeddings and then add a classification layer,
 such as softmax or sigmoid, on top of the concatenated embeddings.
 Neural network libraries provide simple to use tools that allow concatenating
 or averaging layers from several subnetworks.
\end_layout

\begin_layout Description
Topic
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Modeling
\end_layout

\begin_layout Itemize
In text analysis, this is a prevalent 
\series bold
unsupervised learning problem
\series default
.
 
\series bold
You have a collection of text documents, and you would like to discover
 topics present in each document
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Latent Dirichlet Allocation (LDA)
\series default
 is a very effective algorithm of topic discovery.
 You decide how many topics are present in your collection of documents,
 and the algorithm assigns a topic to each word in this collection.
 Then, to extract the topics from a document, you simply count how many
 words of each topic are present in that document.
\end_layout

\begin_layout Description
Probabilistic
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Graphical
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Models
\end_layout

\begin_layout Itemize

\series bold
Conditional random fields
\series default
 (CRF) is one example of probabilistic graphical models (PGMs).
 With CRF you can model the input sequence of words and relationships between
 the features and labels in this sequence as a sequential dependency graph.
\end_layout

\begin_layout Itemize
More generally, a PGM can be any graph.
 A graph is a structure consisting of a collection of nodes and edges that
 join a pair of nodes.
 Each node in PGM represents some random variable (values of which can be
 observed or unobserved), and edges represent the conditional dependence
 of one random variable on another random variable.
 For example, the random variable “sidewalk wetness” depends on the random
 variable “weather condition”.
 By observing values of some random variables, an optimization algorithm
 can learn from data the dependency between observed and unobserved variables.
 If you decide to learn more about PGMs, they are also known under names
 of 
\series bold
Bayesian networks, belief networks
\series default
, and 
\series bold
probabilistic independence networks
\series default
.
\end_layout

\begin_layout Description
Markov
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Chain
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Monte
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Carlo
\end_layout

\begin_layout Itemize
If you work with graphical models and want to sample examples from a very
 complex distribution defined by the dependency graph, you could use Markov
 Chain Monte Carlo (MCMC) algorithms.
\end_layout

\begin_layout Itemize
MCMC is a class of algorithms for sampling from any probability distribution
 defined mathematically.
 Sampling from standard distributions, such as normal or uniform, is relatively
 easy because their properties are well known.
 However, the task of sampling becomes significantly more complicated when
 the probability distribution can have an arbitrary form defined by a complex
 formula.
\end_layout

\begin_layout Description
Handling
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

multiple
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

outputs
\end_layout

\begin_layout Itemize
In some problems, you would like to predict multiple outputs for one input.
 Some problems with multiple outputs can be effectively converted into a
 multi-label classification problem.
 Especially those that have labels of the same nature (like tags) or fake
 labels can be created as a full enumeration of combinations of original
 labels.
 However, in some cases the outputs are multimodal, and their combinations
 cannot be effectively enumerated.
\end_layout

\begin_layout Itemize
Consider the following example: you want to build a model that detects an
 object on an image and returns its coordinates.
 In addition, the model has to return a tag describing the object, such
 as “person,” “cat,” or “hamster.” Your training example will be a feature
 vector that represents an image.
 The label will be represented as a vector of coordinates of the object
 and another vector with a one-hot encoded tag.
\end_layout

\begin_deeper
\begin_layout Itemize
To handle a situation like that, you can create one subnetwork that would
 work as an encoder.
 It will read the input image using, for example, one or several convolution
 layers.
 The encoder's last layer would be the embedding of the image.
 Then you add two other subnetworks on top of the embedding layer:
\end_layout

\begin_deeper
\begin_layout Itemize
The first subnetwork will take the embedding vector as input and predicts
 the coordinates of an object.
 It can have a ReLU as the last layer, which is a good choice for predicting
 positive real numbers, such as coordinates; this subnetwork could use the
 mean squared error cost 
\begin_inset Formula $C_{1}$
\end_inset

.
\end_layout

\begin_layout Itemize
The second subnetwork will take the same embedding vector as input and predict
 the probabilities for each tag.
 It can have a softmax as the last layer, which is appropriate for the probabili
stic output, and use the averaged negative log-likelihood cost 
\begin_inset Formula $C_{2}$
\end_inset

 (also called cross-entropy cost).
\end_layout

\end_deeper
\begin_layout Itemize
Obviously, you are interested in both accurately predicted coordinates and
 the tags.
 However, it is impossible to optimize the two cost functions at the same
 time.
 By trying to optimize one, you risk hurting the second one and the other
 way around.
 What you can do is add another hyperparameter 
\begin_inset Formula $γ$
\end_inset

 in the range 
\begin_inset Formula $(0,1)$
\end_inset

 and define the combined cost function as 
\begin_inset Formula $γC_{1}+(1−γ)C_{2}$
\end_inset

.
 Then you tune the value for 
\begin_inset Formula $γ$
\end_inset

 on the validation data just like any other hyperparameter.
 .
\end_layout

\end_deeper
\begin_layout Description
End-to-end
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

learning
\end_layout

\begin_layout Itemize

\series bold
Sentiment classification 
\end_layout

\begin_deeper
\begin_layout Itemize
The problem of recognizing positive vs.
 negative opinions.
 For example, examining online product reviews if the writer liked or disliked
 a product.
\end_layout

\begin_layout Itemize
One of the problems of sentiment classification is, that there is usually
 not enough labeled data.
 But with word embeddings, far less data are needed.
\end_layout

\begin_layout Itemize
There are multiple algorithms using word embeddings:
\end_layout

\begin_deeper
\begin_layout Itemize
From a given sentiment, get a number (position in dictionary) for each word
 and create a one-hot representation of them, and for each - multiply this
 one-hot vector by 
\begin_inset Formula $E$
\end_inset

 (which you can learn from much bigger corpus), and then you have embedding
 vector (for each word).
 Then, the algorithm averages all embedding vectors (from a given sentiment)
 which result is given to a softmax classifier (over 5 possible outcomes,
 if a given sentiment can be rated by 1 to 5 stars for instance).
 However, this algorithm completely lacks an order of the words in a given
 sentiment.
\end_layout

\begin_layout Itemize
So, instead of using some averaging (previous example/algorithm), RNN can
 be used with many-to-one architecture.
 
\begin_inset Quotes eld
\end_inset

Many
\begin_inset Quotes erd
\end_inset

 are basically all embedding vectors from the input sentiment, 
\begin_inset Quotes eld
\end_inset

one
\begin_inset Quotes erd
\end_inset

 is softmax.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
To build a system for sentiment classification, it is needed to build a
 
\series bold
pipeline
\series default
 
\series bold
of 2 components:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Parser
\series default
.
 A system that annotates the text with information identifying the most
 important words.
 For example, the parser might used to label all the adjectives and nouns.
\end_layout

\begin_layout Itemize

\series bold
Sentiment classifier
\series default
.
 A learning algorithm that takes as input the annotated text and predicts
 the overall sentiment.
 The parser’s annotation could help this learning algorithm greatly: By
 giving adjectives a higher weight, your algorithm will be able to quickly
 hone in on the important words such as “great” and ignore less important
 words such as 
\begin_inset Quotes eld
\end_inset

this
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/sentiment_classification_pipeline.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
For sentiment classification system it is needed to have pipeline of 2 tasks,
 as it is displayed on this figure.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Or another example, speech recognition system - pipeline may have 3 components:
 1) 
\series bold
extract hand-designed features
\series default
 (MFCC), phoneme recognizer, and then 2)
\series bold
 final recognizer.

\series default
 This and the previous 
\series bold
pipelines were linear
\series default
 - the input is sequentially passed from one staged to the next.
 However pipelines can be more complex, like it is depicted on the figure
 below.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/complex_mlsystem_pipeline_example.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of architecture of system for an autonomous cat detection.
 A final component plans a path for a new car that avoids other cars and
 pedestrians.
 It should be noted, that not every component in a pipeline has to be learned.
 Many algorithms do not involve learning.
 Additionally, each of these 3 components is a relatively simple function
 - and can thus be learned with less data than the purely end-to-end approach
 that requires a lot of data.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
However, recent trend is toward replacing such pipeline systems with a single
 learning algorithm - an 
\series bold
end-to-end learning algorithm
\series default

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{The term 
\begin_inset Quotes eld
\end_inset

end-to-end
\begin_inset Quotes erd
\end_inset

 refers to the fact that we are asking the learning algorithm to go directly
 from the input to the desired output.}
\end_layout

\end_inset

.
 The algorithm would simply take as input the raw original text, and try
 to directly recognize the sentiment.
 ANN are commonly used in end-to-end learning systems.
\end_layout

\begin_layout Itemize

\series bold
Feature engineering simplifies the input and throws some information away,
\series default
 which can limit the overall system's performance.
 End-to-end learning is usable if data is abundant.
 If the learning algorithm is a large-enough neural network and if it is
 trained with enough training data, it has the potential to do very well,
 and perhaps even approach the optimal error rate.
\end_layout

\begin_layout Itemize

\series bold
However, end-to-end learning is not always a good choice.
 
\series default
Allowing 
\series bold
hand-engineered components also has some advantages.
 They help simplify the problem for the learning algorithm.
 It is a way how to manually inject knowledge into the system.
\end_layout

\begin_deeper
\begin_layout Itemize
For example, in speech recognition system, to recognize phonemes with a
 separate module in the pipeline can also help the learning algorithm understand
 basic sound components (because phonemes are a reasonable representation
 of speech) and therefore improve the system's performance.
 
\end_layout

\begin_layout Itemize
Having more hand-engineered components generally allows a speech system
 to learn with less data.
 When we don't have much data, this knowledge is useful.
 If you are working on a machine learning problem where the training set
 is very small, most of your algorithm's knowledge will have to come from
 your human insight (i.e., from your “hand engineering” components).
\end_layout

\end_deeper
\begin_layout Itemize
When to choose - 
\begin_inset Quotes eld
\end_inset

pipelines
\begin_inset Quotes erd
\end_inset

 approach vs end-to-end learning? It's mostly about data.
\end_layout

\begin_deeper
\begin_layout Itemize
For example, for end-to-end learning system of driving autonomous car, it
 is needed to have a fleet of specially-instrumented cars (for having steering
 directions captured), and a huge amount of driving to cover a wide range
 of possible scenarios.
 On the other hand, data with pedestrians and cards as images are easier
 to obtain.
\end_layout

\begin_layout Itemize
In summary, when deciding what should be the components of a pipeline, try
 to build a pipeline where each component is a relatively “simple” function
 that can therefore be learned from only a modest amount of data.
\end_layout

\end_deeper
\begin_layout Itemize
New trend in end-to-end learning, is not just to predict some real number
 or an integer (or class), but it is letting us directly
\series bold
 learn outputs which are much more complex than a number.
 
\end_layout

\begin_deeper
\begin_layout Itemize
For example, for an image where 
\begin_inset Formula $x$
\end_inset

 is a yellow bus with trees, a learning algorithm would produce 
\begin_inset Formula $y$
\end_inset

 = 
\begin_inset Quotes eld
\end_inset

A yellow bus driving down a road with green trees and green grass in the
 background.
\begin_inset Quotes erd
\end_inset

 
\end_layout

\end_deeper
\begin_layout Description
Bleu
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Score
\end_layout

\begin_layout Itemize
Bilingual evaluation understudy (from 2002) - given a machine generated
 translation, it allows you to automatically compute a score that measures
 
\series bold
how good is a given machine translation
\series default
 (if you have 2 good translations and want to decide what would person prefer
 - these 2 are equally good answer).
\end_layout

\begin_layout Itemize
Central idea - quality is considered to be the correspondence between a
 machine's output and that of a human: "the closer a machine translation
 is to a professional human translation, the better it is".
 
\series bold
It is a single number evaluation metric.
 
\series default
That is why Bleu score was revolutionary in machine translation.
 But it is also used in image captioning system.
 It is not used in speech recognition as there we have an exact representation
 of the ground truth.
\end_layout

\begin_layout Itemize
Output is always a number between 0 and 1.
 This value indicates how similar the candidate text is to the reference
 texts, with values closer to 1 representing more similar texts.
\end_layout

\begin_layout Itemize
We will give each word credit only up to the maximum number of times it
 appears in the all reference sentences.
 
\series bold
Bleu score is a modified precision defined on n-grams
\series default
 (not just separate words, but maybe pairs - bigrams, or triplets, and so
 on), (
\begin_inset Formula $\hat{y}$
\end_inset

is output from machine translation):
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p_{n}=\frac{\sum_{n-grams\in\hat{y}}Count_{clip}(n-gram)}{\sum_{n-grams\in y}Count\,(n-gram)}\label{eq:bleu_score}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
If 
\begin_inset Formula $p_{n}=1$
\end_inset

 then you are exactly equal to one of the human (translation) references.
\end_layout

\begin_layout Itemize
By convention, you compute 
\begin_inset Formula $p_{1}$
\end_inset

, 
\begin_inset Formula $p_{2}$
\end_inset

, 
\begin_inset Formula $p_{3},$
\end_inset

 and 
\begin_inset Formula $p_{4}$
\end_inset

, and then you compute 
\series bold
the average of the results
\series default
 and then 
\series bold
final Bleu score
\series default
 is defined as 
\begin_inset Formula $BP*exp^{avg}$
\end_inset

.
 
\begin_inset Formula $BP$
\end_inset

 is a parameter and stands for so called 
\series bold
brevity penalty
\series default
.
 This is due to observation, that shorter translations have bigger 
\begin_inset Formula $p_{n}$
\end_inset

 score, and we don't want to always output very short translations.
 BP penalizes translations which are very short.
 BP = 1 if output of machine translation is bigger than human reference
 output.
 Otherwise, it is equal to:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
exp(1-\frac{machine\,translation\,output\,length}{human\,reference\,output\,length})\label{eq:bleu_score_brevity_penalty}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/bleu_score.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of Bleu score on bigrams.
 Resulting score is 4/6.
 
\series bold
Count 
\series default
means number of bigrams from machine translation.
 
\series bold
Count clip 
\series default
means the number of maximum bigrams appearing in 1 from all references.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Description
Model
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

error: bias + variance.
 
\end_layout

\begin_layout Itemize

\series bold
Bias
\series default
 is a learner's tendency to consistently learn the same wrong thing.
 It is a systematic error; the 
\series bold
average error
\series default
 for 
\series bold
different training data
\series default
.
 Can be calculated.
\end_layout

\begin_layout Itemize

\series bold
Variance
\series default
 if the tendency to learn random things irrespective of the real signal.
 It indicates how 
\series bold
sensitive
\series default
 is model
\series bold
 to variously changing training subsets of data from the whole
\series default
 
\series bold
dataset
\series default
.
 Can be calculated.
\end_layout

\begin_layout Description
The
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

noise: a property of data itself.
 Can be calculated.
 
\end_layout

\begin_layout Itemize
k-NN is generally considered intolerant of noise; its similarity measures
 can be easily distorted by errors in attribute values, thus leading it
 to mis-classify a new instance on the basis of the wrong nearest neighbors.
 
\end_layout

\begin_layout Itemize
Contrary to k-NN, rule learners and most decision trees are considered resistant
 to noise because their pruning strategies avoid overfitting the data in
 general and noisy data in particular.
\end_layout

\begin_layout Description
Loss
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

function: usually a function defined on a data point, prediction and label,
 and measures the penalty.
\end_layout

\begin_layout Description
Cost
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

function: measures the accuracy of our hypothesis function.
 
\end_layout

\begin_layout Itemize
It is usually more general.
 It might be a sum of loss functions over your training set plus some model
 complexity penalty (regularization).
 
\end_layout

\begin_layout Itemize
We want to minimize difference between predicted and true value in the whole
 dataset (each data point), so 
\series bold
we want to minimize a cost function.

\series default
 
\end_layout

\begin_layout Itemize
For example - 
\series bold
Squared error (cost) function
\series default
 (one of the best candidate for cost functions in regression problems, see
 Definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lr_cost_function"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for Linear regression with 1 variable).
\end_layout

\begin_deeper
\begin_layout Itemize
However, 
\series bold
Squared error function with a logistic unit has also some drawbacks.

\series default
 If the desire output is 1, and actual output is 0.000000001, then there
 is almost no gradient for logistic unit to fix up the error.
 Slope is almost exactly horizontal.
 So it will take a very very long time to change the weights (regarding
 ANN).
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $E=\frac{1}{2}\text{​}(y\text{−}t)^{2}$
\end_inset

, where 
\begin_inset Formula $y=\sigma(z)=\frac{1}{1+exp(-z)}$
\end_inset

 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
derivatives tend to "plateau-out" when 
\begin_inset Formula $y$
\end_inset

 is close to 
\begin_inset Formula $0$
\end_inset

 or 
\begin_inset Formula $1$
\end_inset

: 
\begin_inset Formula $\frac{dz}{dE}=(y-t)*y*(1-y)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Also, if we are trying to assign probabilities to mutually exclusive class
 labels, we know that the outputs should sum to 1, but we are depriving
 the network of this knowledge (for example, if we know that the probability
 of this is A is 3/4, and the probability that it's a B is also 3/4 is just
 crazy answer).
\end_layout

\begin_layout Itemize
For classification (not for regression), there is an alternative - force
 the outputs to represent a probability distribution across discrete alternative
s (classes).
\end_layout

\end_deeper
\begin_layout Itemize
Cost functions can be visualized using a contour plots
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{A contour plot is a graph that contains many contour lines.
 A contour line of a two variable function has a constant value at all points
 of the same line.
 In other words, cost function for different parameters has the same results
 for these parameters even they are different, and we can see them on the
 same line in a countour plot.}
\end_layout

\end_inset

.
 We can easily see on these plots a global minimum (minimal cost function).
\end_layout

\begin_layout Description
Objective
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

function: is the most general term for any function that you optimize during
 training - it is basically a mathematical expression we are trying to minimize
 or maximize.
 
\series bold
A loss function is a part of a cost function which is a type of an objective
 function.
 All model-based learning algorithms have a loss function and what we do
 to find the best model is we try to minimize the objective known as cost
 function.
\end_layout

\begin_layout Description
Decision
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

boundary: a boundary between 2 groups, it is some function.
 It is an ability of hypothesis and not a dataset!
\end_layout

\begin_layout Description
Learning
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

algorithm: again, our goal is to 
\series bold
find or approximate the target function
\series default
, and the learning algorithm is a set of instructions that tries to model
 the target function using our training dataset.
 A learning algorithm comes with a hypothesis space, the set of possible
 hypotheses it can come up with in order to model the unknown target function
 by formulating the final hypothesis.

\series bold
 
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/learning_alg_comparison.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of learning algorithms (**** stars represent the best and * star
 the worst performance).
 From [7].
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Description
Genetic
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Algorithms
\end_layout

\begin_layout Itemize
They 
\begin_inset Quotes eld
\end_inset

sort of
\begin_inset Quotes erd
\end_inset

 belong to machine learning, because these are numerical optimization technique,
 that are used to optimize non-differentiable optimization objective functions.
 They use concepts from evolutionary biology to search for a global optimum
 (minimum or maximum) of an optimization problem, by mimicking evolutionary
 biological processes.
\end_layout

\begin_layout Itemize
Genetic algorithms work by starting with an initial generation of candidate
 solutions.
 If we look for optimal values of the parameters of our model, we first
 randomly generate multiple combinations of parameter values.
 We then test each combination of parameter values against the objective
 function.
 Imagine each combination of parameter values as a point in a multi-dimensional
 space.
 We then generate a subsequent generation of points from the previous generation
 by applying such concepts as 
\series bold
selection
\series default
, 
\series bold
crossover
\series default
, and 
\series bold
mutation
\series default
.
 In a nutshell, that results in each new generation keeping more points
 similar to those points from the previous generation that performed the
 best against the objective.
 In the new generation, the points that performed the worst in the previous
 generation are replaced by “mutations” and “crossovers” of the points that
 performed the best.
\end_layout

\begin_layout Itemize
A 
\series bold
mutation
\series default
 of a point is obtained by a random distortion of some attributes of the
 original point.
\end_layout

\begin_layout Itemize
A 
\series bold
crossover
\series default
 is a certain combination of several points (for example, an average).
\end_layout

\begin_layout Itemize
Genetic algorithms allow finding solutions to any measurable optimization
 criteria.
 For example, GA can be used to optimize the hyperparameters of a learning
 algorithm.
 They are typically much slower than gradient-based optimization techniques
 or some other methods and they are not often used as components of machine
 learning algorithms - but there exists algorithms that incorporates them
 into neural networks for instance.
\end_layout

\begin_layout Description
Multi-Label
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Classification
\end_layout

\begin_layout Itemize
In some situation, more than one label is appropriate to describe an example
 from the dataset.
 For example, if we want to describe an image, we could assign several labels
 to it: 
\begin_inset Quotes eld
\end_inset

road
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

mountain
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

car
\begin_inset Quotes erd
\end_inset

 at the same time.
\end_layout

\begin_layout Itemize
We can transform each labeled example into several labeled examples, one
 per label.
 These new examples all have the same feature vector and only one label.
 That becomes a multi-class classification problem.
 The only difference with the usual multi-class problem is that now we have
 a new hyperparameter - threshold.
 If the prediction score for some label is above the threshold, this label
 is predicted for the input feature vector.
 In this scenario, multiple labels can be predicted for one feature vector.
\end_layout

\begin_layout Itemize
Neural networks can naturally train multi-label classification models by
 using binary cross-entropy cost function.
\end_layout

\begin_layout Description

\series bold
Learning
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

consists
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

of
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

the
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

following
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

3
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

components
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Representation
\series default
 - a classifier must be represented in some formal language that the computer
 can handle.
 Representable != learnable (eg.
 many local optimas in hypothesis space, learner may not find the true function
 even if it's representable).
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
Examples
\series default
:
\end_layout

\begin_layout Itemize
Instances (k-nearest neighbor, SVM).
\end_layout

\begin_layout Itemize
Hyperplanes (Naive Bayes, Logistic regression) form a linear combination
 of the features per class and predict the class with the highest-valued
 combination.
\end_layout

\begin_layout Itemize
Decision trees (CART) test 1 feature at each internal node, with 1 branch
 for each feature value, and have class predictions at the leaves.
\end_layout

\begin_layout Itemize
Set of rules (Propositional rules, Logic programs).
\end_layout

\begin_layout Itemize
Neural networks.
\end_layout

\begin_layout Itemize
Graphical models (Bayesian networks, Conditional random fields).
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Evaluation
\series default
 - an evaluation function (also called objective or scoring function) is
 needed to distinguish good classifiers from bad ones.
 The evaluation function used internally by the algorithm may differ from
 the external one, that we want the classifier to optimize.

\series bold
 
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
Examples
\series default
:
\end_layout

\begin_layout Itemize
Accuracy, error rate, and Squared error.
\end_layout

\begin_layout Itemize
Precision and recall.
\end_layout

\begin_layout Itemize
Likelihood.
\end_layout

\begin_layout Itemize
Posterior probability.
\end_layout

\begin_layout Itemize
Information gain.
\end_layout

\begin_layout Itemize
K-L divergence.
\end_layout

\begin_layout Itemize
Cost/utility.
\end_layout

\begin_layout Itemize
Margin.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Optimization
\series default
 - a methods to search among the classifiers in the language for the highest
 scoring one.
 The choice of optimization technique is key to the efficiency of the learner,
 and also helps determine the classifier produced if he evaluation function
 has more than 1 optimum.

\series bold
 
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
Examples
\series default
:
\end_layout

\begin_layout Itemize
Combinatorial optimization (Greedy search, Beam search, Branch-and-bound).
\end_layout

\begin_layout Itemize
Continuous optimization, which can be constrained (Linear or Quadratic programmi
ng), or unconstrained (Gradient descent, Conjugate gradient, Quasi-Newton
 methods).
\end_layout

\end_deeper
\end_deeper
\begin_layout Description
Building
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

blocks
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

of
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

a
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

learning
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

algorithm
\end_layout

\begin_layout Itemize

\series bold
Loss function.
\end_layout

\begin_layout Itemize

\series bold
Cost function 
\series default
- an optimization criterion based on the loss function.
\end_layout

\begin_layout Itemize

\series bold
Optimization routine
\series default
 leveraging training data
\series bold
 to find a solution to the optimization criterion
\series default
 (e.g.
 gradient descent is the most frequently used optimization algorithm in
 cases where the optimization criterion is differentiable).
\end_layout

\begin_layout Description
Covariate
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

shift: if you train your model with data with some distribution, and then
 you will try the model with data from different distribution.
 Maybe both data could have the same decision boundary and there could exist
 a model that would perform very well on both, but a learning algorithm
 cannot find that decision boundary from a training data.
\end_layout

\begin_layout Description
Edge
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

deployment - you put a processor right where the data is collected so that
 you can process the data and make a decision very quickly without needing
 to transmit the data over the Internet to be processed somewhere else (for
 example, self-driving cars).
 Other types of deployments of AI projects are on 
\series bold
cloud
\series default
 (if you rent compute servers such as from Amazon's AWS, or Microsoft's
 Azure, or Google's GCP in order to use someone else's service to do your
 computation), or 
\series bold
on-premises 
\series default
(buying your own compute servers and running the service locally in your
 own company).
\end_layout

\begin_layout Description
Corpus: terminology from NLP, which means basically a set of large body
 or a very large set of text of human-language (mostly English?) sentences.
 For NLP problems, first it is needed to tokenize the sentences in corpus
 (using vocabulary), and then map each word to one-hot vector.
 Also, a common thing is to model when a given sentence ends (adding extra
 token <EOS> - end of sentence).
 Words outside of your dictionary will be replaced by token <UNK> = unknown
 word.
\end_layout

\begin_layout Description
Kolmogorov
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Complexity: concept in the field of Information Theory.
 It says, that the 
\series bold
complexity of a learned function is the length of the shortest computer
 program that can produce that function
\series default
.
 This theoretical concept has found few practical applications in AI as
 well.
\end_layout

\begin_layout Description
Hyperparameters are properties of a learning algorithm, usually (but not
 always) having a numerical value.
 A value of a hyperparameter influences the way the algorithm works.
 Hyperparameters aren't learned by the algorithm itself from data.
 They have to be set by the data analyst before running the algorithm.
\end_layout

\begin_layout Description
Parameters are variables that define the model learned by the learning algorithm.
 Parameters are directly modified by the learning algorithm based on the
 training data.
 The goal of learning is to find such values of parameters that make the
 model optimal in a certain sense.
\end_layout

\begin_layout Description
Probability
\end_layout

\begin_layout Itemize

\series bold
Random variable 
\series default
(usually written as italic capital letter, like 
\begin_inset Formula $X$
\end_inset

), is a variable whose possible values are numerical outcomes of a random
 phenomenon.
 There are 2 types of random variables - 
\series bold
discrete
\series default
 (e.g.
 toss a coin - it takes only a countable number of distinct values), and
 
\series bold
continuous
\series default
 (e.g.
 height of the first stranger you meet outside).
\end_layout

\begin_layout Itemize

\series bold
Probability distribution
\series default
 of
\end_layout

\begin_deeper
\begin_layout Itemize
A 
\bar under
discrete random variable
\bar default
 is described by a list of probabilities associated with each of its possible
 values.
 This list of probabilities is called a 
\series bold
probability mass function
\series default
.
 For example, 
\begin_inset Formula $P(X=red)=0.3$
\end_inset

, 
\begin_inset Formula $P(X=yellow)=0.45$
\end_inset

, 
\begin_inset Formula $P(X=blue)=0.25$
\end_inset

.
 Each probability in a probability mass function is a value greater than
 or equal to 
\begin_inset Formula $0$
\end_inset

.
 The sum of probabilities equals 
\begin_inset Formula $1$
\end_inset

.
\end_layout

\begin_layout Itemize
A 
\bar under
continuous random variable
\series bold
\bar default
 
\series default
takes an infinite number of possible values in some interval.
 Because the number of values of a continuous random variable 
\begin_inset Formula $X$
\end_inset

 is infinite, the probability 
\begin_inset Formula $P(X=c)$
\end_inset

 for any 
\begin_inset Formula $c$
\end_inset

 is 
\begin_inset Formula $0$
\end_inset

.
 Therefore, instead of the list of probabilities, the probability distribution
 of a continuous random variable is described by a 
\series bold
probability density function
\series default
.
 It is a function whose co-domain
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{Funtion is a relation that associates each element x of a set X,
 the domain of the function, to a single element y of another set Y, the
 codomain of the function.}
\end_layout

\end_inset

 is non-negative and the area under the curve is equal to 
\begin_inset Formula $1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Expectation of a discrete random variable
\series default
 
\begin_inset Formula $X$
\end_inset

, that have 
\begin_inset Formula $k$
\end_inset

 possible values 
\begin_inset Formula $\{x_{i}\}_{i=1}^{k}$
\end_inset

 is denoted as 
\begin_inset Formula $E[X]$
\end_inset

 and is given by:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E[X]=\sum_{i=1}^{k}[x_{i}.P(X=x_{i})]=x_{1}.P(X=x_{1})+x_{2}.P(X=x_{2})+...+x_{k}.P(X=x_{k})\label{eq:expectation_discrete}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $P(X=x_{i})$
\end_inset

is the probability that 
\begin_inset Formula $X$
\end_inset

 has the value 
\begin_inset Formula $x_{i}$
\end_inset

 according to the probability mass function.
 The expectation of a random variable is also called the
\series bold
 mean
\series default
,
\series bold
 average
\series default
, or 
\series bold
expected value
\series default
, and is frequently denoted with the letter 
\begin_inset Formula $\mu$
\end_inset

.
 It is one of the most important statistics of a random variable.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Expectation of a continuous random variable
\series default
 
\begin_inset Formula $X$
\end_inset

 is given by
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E[X]=\int_{R}xf_{X}(x)dx\label{eq:expectation_continuous}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $f_{x}$
\end_inset

 is the probability density function of the variable 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\int_{R}$
\end_inset

is the integral of function 
\begin_inset Formula $xf_{X}$
\end_inset

.
 The property of the probability density function that the area under its
 curve is 
\begin_inset Formula $1$
\end_inset

 means that 
\begin_inset Formula $\int_{R}f_{X}(x)dx=1$
\end_inset

.
 Most of the time we don't know 
\begin_inset Formula $f_{X}$
\end_inset

, but we can observe some values of 
\begin_inset Formula $X$
\end_inset

 (these are called examples, and its collection is called dataset).
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Standard deviation
\series default
 is defined as 
\begin_inset Formula $\sigma=\sqrt{E[(X-\mu)^{2}]}$
\end_inset

 and 
\series bold
variance
\series default
 is 
\begin_inset Formula $\sigma^{2}=E[(X-\mu)^{2}]$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
It is a measure how (un)certain your decision is.
 It lies between 0 and 1, where 0 means impossible and 1 means certain.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://medium.com/@laumannfelix/statistics-probability-fundamentals-1-1325ef
72f3f}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Unbiased estimators
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize
If probability density function of the variable 
\begin_inset Formula $X$
\end_inset

 is unknown (which usually is), but we have a dataset 
\begin_inset Formula $S_{X}=\{x_{i}\}_{i=1}^{N}$
\end_inset

, we are often not satisfied with true values of statistics of the probability
 distribution, such as expectation, but with their unbiased estimators.
 We say that 
\begin_inset Formula $\widehat{\theta}(S_{X})$
\end_inset

 is an unbiased estimator of some statistic 
\begin_inset Formula $\theta$
\end_inset

 calculated using a dataset 
\begin_inset Formula $S_{X}$
\end_inset

drawn from an unknown probability distribution, if 
\begin_inset Formula $\widehat{\theta}(S_{X})$
\end_inset

 has the following property: 
\begin_inset Formula $E[\widehat{\theta}(S_{X})]=\theta$
\end_inset

, where 
\begin_inset Formula $\widehat{\theta}$
\end_inset

 is a sample (dataset) statistic, obtained using a dataset 
\begin_inset Formula $S_{X}$
\end_inset

and not the real statistic 
\begin_inset Formula $\theta$
\end_inset

 that can be obtained only knowing 
\begin_inset Formula $X$
\end_inset

; the expectation is taken over all possible samples drawn from 
\begin_inset Formula $X$
\end_inset

.
 This means that if you have an unlimited number of such samples as 
\begin_inset Formula $S_{X}$
\end_inset

, and you compute some unbiased estimator, such as 
\begin_inset Formula $\widehat{\mu}$
\end_inset

 using each sample, then the average of all these 
\begin_inset Formula $\hat{\mu}$
\end_inset

 equals to the real statistic 
\begin_inset Formula $\mu$
\end_inset

 that you would get computed on 
\begin_inset Formula $X$
\end_inset

.
 Unbiased estimator of an unknown 
\begin_inset Formula $E[X]$
\end_inset

 is given by 
\begin_inset Formula $\frac{1}{N}\sum_{i=1}^{N}x_{i}$
\end_inset

 which is called 
\bar under
sample mean
\bar default
.
\end_layout

\begin_layout Itemize
In statistics, the bias (or bias function) of an estimator is the 
\series bold
difference between this estimator's expected value and the true value of
 the parameter being estimated.
 An estimator with zero bias is called unbiased.
 
\series default
Otherwise the estimator is said to be biased.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Marginal probability
\series default
: probability of event A regardless whether event B occurred or not.
\end_layout

\begin_layout Itemize

\series bold
Conditional probability
\series default
 is a probability of one random variable given an another random variable
 has a particular value.
 Formally, we say “the conditional probability of X given Y is the probability
 of event X when event Y is known”.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(X|Y=y)=\frac{p(X,Y=y)}{p(Y=y)}\label{eq:conditional_probability}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
where 
\begin_inset Formula $p(X,Y)$
\end_inset

 is a probability, that event 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 occur at once.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Posterior probability
\series default
 is just the conditional probability that is outputted by the Bayes theorem.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://stats.stackexchange.com/questions/347526/posterior-vs-conditional-prob
ability}}
\end_layout

\end_inset

 So it is assigned after the relevant evidence or background is taken into
 account.
\end_layout

\begin_layout Itemize

\series bold
Normalization constraint 
\series default
is the probability theory's law that all probability values of any given
 random variable must add up to 1.
\end_layout

\begin_layout Itemize

\series bold
Probability distribution
\series default
 is a function which gives the probability for every possible value of a
 random variable.
 Can be discrete or continuous (depends on values).
\end_layout

\begin_layout Itemize

\series bold
Probability mass function
\series default
 is a function of a 
\series bold
discrete
\series default
 
\series bold
random variable
\series default
, whose
\series bold
 sum across an interval 
\series default
gives the probability that the value of the variable lies within the same
 interval.
 It gives the probability that a discrete random variable is exactly equal
 to some value.
 We must define it for each possible value.
\end_layout

\begin_layout Itemize

\series bold
Probability density function 
\series default
(pdf) a function of a 
\series bold
continuous
\series default
 
\series bold
random variable
\series default
, whose 
\series bold
integral across an interval 
\series default
gives the probability that the value of the variable lies within the same
 interval.
 For modeling the pdf of unknown probability distribution from which some
 dataset has been drawn, we can use 
\bar under
density estimation
\bar default
 (see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Density-Estimation"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Itemize

\series bold
Expected value 
\series default
is the long-run average value of repetitions of the experiment it represents.
 We weigh each observation 
\begin_inset Formula $x$
\end_inset

 with its probability 
\begin_inset Formula $p$
\end_inset

: 
\begin_inset Formula 
\begin{equation}
E[X]=x_{1}p_{1}+x_{2}p_{2}+...+x_{k}p_{k}\label{eq:expected_value}
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
If you don't know the probability of each observation, calculate the arithmetic
 mean and you'll see that they are exactly the same.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Independence
\series default
: random variables are independent if knowing about X tells us nothing about
 Y.
 They are independent if and only if: a) 
\begin_inset Formula $p(Y|X)=p(Y)$
\end_inset

, and 
\begin_inset Formula $p(X,Y)=p(X).p(Y)$
\end_inset

.
 To prove independence, there is an algorithm called 
\series bold
D-Separation
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Marginal independence: 
\series default
Random variable 
\begin_inset Formula $X$
\end_inset

 is conditionally independent from random variable 
\begin_inset Formula $Y$
\end_inset

 if for all 
\begin_inset Formula $x_{i}\in dom(X)$
\end_inset

, 
\begin_inset Formula $y_{j}\in dom(Y)$
\end_inset

, 
\begin_inset Formula $y_{k}\in dom(Y)$
\end_inset

, then 
\begin_inset Formula $P(X=x_{i}|Y=y_{j})=P(X=x_{i}|Y=y_{k})=P(X=x_{i})$
\end_inset

.
 So, knowledge of 
\begin_inset Formula $Y$
\end_inset

 value doesn't affect your belief in the value of 
\begin_inset Formula $X$
\end_inset

.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://www.cs.ubc.ca/~kevinlb/teaching/cs322
\backslash
%20-
\backslash
%202006-7/Lectures/lect25.pdf}}
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Conditional independence
\series default
: a random variable 
\begin_inset Formula $X$
\end_inset

 is conditionally independent from 
\begin_inset Formula $Y$
\end_inset

 given 
\begin_inset Formula $Z$
\end_inset

: 
\begin_inset Formula $X\coprod Y|Z=p(X,Y|Z)=p(X|Z).p(Y|Z)$
\end_inset

, which can be seen in the following diagram:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/conditional_prob.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Conditional independence example.
 As soon as we know something about Z, we don’t need any information about
 Y to know something about X, and we don’t need any information about X
 to know something about Y.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Another definition of conditional independence: Random variable 
\begin_inset Formula $X$
\end_inset

 is conditionally independent from random variable 
\begin_inset Formula $Y$
\end_inset

 given random variable 
\begin_inset Formula $Z$
\end_inset

 if for all 
\begin_inset Formula $x_{i}\in dom(X)$
\end_inset

, 
\begin_inset Formula $y_{j}\in dom(Y)$
\end_inset

, 
\begin_inset Formula $y_{k}\in dom(Y)$
\end_inset

, and 
\begin_inset Formula $z_{m}\in dom(Z)$
\end_inset

, then 
\begin_inset Formula $P(X=x_{i}|Y=y_{j}\land Z=z_{m})=P(X=x_{i}|Y=y_{k}\land Z=z_{m})=P(X=x_{i}|Z=z_{m})$
\end_inset

.
 So, knowledge of 
\begin_inset Formula $Y$
\end_inset

 value doesn't affect your belief in the value of 
\begin_inset Formula $X$
\end_inset

 given a value of 
\begin_inset Formula $Z$
\end_inset

.
 Sometimes, two random variables might not be marginally independent.
 However, they can become independent after we observe some third variable.
\end_layout

\begin_layout Itemize
Conditional independence means for graph (A->B->C) that observing 
\begin_inset Formula $A$
\end_inset

 would not be influential at all for 
\begin_inset Formula $C$
\end_inset

, if we observed 
\begin_inset Formula $B$
\end_inset

.
 To express this more sophisticated: 
\begin_inset Formula $C$
\end_inset

 is conditional independent of 
\begin_inset Formula $A$
\end_inset

 given 
\begin_inset Formula $B$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Maximum Likelihood Estimation
\series default
 (MLE) is an algorithm often used for learning parameters of statistical
 model.
 So it is a technique used for estimating the parameters of a given distribution
, using some observed data.
 
\series bold
Expectation-Maximization
\series default
 (EM) is algorithm for finding out MLE parameters of stochastic models,
 where a model is depending on hidden relationships between variables, that
 contain hidden variables.
 EM is trying to optimize likelihood.
 Likelihood is a probability of data in statistical model given by parameters.
 Likelihood is defined as a set of parameter values of a model for input
 variables, which equals to probability of occurrence of input variables
 given parameters of model.
 
\series bold
Likelihood function
\series default
 is a measure how well the data summarizes the parameters of our model,
 i.e.
 our probability distribution.
 Formal definition of MLE:
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://brilliant.org/wiki/maximum-likelihood-estimation-mle}}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $x_{1},x_{2},...,x_{n}$
\end_inset

 be observations from 
\begin_inset Formula $n$
\end_inset

 independent and identically distributed random variables drawn from a probabili
ty distribution 
\begin_inset Formula $f_{0}$
\end_inset

, where 
\begin_inset Formula $f_{0}$
\end_inset

is known to be from a family of distributions 
\begin_inset Formula $f$
\end_inset

 that depend on some parameters 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Itemize
For example, 
\begin_inset Formula $f_{0}$
\end_inset

could be known to be from the family of normal distributions 
\begin_inset Formula $f$
\end_inset

, which depend on parameters 
\begin_inset Formula $\sigma$
\end_inset

 (standard deviation) and 
\begin_inset Formula $\mu$
\end_inset

 (mean), and 
\begin_inset Formula $x_{1},x_{2},...,x_{n}$
\end_inset

 would be observations from 
\begin_inset Formula $f_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
The goal of MLE is to maximize the likelihood function:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $L=f(x1,x2,...,xn|θ)=f(x1|θ)\times f(x2|θ)\times...\times f(xn|θ)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Often, the average log-likelihood function is easier to work with:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $\hat{l}=\frac{1}{n}log(L)=\frac{1}{n}\sum_{i=1}^{n}log(f(x_{i}|\theta))$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
There are several ways that MLE could end up working: it could discover
 parameters 
\begin_inset Formula $\theta$
\end_inset

 in terms of the given observations, it could discover multiple parameters
 that maximize the likelihood function, it could discover that there is
 no maximum, or it could even discover that there is no closed form to the
 maximum and numerical analysis is necessary to find an MLE.
\end_layout

\begin_layout Itemize
Though MLEs are not necessarily optimal (in the sense that there are other
 estimation algorithms that can achieve better results), it has several
 attractive properties, the most important of which is consistency: a sequence
 of MLEs (on an increasing number of observations) will converge to the
 true value of the parameters.
\end_layout

\begin_layout Itemize

\series bold
Inference
\series default
: the process of computing probability distributions over certain specified
 random variables, usually after observing the value of some other variables
 in the model.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Example: you got model A->B->C, we observe 
\begin_inset Formula $B$
\end_inset

, but don't know 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $C$
\end_inset

.
 
\begin_inset Formula $A$
\end_inset

 does not matter at all for calculating 
\begin_inset Formula $C$
\end_inset

, if we observe 
\begin_inset Formula $B$
\end_inset

.
 How we can calculate probability of 
\begin_inset Formula $C$
\end_inset

? Exactly with 
\begin_inset Formula $P(C|B)$
\end_inset

 and this is inference.
\end_layout

\end_deeper
\begin_layout Description
Gaussian
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Process
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Models: these models (GP models) assume that similar inputs give similar
 outputs.
 This is very weak but very sensible prior for the effects of hyper-parameters.
 For each input dimension, they learn the appropriate scale for measuring
 similarity.
 These models predict a Gaussian distribution of values.
 If you have the resources to run a lot of experiments, Bayesian optimization
 is much better than a person at finding good combination of hyper-parameters.
\end_layout

\begin_layout Description
Random
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

variable: a named quantity (=variable) whose value is uncertain.
 Consequently, we can only give a probability that this quantity has a given
 value.
\end_layout

\begin_layout Description
Dummy
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

variable: (also known as an indicator variable, design variable, one-hot
 encoding, Boolean indicator, binary variable, or qualitative variable)
 are used as devices to sort data into mutually exclusive categories (such
 as smoker/non-smoker, etc.).
 Dummy variable takes the value 0 or 1 to indicate the absence or presence
 of some categorical effect.
\end_layout

\begin_layout Itemize
One-Hot Encoding is especially useful when we want, for some reason, to
 transform categorical values into several binary ones (the reason can be,
 that some learning algorithms only work with numerical feature vectors,
 and we would have vector of binary numerical values as a result).
 So if we have for example a feature that has 3 possible values: red, yellow,
 and green, we could transform them into 
\begin_inset Formula $[1,0,0]$
\end_inset

, 
\begin_inset Formula $[0,1,0]$
\end_inset

, and 
\begin_inset Formula $[0,0,1]$
\end_inset

, but not to 
\begin_inset Formula $1$
\end_inset

, 
\begin_inset Formula $2$
\end_inset

, and 
\begin_inset Formula $3$
\end_inset

, because that would mean that there is an order among the values, and this
 specific order is important for the decision making (yes, we would avoid
 the dimensionality, but we cannot do that).
 If the order of a features' values is not important, using ordered numbers
 as values is likely to confuse a learning algorithm, because the algorithm
 will try to find a regularity where there's no one, which may potentially
 lead to overfitting.
\end_layout

\begin_layout Itemize
On the other hand, when the ordering of values of some categorical variable
 matters, we can replace those values by numbers by keeping only one variable.
 For example if we have feature with 3 values: poor, decent, excellent,
 when we can replace them by 
\begin_inset Formula $1$
\end_inset

, 
\begin_inset Formula $2$
\end_inset

, and 
\begin_inset Formula $3$
\end_inset

.
\end_layout

\begin_layout Description
Median: the value separating the higher half of a data sample / population
 / probability distribution, from the lower half.
\end_layout

\begin_layout Description
Mode: the value that appears the most often.
\end_layout

\begin_layout Description
Variance: a measure how much the values vary around the mean.
 
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sigma^{2}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-m)^{2}\label{eq:variance}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Description
Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

deviation: the square root of the variance, abbreviated as 
\begin_inset Formula $\sigma$
\end_inset

.
\end_layout

\begin_layout Description
Likelihood: it works with discrete values, SUM of all likelihoods of some
 observations is INF.
 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $L$
\end_inset

 likelihood that this comes from this category
\begin_inset Quotes erd
\end_inset

.
 It is conditional probability.
 The unknown (variable) is an assumption.
 It has inductive character.
\end_layout

\begin_layout Description
Entropy: 
\end_layout

\begin_layout Itemize
It is average rate at which information is produced by a stochastic source
 of data.
 
\end_layout

\begin_layout Itemize
In other words, entropy (Shannon entropy) measures the amount (or ratio)
 of information in data.
 The higher value of entropy we have, the more ambiguity is in our data
 and information value is lower.
 So, lower entropy means that we can better predict type of record in our
 data.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H(X)=E[I(X)]=-\sum_{\forall x\in X}p(x)I(x)=-\text{\sum_{\forall x\in X}}p(x)log\,p(x)\label{eq:entrophy}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Example
\series default
: if all records in our dataset are yellow cards, then entropy is 0 (if
 our classification attribute is color of car), since we can predict color
 of a car.
\end_layout

\begin_layout Description
Occams
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Razor:
\end_layout

\begin_layout Standard
This tells us, that simpler models are more probable than complex ones.
 ANN can overfits, because a model is more complex.
 So, by computation of the most probable model we get the simplest possible
 model, and this model is the lowest prone to be overfitting.
 
\end_layout

\begin_layout Itemize
An example: we have a sequence of numbers: 
\begin_inset Formula $-1,3,7,11$
\end_inset

.
 What is the next number?
\end_layout

\begin_deeper
\begin_layout Itemize
Let's state the first hypothesis 
\begin_inset Formula $H_{1}=x_{i+1}=x_{i}+4$
\end_inset

.
 This makes sense, the next number should b e 15.
\end_layout

\begin_layout Itemize
Let's state the second hypothesis: 
\begin_inset Formula $H_{2}=x_{i+1}=\frac{-1}{11}*x_{i}^{3}+\frac{9}{11}*x_{i}^{2}+\frac{23}{11}$
\end_inset

.
 This is more complicated case, but still correct and the next number should
 be -219/11.
\end_layout

\begin_layout Itemize
What is more probable hypothesis? Occams Razor will prefer the first one,
 because is more simple.
 For computation, Occams Razor uses (for hypothesis comparison) the amount
 of data, that can occur by the hypotheses.
 For estimating probabilities of our hypotheses, we use Bayesian Theorem:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $\frac{P(H_{1}|D)}{P(H_{2}|D)}=\frac{P(H_{1})}{P(H_{2})}.\frac{P(D|H_{1})}{P(D|H_{2})}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Let's consider, that we don't have any initial thoughts and clues about
 probabilities of each hypotheses.
 That means, that the first part of the equation is the following: 
\begin_inset Formula $\frac{P(H_{1})}{P(H_{2})}=1$
\end_inset

.
 For avoiding infinity and so on, let's consider interval between 
\begin_inset Formula $[-50,50]$
\end_inset

.
\end_layout

\begin_layout Itemize
For the first case, 
\begin_inset Formula $P(D|H_{1})$
\end_inset

: 
\begin_inset Formula $H_{1}$
\end_inset

is a linear function.
 The first number of sequence as well as the constant (+4 in our equation)
 can both have 101 possibilities (see interval).
 This means, that they both can have 101 possibilities: 
\begin_inset Formula $P(D|H_{1})=\frac{1}{101}.\frac{1}{101}=10^{-4}$
\end_inset

.
\end_layout

\begin_layout Itemize
For the second case, 
\begin_inset Formula $P(D|H_{2})$
\end_inset

: 
\begin_inset Formula $H_{2}$
\end_inset

 can be calculated as the first value in the sequence (1/101) and with 3
 fractions.
 The first number -1/11 can be expressed by 4 possibilities: 
\begin_inset Formula $-1/11\,\bigvee\,-2/22\,\bigvee\,-3/33\,\bigvee\,-4/44$
\end_inset

.
 Similarly for number 9/11, we can have 4 options.
 For number 23/11, with only 2 options (because we have restriction to 50,
 see our interval): 
\begin_inset Formula $23/11\,\bigvee\,46/22$
\end_inset

.
 As a result, 
\begin_inset Formula $P(D|H_{2})=\frac{1}{101}.(\frac{4}{101}.\frac{1}{50}).(\frac{4}{101}.\frac{1}{50}).(\frac{2}{101}.\frac{1}{50}).(\frac{2}{101}.\frac{1}{50})=2.5.10^{-12}$
\end_inset

.
\end_layout

\begin_layout Itemize
As a result, 
\begin_inset Formula $\frac{P(H_{1}|D)}{P(H_{2}|D)}=\frac{10^{-4}}{2.5.10^{-12}}=4.10^{7}$
\end_inset

and therefore the first hypothesis is preferred in ratio by almost 40,000,000:1.
 And if we did not restrict possible values into interval of 101 values,
 this win would be even bigger.
\end_layout

\end_deeper
\begin_layout Description
Exponentially
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

weighted
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

averages
\end_layout

\begin_layout Itemize
Averaging over the last 
\begin_inset Formula $n$
\end_inset

 samples (this is just an approximation, where 
\begin_inset Formula $n\approx\frac{1}{1-\beta}$
\end_inset

 - so the bigger 
\begin_inset Formula $\beta$
\end_inset

 is, the more previous data are taken into account; for example, if 
\begin_inset Formula $\beta=0.5$
\end_inset

, then only 2 samples are taken into account, if 
\begin_inset Formula $\beta=0.98$
\end_inset

, then 50 samples; and thus 
\begin_inset Formula $\beta$
\end_inset

 is another hyperparameter to optimize).
 The formula is following, where 
\begin_inset Formula $\theta_{t}$
\end_inset

is an input feature (for example):
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
V_{t}=\beta*V_{t-1}+(1-\beta)*\theta_{t}\label{eq:exp_weighted_avg}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/exp_weighted_average_implementation.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of exponentially weighted averages on temperature data for a
 year.
 Yellow curve is when 
\begin_inset Formula $\beta=0.5$
\end_inset

 (averaging cca through 2 days, so it adapts quickly), red is when 
\begin_inset Formula $\beta=0.9$
\end_inset

 (averaging cca over the last 10 days), and green line is 
\begin_inset Formula $\beta=0.98$
\end_inset

 (equal to cca 50 previous days).
 
\begin_inset Formula $\beta$
\end_inset

 is a hyperparameter to estimate.
 It would be good also to perform bias correction (final value is divided
 by 
\begin_inset Formula $1-\beta^{iteration}$
\end_inset

 where 
\begin_inset Formula $iteration$
\end_inset

 is a current iteration number), but people don't often bother.
 This is a problem when the algorithm starts (curve starts off really low)
 and is still warming (because we take data (gradients for example) from
 the previous iterations, but at the beginning there are no values.
 Increasing 
\begin_inset Formula $\beta$
\end_inset

 will shift the line slightly to the right, and decreasing it will create
 more oscillation within the line.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
However, it is needed to perform bias correction.
 The problem arises from initializing 
\begin_inset Formula $V_{0}=0$
\end_inset

, which causes 
\begin_inset Formula $V_{1}=0.98V_{0}+(1-0.98)\theta_{1}=0+0.02\theta_{1}$
\end_inset

, which is only 2% of 
\begin_inset Formula $\theta_{1}$
\end_inset

 - a very poor estimate of the first data sample.
 And if you would compute 
\begin_inset Formula $V_{2}$
\end_inset

, then it would be much less then either 
\begin_inset Formula $\theta_{1}$
\end_inset

 or 
\begin_inset Formula $\theta_{2}$
\end_inset

 and it is not a good estimate for the first two samples.
\end_layout

\begin_layout Itemize
A better approach is to use:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
V_{t}=\frac{V_{t}}{1-\beta^{t}}\label{eq:exp_weighted_avg_improved}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Especially for small values of 
\begin_inset Formula $t$
\end_inset

 - when this algorithm is still 
\begin_inset Quotes eld
\end_inset

warming up
\begin_inset Quotes erd
\end_inset

.
 Not everyone uses this approach, but it is useful.
 As 
\begin_inset Formula $t$
\end_inset

 increases, so 
\begin_inset Formula $1-\beta^{t}\rightarrow1$
\end_inset

, then the bias correction effect is no longer making a difference on the
 exponentially weighted average output.
\end_layout

\end_deeper
\begin_layout Description

\series bold
Covariance:
\series default
 
\series bold
how are 2 quantities 
\series default
(or parameters) 
\series bold
similar to each other
\series default
 regarding their 
\series bold
average values
\series default
 (or means) = joint variability of 2 random variables.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
s_{xy}=\frac{\sum(x-\bar{x})(y-\bar{y})}{n-1}\label{eq:covariance}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Description

\series bold
Correlation:
\series default
 standardize (normalize) covariance (so divided by multiplication of both
 standard deviations),
\series bold
 the rate of linear dependency
\series default
.
 It ranges from -1 to 1.
 You should normalize your data before you compute the correlation between
 2 random variables.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
r=\frac{s_{xy}}{s_{x}s_{y}}=\frac{\sum(x-\bar{x})(y-\bar{y})}{\sqrt{\sum(x-\bar{x})^{2}*}\sqrt{(y-\bar{y})^{2}}}\label{eq:correlation}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Description
Causation: causation exists between 2 random variables, if changes in one
 is the reason of changes in the other.
 “Correlation Does Not Imply Causation”.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://medium.com/@laumannfelix/statistics-probability-fundamentals-2-cbb123
9f9605}}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard

\series bold
Example
\series default
: imagine it is 23th May in Copenhagen, around 22 degrees of Celsium.
 Many people are eating ice-cream and you ask if that's because it is hot,
 or local people just like ice-cream.
 After a few more days in city, temperature ranging from 10-25 Celsiums,
 you guess that it is because of the high temperature.
 So you drawn a causal relation between 2 random variables 
\begin_inset Quotes eld
\end_inset

temperature
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

eating ice-cream
\begin_inset Quotes erd
\end_inset

.
 But you could also observe that people go shopping a lot in the warm days,
 more than on the colder days you spent in the city.
 Does this also imply a causal relation? You cannot be certain, because
 there could simply has been public holidays on the cold days, and most
 stores were closed, so less people were attracted to go shopping.
\end_layout

\end_deeper
\begin_layout Description

\series bold
Regression:
\series default
 also standardize covariance, but it is calculated differently via influences
 of variables, not standard deviations.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
r_{x}=\frac{s_{xy}}{s_{x}s_{x}}=\frac{\sum(x-\bar{x})(y-\bar{y})}{\sum(x-\bar{x})^{2}}\label{eq:regression}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Description
Regularization: is a process of introducing additional information in order
 to prevent overfitting.
 
\end_layout

\begin_layout Itemize
These algorithms are basically an extension made to another method (typically
 regression methods) that penalizes models based on their complexity, favoring
 simpler models that are also better at generalizing.
\end_layout

\begin_layout Itemize
It makes hypothesis 
\begin_inset Quotes eld
\end_inset

simpler
\begin_inset Quotes erd
\end_inset

.
 We usually add regularization to cost function (so it results in adding
 regularization in gradient descent; however it is possible to use regularizatio
n also in normal equation).
\end_layout

\begin_layout Itemize
Keep all the features, but reduce magnitude/values of parameters .
 
\end_layout

\begin_layout Itemize
Works well when we have a lot of features, each of which contributes a bit
 to predicting .
\end_layout

\begin_layout Itemize
Penalization of some thetas, so they are very small (close to 0).
\end_layout

\begin_layout Itemize

\series bold
Too much regularization
\series default
 - we can 
\series bold
under-fit
\series default
 the training set and this can lead to worse performance even for examples
 not in the training set.
\end_layout

\begin_layout Itemize
But we do not know, what thetas we should shrink, so we use regularization
 on all, we practically use regularization in cost function itself (inside)
 with 
\end_layout

\begin_deeper
\begin_layout Itemize
regularization term (sum over all thetas) 
\end_layout

\begin_layout Itemize
regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

.
 This controls trade-off between a) fit the training data well b) keep the
 parameters small.
 If 
\begin_inset Formula $\lambda$
\end_inset

 is too big, then we end up penalizing all thetas very highly, so our hypothesis
 is causing underfitting.
 It can be automatically chosen by multi-selection algorithms
\end_layout

\begin_deeper
\begin_layout Itemize
Explanation: if we chose very big 
\begin_inset Formula $\lambda$
\end_inset

, then to get cost function close to 0, thetas have to be very small (if
 they are big, we have very big output of cost function - big penalization).
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Weight decay
\series default
 - a regularization technique (also known as L2 regularization) that results
 in gradient descent shrinking the weights on every iteration.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://stats.stackexchange.com/questions/29130/difference-between-neural-net-
weight-decay-and-learning-rate}}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
See L2 regularization, update rule for weight on some layer: 
\begin_inset Formula $w=w-\alpha(\frac{\partial J}{\partial w}+\frac{\lambda}{m}w-\alpha\frac{\partial J}{\partial w})=w-\alpha\frac{\lambda}{m}w-\alpha\frac{\partial J}{\partial w}=w(1-\alpha\frac{\lambda}{m})-\alpha\frac{\partial J}{\partial w}$
\end_inset

, where term 
\begin_inset Formula $1-\alpha\frac{\lambda}{m}$
\end_inset

 is weight decay - because on every update of a given weight, we decrease
 a given weight by 
\begin_inset Formula $1-\alpha\frac{\lambda}{m}<1$
\end_inset

 regardless of 
\begin_inset Formula $\frac{\partial J}{\partial w}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
L1 regularization 
\series default
- similar to L2 regularization, penalization of large weights and tending
 to make the network prefer small weights.
 Of course, it is not the same.
 L1 regularization is also known as 
\series bold
lasso
\series default
.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
C=C_{0}+\frac{\lambda}{n}\sum_{w}|w|\label{eq:L1}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
Partial derivatives of the cost function is 
\begin_inset Formula $\text{\ensuremath{\frac{\partial C}{\partial w}=\frac{\partial C_{0}}{\partial w}+\frac{\lambda}{n}sign(w)} },$
\end_inset

 where 
\begin_inset Formula $sign(w)$
\end_inset

 is the sign of 
\begin_inset Formula $w$
\end_inset

, that is 
\begin_inset Formula $\pm1$
\end_inset

.
 Update rule for L1 regularized network is 
\begin_inset Formula $w\rightarrow w^{'}=w-\frac{\eta\lambda}{n}sign(w)-\eta\frac{\partial C_{0}}{\partial w}$
\end_inset

 and we can estimate also this using mini-batch average (just a summed average
 over 
\begin_inset Formula $m$
\end_inset

 training samples in mini-batch).
\end_layout

\begin_layout Itemize
If we compare this with resulting update rule for L2: 
\begin_inset Formula $w\rightarrow w^{'}=w(1-\frac{\eta\lambda}{n})-\eta\frac{\partial C_{0}}{\partial w}$
\end_inset

, we can see, that in both expressions the effect of regularization is to
 shrink the weights.
 In L1, however, the weights are shrinked by a constant amount toward 0.
 In L2 regularization, the weights shrink by an amount which is proportional
 to 
\begin_inset Formula $w$
\end_inset

.
 So, when a particular weight has a large magnitude, L1 regularization shrinks
 the weight much less than L2 regularization does.
 By contrast, when 
\begin_inset Formula $|w|$
\end_inset

 is small, L1 regularization shrinks the weight much more than L2 regularization.
 So, L1 regularization tends to concentrate he weight of the network in
 a relatively small number of high-importance connections, while the other
 weights are driven toward zero.
\end_layout

\begin_layout Itemize
By the way, for L1 regularization, 
\begin_inset Formula $\partial C\,/\,\partial w$
\end_inset

 is not defined when 
\begin_inset Formula $w=0$
\end_inset

.
 The reason is that function 
\begin_inset Formula $|w|$
\end_inset

 has a sharp corner at 
\begin_inset Formula $w=0$
\end_inset

, and so it isn't differentiable at that point.
 But it's okay.
 When 
\begin_inset Formula $w=0$
\end_inset

, we will apply just usual unregularized rule for stochastic gradient descent.
 So intuitively, the effect of regularization is to shrink weights, and
 obviously it can't shrink a weight which is already 0.
 By the way, L2 regularization is fully differentiable.
\end_layout

\begin_layout Itemize
L1 regularization results in a 
\series bold
sparse
\series default
 (a lot of zeros) weight vectors, which some people argue that it helps
 to compress the model because for the parameters that are zero we need
 less memory to store the model.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
L2 regularization
\series default
 (see ANN chapter) also known as 
\series bold
ridge regularization
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Frobenius norm 
\series default
is similar to L2, but often used more in practice.
 It can be calculated as:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
||w^{[layer]}||_{F}^{2}=\sum_{i}^{n^{[layer-1]}}\sum_{j}^{n^{[layer]}(w_{ij}^{[layer]})^{2}}\label{eq:frobenius_norm}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where the indices of the summation are determined by dimensions of number
 of hidden units in layers 
\begin_inset Formula $layer$
\end_inset

 and 
\begin_inset Formula $layer-1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Dropout
\end_layout

\begin_deeper
\begin_layout Itemize
Widely used regularization technique that is specific to deep learning.
 It randomly shuts down some neurons in each iteration.
\end_layout

\begin_layout Itemize
This is radically different technique for regularization.
 It does not rely on modifying the cost function, but the network itself.
 Input and output neurons are untouched, but neurons in all hidden layers
 are (with some chance) temporarily deleted (for instance, let's say half
 of them).
 Input is then forward propagated through such modified network, and then
 the result is backpropagated, also through the modified network.
 This can be done on the whole mini-batch of examples.
 Then, we can repeat the process, firstly restoring the dropout neurons,
 then choosing a new random subset of hidden neurons to delete, estimating
 the gradient for a different mini-batch, and updating the weights and biases
 in the network.
\end_layout

\begin_layout Itemize
Different networks may over-fit in different ways, and averaging of multiple
 different nets may help to eliminate overfitting.
 And basically, similar thing is going on with dropout.
 Dropout eliminates different sets of neurons, so it's like training different
 neural nets (so, it's something like ensemble learning).
 Dropout is like averaging the effects of a very large number of different
 networks.
 The idea was that different networks will over-fit in different ways, and
 so hopefully, the net effect of dropout will be to reduce overfitting.
\end_layout

\begin_layout Itemize
Cost function 
\begin_inset Formula $J$
\end_inset

 is no longer well defined when using dropout, because on any given iteration
 some units are zeroed out.
 So checking that 
\begin_inset Formula $J$
\end_inset

 is going down with every iteration is impossible and thus we lost this
 debugging tool.
 As a workaround, if you turn off dropout by setting 
\begin_inset Formula $keep\,prob=1$
\end_inset

 and check if 
\begin_inset Formula $J$
\end_inset

 is decreasing monotonously.
 If yes, then turn on dropout and hope no bugs were introduced.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Weight sharing
\series default
 - it prevents overfitting as well.
 It basically 
\series bold
makes a model
\series default
 (ANN) 
\series bold
simpler
\series default
 by insisting that many weights have exactly the same value as each other.
 You don't know what the value is, and you are going to learn it, but it
 has to be exactly the same for many of the weights.
\end_layout

\begin_layout Itemize

\series bold
Artificially expanding the training data
\series default
 - another technique that can help to reduce overfitting.
 For example, if we have a pictures of handwritten digits (MNIST for example),
 we can rotate each image multiple times, always under different angle.
 And so on.
 In general, the idea is to expand training data by applying operations
 that reflect real-world variation.
\end_layout

\begin_layout Description
Feature
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

selection: family of algorithms that improves estimators' accuracy scores
 or boosts their performance on very high-dimensional datasets.
\end_layout

\begin_layout Itemize
On the other hand, 
\series bold
adding many new features
\series default
 gives us more expressive models which are able to 
\series bold
better fit our training set
\series default
.
 If too many new features are added, this can lead to 
\series bold
overfitting
\series default
 
\series bold
of the training set
\series default
.
\end_layout

\begin_layout Itemize
Feature selection to decrease number/type of input features: This technique
 might help with variance problems, but it might also increase bias.
 Reducing the number of features slightly (say going from 1,000 features
 to 900) is unlikely to have a huge effect on bias.
 Reducing it significantly (say going from 1,000 features to 100 - a 10x
 reduction) is more likely to have a significant effect, so long as you
 are not excluding too many useful features.
 In modern deep learning, when data is plentiful, there has been a shift
 away from feature selection, and we are now more likely to give all the
 features we have to the algorithm and let the algorithm sort out which
 ones to use based on the data.
 But when your training set is small, feature selection can be very useful.
 
\end_layout

\begin_layout Description
Feature
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

scaling: sometimes necessary, sometimes good to have because models can
 converge faster (for example, see contour graphs on 2 variables with totally
 different ranges - graph is skewed and gradient descent must perform many
 steps, in comparison to more circular graph with much less steps
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{Gradient Descent in Practice I - Feature Scaling
\backslash

\backslash

\backslash
url{https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent
-in-practice-i-feature-scaling}}
\end_layout

\end_inset

).
 Normalize the whole dataset at once! Do not normalize train and test data
 separately - use the same mean, standard deviation and so on.
 It is needed that all data goes through the same transformation.
 There exist different techniques:
\end_layout

\begin_layout Itemize

\series bold
Rescaling
\series default
 (also known as Min-Max normalization) is the simplest method, and it puts
 values into the range of [0;1] or [-1;1]
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x_{i}^{'}=\frac{x_{i}-min}{max-min}\label{eq:rescalling}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
the denominator is basically the range of possible values.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Mean normalization
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x_{i}^{'}=\frac{x_{i}-\overline{x_{i}}}{max-min}\label{eq:mean_norm}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\overline{x}$
\end_inset

 is mean and the denominator is basically the range of possible values.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Standardization
\series default
 - widely used (SVM, Logistic regression, ANN)
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x_{i}^{'}=\frac{x_{i}-\bar{x_{i}}}{\sigma_{i}}\label{eq:standardization}
\end{equation}

\end_inset

 where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\overline{x}$
\end_inset

 is mean (also can be noted as 
\begin_inset Formula $\mu$
\end_inset

 in some literature) and 
\begin_inset Formula $\sigma$
\end_inset

 is standard deviation.
 Standardization (z-score normalization) is a procedure during which the
 feature values are rescaled so that they have properties of a standard
 normal distribution with 
\begin_inset Formula $\mu=0$
\end_inset

 and 
\begin_inset Formula $\sigma=1$
\end_inset

.
 Unsupervised learning algorithms usually benefit more from standardization
 than from normalization.
 It is also preferred for a feature if the values this feature takes, are
 distributed close to a normal distribution.
 Or, for a feature which may have sometimes very high or low values (outliers),
 because this will squeeze the normal values into a very small range.
 Usually, in other cases, normalization is preferable, but this is not strictly
 defined, rather one needs to experiment and try multiple approaches (this
 depends on a dataset size and training time).
\end_layout

\end_deeper
\begin_layout Description
Missing
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

features
\end_layout

\begin_layout Itemize
Remove examples with missing features from dataset - if your dataset is
 big enough so that you can sacrifice some training examples.
\end_layout

\begin_layout Itemize
Some algorithms can deal with missing values and this depends on the implementat
ion and/or modifications to it.
\end_layout

\begin_layout Itemize
Use data imputation technique.
 You can a lot of possible strategies, for example, replace the missing
 value of a feature by:
\end_layout

\begin_deeper
\begin_layout Itemize
an average value of this feature in the dataset.
\end_layout

\begin_layout Itemize
middle value of a given range.
\end_layout

\begin_layout Itemize
with a value outside of typical range, so that the learning algorithm will
 learn what is best to do when the feature has a value significantly different
 from regular values.
\end_layout

\begin_layout Itemize
consider it to be a target variable for a regression problem.
 You can use all remaining features to form a feature vector, build a regression
 model to predict it.
 Of course, to build training examples, you only use those examples from
 the original dataset, in which the value of missing feature is present.
\end_layout

\begin_layout Itemize
add a new feature that will represent a binary flag whether a given potentially
 missing feature is present or not (and fill the missing feature with 
\begin_inset Formula $0$
\end_inset

 or any number of your choice).
\end_layout

\end_deeper
\begin_layout Description
Error
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

analysis: 
\end_layout

\begin_layout Itemize
The process of looking at misclassified examples.
 
\end_layout

\begin_layout Itemize
Usually on dev set.
 
\end_layout

\begin_layout Itemize
Error analysis can often help you figure out how promising different directions
 are.
 Don't spend a month of effort on some approach which could lead that your
 model accuracy will increase just by 
\begin_inset Formula $0.5\%$
\end_inset

.
 Investigate what could be the outcome of an approach by error analysis
 (by manually examining misclassified examples).
\end_layout

\begin_layout Itemize
Evaluate multiple ideas in parallel during error analysis - gather all ideas,
 put it into spreadsheet and go through misclassified dev set samples, consider
 each idea, or even make notes on each dev sample (100 or a bit more samples
 are manageable).
\end_layout

\begin_layout Itemize
Suppose you have a large dev set of 5,000 examples in which you have a 
\begin_inset Formula $20\%$
\end_inset

 error rate.
 Thus, your algorithm is misclassifying ~1,000 dev images.
 It takes a long time to manually examine 1,000 images, so we might decide
 not to use all of them in the error analysis.
 In this case, I would explicitly split the dev set into two subsets, one
 of which you look at (randomly pick 
\begin_inset Formula $10\%$
\end_inset

 of all dev set data - 500 samples, about 
\begin_inset Formula $20\%$
\end_inset

 of them will be misclassified - 100 samples; you manually examining these
 500 samples - 
\series bold
eyeball dev set
\series default
), and one of which you don’t (4500 samples - 
\series bold
blackbox dev set
\series default
).
 You will more rapidly over-fit the portion that you are manually looking
 at.
 You can use the portion you are not manually looking at to tune parameters.
\end_layout

\begin_deeper
\begin_layout Itemize
Eyeball dev set manual examination of errors - use this information to prioritiz
e what types of errors to work on fixing.
\end_layout

\begin_layout Itemize
Why do we explicitly separate the dev set into Eyeball and Blackbox dev
 sets? Since you will gain intuition about the examples in the Eyeball dev
 set, you will start to over-fit the Eyeball dev set faster.
 If you see the performance on the Eyeball dev set improving much more rapidly
 than the performance on the Blackbox dev set, you have over-fit the Eyeball
 dev set.
 In this case, you might need to discard it and find a new Eyeball dev set
 by moving more examples from the Blackbox dev set into the Eyeball dev
 set or by acquiring new labeled data.
\end_layout

\begin_layout Itemize
If you are working on a task that even humans cannot do well, then the exercise
 of examining an Eyeball dev set will not be as helpful because it is harder
 to figure out why the algorithm didn't classify an example correctly.
 In this case, you might omit having an Eyeball dev set.
\end_layout

\begin_layout Itemize
If you have a small dev set, then you might not have enough data to split
 into Eyeball and Blackbox dev sets that are both large enough to serve
 their purposes.
 Instead, your entire dev set might have to be used as the Eyeball dev set—i.e.,
 you would manually examine all the dev set data.
\end_layout

\begin_layout Itemize
If you only have an Eyeball dev set, you can perform error analyses, model
 selection and hyperparameter tuning all on that set.
 The downside of having only an Eyeball dev set is that the risk of overfitting
 the dev set is greater.
\end_layout

\begin_layout Itemize
If you have plentiful access to data, then the size of the Eyeball dev set
 would be determined mainly by how many examples you have time to manually
 analyze.
 For example, I've rarely seen anyone manually analyze more than 1,000 errors.
\end_layout

\begin_layout Itemize
The Eyeball dev set should be big enough so that your algorithm mis-classifies
 enough examples for you to analyze.
 A Blackbox dev set of 1,000-10,000 examples is sufficient for many applications.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Description
Recommender
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

systems
\end_layout

\begin_layout Itemize
These systems seek to predict the "rating" or "preference" a user would
 give to an item.
\end_layout

\begin_layout Itemize
An example of one problem solved with recommender systems is on the following
 figure.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/recommender_systems_example.png
	scale 22

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Formulation of predicting movie ratings problem, which can be implemented
 with recommender systems.
 Some users did not watch all the movies (purple), which is completely fine
 and realistic.
 In this example, Alice and Bob clearly likes romantic movies and do not
 like action movies (in opposite to Carol and Dave).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Traditionally, two approaches were used to give recommendations:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Content-based filtering
\series default
 - how users would rate movies that they did not watch/rated yet.
 In order to make predictions, we could treat predicting ratings of a user
 as a separate linear regression problem, see figures below.
 
\end_layout

\begin_deeper
\begin_layout Itemize
This method (in this case) is called content-based recommendation, because
 we assume that features for different movies are available to us.
 These features capture what is the content of these movies = how romantic
 or action is a given movie.
 So, for making predictions, we're using features of a movie content.
 
\end_layout

\begin_layout Itemize
Another approach, that isn't content based and doesn't assume that we have
 someone else giving us all of these features for all of the movies in our
 data set, is called 
\series bold
collaborative filtering
\series default
.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/recommender_systems_content_based.png
	scale 24

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Content-based recommender systems, an example of problem with movie rating
 prediction using linear regression.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/recommender_systems_content_based_formulation.png
	scale 24

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Content-based recommender system problem formulation using linear regression.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Collaborative filtering 
\series default
- as mentioned in the previous paragraph, if we don't have any information
 about feature values (like, how romantic or action is a specific movie),
 we can compute them from data (for example users' data - users provided
 how they like romantic or action movies, and rated some movies, see a figure
 below).
\end_layout

\begin_deeper
\begin_layout Itemize
So basically, users are collaborating and each user hopes that the algorithm
 will learn better features because of him, and perhaps the whole system
 (for example for movie prediction) will perform better.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/recommender_systems_collaborative_filtering_example.png
	scale 23

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Formulation of predicting movie ratings problem, which can be implemented
 with recommender systems using collaborative filtering.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/recommender_systems_collaborative_filtering_optimization_objective.png
	scale 23

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Collaborative filtering optimization objective.
 We want to minimize all 
\begin_inset Formula $x$
\end_inset

 and all 
\begin_inset Formula $\theta$
\end_inset

 - so we want to find all parameters 
\begin_inset Formula $\theta$
\end_inset

 for our users, and we want to estimate features 
\begin_inset Formula $x$
\end_inset

.
 We could go back and forth - solve parameters, features, parameters, features,
 and so on - but much better, and more efficient approach is to solve them
 both simultaneously.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/recommender_systems_collaborative_filtering_algorithm.png
	scale 21

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Collaborative filtering algorithm summary.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Most real-world recommender systems use a hybrid approach: they combine
 recommendations obtained by the content-based and collaborative filtering
 models.
 Two effective recommender system learning algorithms are
\series bold
:
\end_layout

\begin_layout Itemize

\series bold
Factorization machines
\series default
 (FM)
\end_layout

\begin_deeper
\begin_layout Itemize
Relatively new kind of algorithm, explicitly designed for sparse datasets.
\end_layout

\begin_layout Itemize
Its model is defined as follows:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x)=b+\sum_{i=1}^{D}w_{i}x_{i}+\sum_{i=1}^{D}\sum_{j=i+1}^{D}(v_{i}v_{j})x_{i}x_{j}\label{eq:factorization_machine_model}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $b$
\end_inset

 and 
\begin_inset Formula $w_{i}$
\end_inset

, 
\begin_inset Formula $i=1,...,D$
\end_inset

 are scalar parameters similar to those used in linear regression.
 Vectors 
\begin_inset Formula $v_{i}$
\end_inset

 are 
\begin_inset Formula $k$
\end_inset

-dimensional vectors of 
\series bold
factors
\series default
.
 
\begin_inset Formula $k$
\end_inset

 is a hyperparameter and is usually much smaller than 
\begin_inset Formula $D$
\end_inset

.
 So, instead looking for one wide vector of parameters, which can reflect
 poorly the interactions between features because of sparsity, we complete
 it by additional parameters that apply to pairwise interactions 
\begin_inset Formula $x_{i}x_{j}$
\end_inset

 between features.
 However, instead of having a parameter 
\begin_inset Formula $w_{i,j}$
\end_inset

 for each interaction, which would add an enormous quantity of new parameters
 to the model, we factorize 
\begin_inset Formula $w_{i,j}$
\end_inset

 into 
\begin_inset Formula $v_{i}v_{j}$
\end_inset

 by adding only 
\begin_inset Formula $Dk<<D(D-1)$
\end_inset

 parameters to the model.
\end_layout

\end_deeper
\begin_layout Itemize
Depending on the problem, the loss function could be squared error loss
 (for regression), or hinge loss.
 For classification with 
\begin_inset Formula $y\in\{-1,+1\}$
\end_inset

, with hinge loss or logistic loss the prediction is made as 
\begin_inset Formula $y=sign(f(x))$
\end_inset

.
\end_layout

\begin_layout Itemize
Gradient descent can be used to optimize the average loss.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Denoising autoencoders
\series default
 (DAE) see Section 
\begin_inset CommandInset ref
LatexCommand pageref
reference "sec:Autoencoder"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The fact, that the input is corrupted by a noise, while the output shouldn't
 be, makes denoising autoencoders an ideal tool to build a recommender model.
 The idea is very straightforward - new movies that a user could like, are
 seen as removed from the complete set of preferred movies by some corruption
 process.
 The goal of the denoising autoencoder is to reconstruct those removed items.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section
Performance
\end_layout

\begin_layout Standard
We need to measure the performance of a given model somehow.
 There exist several metrics.
\end_layout

\begin_layout Itemize
For 
\series bold
regression problems
\series default
, the situation is quite simple.
 A well-fitting regression model results in predicted values close to the
 observed data values.
 The mean model, which always predicts the average of the labels in the
 training data, generally would be useful if there were no informative values.
 The fit of a regressor should be better than the fit of the mean model.
 So we can compare the performances of the model on the training and test
 data, for example with MSE (or other type of average loss function that
 makes sense) for training, and then separately, for test data.
 If the MSE of the model on the test data is substantially higher than MSE
 obtained on the training data, this is a sign of overfitting.
\end_layout

\begin_layout Itemize
For
\series bold
 classification problems
\series default
, we have a lot of options: confusion matrix, accuracy, cost-sensitive accuracy,
 precision/recall, or area under the ROC curve.
\end_layout

\begin_layout Description
Basic
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

terminology
\end_layout

\begin_layout Itemize
condition positive (P) - the number of real positive cases in the data
\end_layout

\begin_layout Itemize
condition negative (N) - the number of real negative cases in the data
\end_layout

\begin_layout Itemize
true positive (TP) - eqv.
 with 
\series bold
hit
\end_layout

\begin_layout Itemize
true negative (TN) - eqv.
 with 
\series bold
correct rejection
\end_layout

\begin_layout Itemize
false positive (FP) - eqv.
 with 
\series bold
false alarm
\series default
, 
\series bold
Type I error 
\end_layout

\begin_layout Itemize
false negative (FN) - eqv.
 with 
\series bold
miss
\series default
, 
\series bold
Type II error
\end_layout

\begin_layout Description
Accuracy - carefully, there can be Accuracy Paradox
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{Accuracy paradox - 
\backslash
url{https://en.wikipedia.org/wiki/Accuracy_paradox}}
\end_layout

\end_inset

 when working with unbalanced dataset (skewed classes).
 
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{TruePositives+TrueNegatives}{TruePositives+TrueNegatives+FalsePositives+FalseNegatives}\label{eq:accuracy}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
which is basically 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{CorrectPredictions}{AllPredictions}\label{eq:accuracy_simpler}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Description
Precision - from all patients, ones we predicted to have cancer - how many
 truly has cancer? (
\series bold
true positives / predicted positives
\series default
, see confusion matrix if you are confused :-) ).
 Basically, how precise is what I recall?
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{https://www.quora.com/What-is-the-best-way-to-understand-the-terms-precis
ion-and-recall}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{TruePositives}{TruePositives+FalsePositives}\label{eq:precision}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Description
Recall - (a.k.a.
 sensitivity, or probability of detection) from all patients for truly have
 cancer, how many of them we correctly detected that they have cancer? (
\series bold
true positives / actual positives
\series default
, see confusion matrix if you are confused :-) ).
 Measures how good classifier we have.
 It is always a balance between recall and precision.
 One single metric, between precision and recall is 
\series bold
F1-score
\series default
.
 
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{TruePositives}{TruePositives+FalseNegatives}\label{eq:recall}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Description
Trading
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

off
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

precision
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

and
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

recall
\end_layout

\begin_layout Itemize
For example, in logistic regression, if we predict '1' if 
\begin_inset Formula $h_{\theta}(x)\geq0.9$
\end_inset

 (0 otherwise), then we tell someone to has cancer only when we are sure
 on at least 90%.
 This is higher precision, but lower recall.
 On other other side, if we predict '1' if 
\begin_inset Formula $h_{\theta}(x)\geq0.3$
\end_inset

 (0 otherwise), we claim that someone has cancer even when we are not very
 certain (at least 30%) - higher recall, but lower precision.
 Depends on a final system.
 
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/precision_recall_tradeoff.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of plotting a tradeoff between precision a recall.
 The curve can have very different shapes, like on the graph (different
 colors).
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
To understand the meaning and importance of precision and recall for the
 model assessment it is often useful to think about the prediction problem
 as the problem of research of documents in the database using a query.
 The precision is the proportion of relevant documents in the list of all
 returned documents.
 The recall is the ratio of the relevant documents returned by the search
 engine to the total number of the relevant documents that could have been
 returned.
\end_layout

\begin_layout Itemize
In the case of the spam detection problem, we want to have high precision
 (we want to avoid making mistakes by detecting that a legitimate message
 is spam), and we are ready to tolerate lower recall (some spam messages
 in our box).
\end_layout

\begin_layout Itemize
It is usually impossible to have both, so you can achieve either of the
 two by various approaches:
\end_layout

\begin_deeper
\begin_layout Itemize
By assigning a higher weighting to the examples of a specific class.
\end_layout

\begin_layout Itemize
By tuning hyperparameters to maximize precision or recall on the validation
 set.
\end_layout

\begin_layout Itemize
By varying the decision threshold for algorithms that return probabilities
 of classes.
 For example, to set that a given prediction will be positive only if the
 probability returned by the model is higher than 
\begin_inset Formula $0.9$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Description
True
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

negative
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

rate - (a.k.a.
 specificity)
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{TrueNegative}{TrueNegative+FalsePositive}\label{eq:tnr}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Description
F1-score - harmonic average of the precision and recall, where an F1 score
 reaches its best value at 1 (perfect precision and recall) and worst at
 0.
 Not just average, because of skewed classes problem.
 Why the following formula? Debug it when put 
\begin_inset Formula $P=0\,or\,R=0$
\end_inset

, and then also 
\begin_inset Formula $P=1\,and\,R=1$
\end_inset

 and you will see that it is always between 0 and 1.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
F_{1=}\frac{2}{\frac{1}{Recall}+\frac{1}{Precision}}=2*\frac{Precision*Recall}{Precision+Recall}\label{eq:f1-score}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
There exists 
\begin_inset Formula $F_{\beta}-score$
\end_inset

.
 It affects a 
\begin_inset Quotes eld
\end_inset

weight
\begin_inset Quotes erd
\end_inset

 of either precision to have bigger, or recall.
 If we want treat them equally (most cases, but it depends on use-case of
 resulting system), then 
\begin_inset Formula $\beta=1$
\end_inset

.
 If
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\beta$
\end_inset

 in range (0,1), then Recall is considered to be lower than Precision.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta$
\end_inset

 bigger than 1, then Recall is considered to be higher than Precision.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
F_{\beta}=(1+\beta^{2})*\frac{Precision*Recall}{(\beta^{2}*Precision)+Recall}\label{eq:fb_score}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Often a resulting metric of some learned model.
 
\end_layout

\begin_layout Description
Confusion
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

matrix - (also known as error matrix) is a specific table layout that allows
 visualization of the performance of an algorithm.
 Each row of the matrix represents the instances in a predicted class while
 each column represents the instances in an actual class (
\series bold
or vice versa
\series default
, but very often like on the figure below).
 Confusion matrix is used to calculate two other performance metrics: precision
 and recall.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/conf_matrix.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Confusion matrix scheme, from Wikipedia
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Description
Area
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

under
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

ROC
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

Curve
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

(AUC)
\end_layout

\begin_layout Itemize
The ROC curve (receiver operating characteristic, from radar engineering)
 is also commonly used method for measuring performance of classification
 models.
 ROC curve uses a combination of the 
\series bold
true positive rate 
\series default
(TPR is basically the recall, defined as 
\begin_inset Formula $\frac{TP}{TP+FN}$
\end_inset

) and 
\series bold
false positive rate
\series default
 (FPR, proportion of negative examples predicted incorrectly, defined as
 
\begin_inset Formula $\frac{FP}{FP+TN}$
\end_inset

).
\end_layout

\begin_layout Itemize
ROC curves can only be used with classifiers that return some confidence
 score or a probability of prediction.
\end_layout

\begin_layout Itemize
To draw a ROC curve, you first discretize the range of the confidence score.
 It is it 
\begin_inset Formula $[0,1]$
\end_inset

, then you can discretize if like this: 
\begin_inset Formula $[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]$
\end_inset

.
 Then, you use each discrete value as the prediction threshold and predict
 labels of examples in your dataset using the model and this threshold.
 For example, if you want to compute TPR and FPR for the threshold equal
 to 
\begin_inset Formula $0.7$
\end_inset

, you apply the model to each example, get the score, and if the score is
 higher than or equal to 
\begin_inset Formula $0.7$
\end_inset

, you predict the positive class.
 Otherwise, you predict the negative class.
 In case of ROC curve, upper right corner is when the threshold is 
\begin_inset Formula $0$
\end_inset

, thus all predictions will be positive, and if the threshold is 
\begin_inset Formula $1$
\end_inset

, then no positive prediction will be made and both TPR and FPR will be
 
\begin_inset Formula $0$
\end_inset

 (lower left corner).
\end_layout

\begin_layout Itemize
The higher the area under the ROC curve (AUC), the better the classifier.
 Usually, if your model be haves well, you obtain a good classifier by selecting
 the value of the threshold that gives TPR close to 
\begin_inset Formula $1$
\end_inset

while keeping FPR near 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Itemize
ROC curves are popular because they are relatively simple to understand,
 they capture more than one aspect of the classification (taking both, false
 positives and false negatives into account), and allow visually and with
 low effort comparing the performance of different models.
\end_layout

\begin_layout Description
An
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

example - Suppose you are working on a spam classifier, where spam emails
 are positive examples (
\begin_inset Formula $y=1$
\end_inset

) and non-spam emails are negative examples (
\begin_inset Formula $y=0$
\end_inset

).
 You have a training set of emails in which 99% of the emails are non-spam
 and the other 1% is spam.
\end_layout

\begin_deeper
\begin_layout Itemize
If you always predict non-spam, your classifier will have 99% accuracy on
 the training set, and it will likely perform similarly on the cross validation
 set.
 The classifier achieves 99% accuracy on the training set because of how
 skewed the classes are.
 We can expect that the cross-validation set will be skewed in the same
 fashion, so the classifier will have approximately the same accuracy.
\end_layout

\begin_layout Itemize
If you always predict spam, your classifier will have a recall of 100% and
 precision of 1%.
 Since every prediction is (
\begin_inset Formula $y=1$
\end_inset

), there are no false negatives, so recall is 100%.
 Furthermore, the precision will be the fraction of examples with are positive,
 which is 1%.
\end_layout

\begin_layout Itemize
f you always predict non-spam, your classifier will have a recall of 0%.
 Since every prediction is (
\begin_inset Formula $y=0$
\end_inset

), there will be no true positives, so recall is 0%.
\end_layout

\end_deeper
\begin_layout Description
Combining
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

multiple
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

evaluation
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset

metrics
\end_layout

\begin_layout Itemize
Suppose you care about both the accuracy and the running time of a learning
 algorithm.
 It seems unnatural to derive a single metric by putting accuracy and running
 time into a single formula, such as 
\begin_inset Formula $accuracy-0.5*runtime$
\end_inset

.
 Instead, define what is an “acceptable” running time.
 Lets say anything that runs in 100ms is acceptable.
 Then, maximize accuracy, subject to your classifier meeting the running
 time criteria.
 Here, running time is a “satisfying metric” - your classifier just has
 to be “good enough” on this metric, in the sense that it should take at
 most 100ms.
 Accuracy is the “optimizing metric.” 
\series bold
If you have 
\begin_inset Formula $N$
\end_inset

 different criteria, then consider 
\begin_inset Formula $N-1$
\end_inset

 of them as 
\begin_inset Quotes eld
\end_inset

satisfying
\begin_inset Quotes erd
\end_inset

 and pick one as 
\begin_inset Quotes eld
\end_inset

optimizing
\begin_inset Quotes erd
\end_inset

 one.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section
Evaluating a Learning Algorithm
\end_layout

\begin_layout Standard

\series bold
\begin_inset Quotes eld
\end_inset

Choose dev and test sets to reflect data you expect to get in the future
 and want to do well on.
\begin_inset Quotes erd
\end_inset

 [8]
\end_layout

\begin_layout Standard
There are 3 accuracy estimation methods: holdout, cross-validation, and
 bootstrap.
 
\end_layout

\begin_layout Itemize
This means, that we train some model, and then perform model selection or
 calculate prediction accuracy with these methods.
 CV and bootstrap refit a model from samples formed from the training set,
 in order to obtain additional information about the fitted model, for example
 test-set prediction error, standard deviation and bias of our parameter
 estimates
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.
pdf}}
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
Training and test error can be different, as we can see on the figure below:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/training_test_err_performance.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example in which we can see that the training error rate is quite different
 from the test error rate.
 Test error rate can dramatically underestimate the training error rate.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\series bold
Holdout method
\end_layout

\begin_layout Itemize
The data set is randomly separated into 2 mutually exclusive subsets usually
 called the training set (2/3) and the test set (1/3).
 This method involves a single run, in comparison to other methods.
\end_layout

\begin_layout Itemize
The more data we leave for test set, the higher bias of our estimation.
 Fewer test set instances however means, that the confidence interval for
 the accuracy will be wider.
 Generally, the larger the training data the better the classifier.
 The larger the test data the more accurate performance measures estimate
 (e.g.
 accuracy)
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{http://staffwww.itn.liu.se/~aidvi/courses/06/dm/lectures/lec6.pdf}}
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
The function approximator fits a function using the training set only.
 Then the function approximator is asked to predict the output values for
 the data in the testing set (it has never seen these output values before).
 The errors it makes are accumulated as before to give the mean absolute
 test set error, which is used to evaluate the model.
 
\end_layout

\begin_layout Itemize
In random sub-sampling, the holdout method is repeated k times, and the
 estimated accuracy is derived by averaging the runs.
\end_layout

\begin_layout Itemize
Many sources claim, that holdout is the simplest kind of cross-validation.
 Many other sources instead classify holdout as a type of simple validation,
 rather than a simple or degenerate form of cross-validation.
\end_layout

\begin_layout Itemize

\series bold
Pros
\series default
 
\end_layout

\begin_deeper
\begin_layout Itemize
Fully independent data.
\end_layout

\begin_layout Itemize
Only needs to be run once so has lower computational costs.
 
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Cons
\end_layout

\begin_deeper
\begin_layout Itemize
It makes inefficient use of the data: 1/3 (testing subset) of dataset is
 not used for training the inducer.
 
\end_layout

\begin_layout Itemize
The evaluation can have a high variance.
 It may depend heavily on which data points end up in the training set and
 which end up in the test set, and thus the evaluation may be significantly
 different depending on how the division is made.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Cross-validation
\end_layout

\begin_layout Itemize

\series bold
Any particular cross-validation experiment yields only an approximation
 of the true error rate.
\end_layout

\begin_layout Itemize
Sampling without replacement.
 The same instance, once selected, can not be selected again for a sample.
 There are no duplicate records in the training and test sets.
\end_layout

\begin_layout Itemize
A dataset can be repeatedly split into a training dataset and a validation
 dataset: this is known as cross-validation.
\end_layout

\begin_layout Itemize
Cross-validation doesn't work in situations where you can't shuffle your
 data, most notably in time-series.
\end_layout

\begin_layout Itemize
It is mainly used in settings where the goal is prediction, and one wants
 to estimate how accurately a predictive model will perform in practice.
 So not for model building (training itself).
\end_layout

\begin_layout Itemize
The goal of cross-validation is to test the model’s ability to predict new
 data that was not used in estimating it, in order to flag problems like
 overfitting 
\series bold
(and CV is mostly used for overcoming overfitting!) 
\series default
and to give an insight on how the model will generalize to an independent
 dataset
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://en.wikipedia.org/wiki/Cross-validation_(statistics)}}
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
One round of cross-validation involves partitioning a sample of data into
 complementary subsets, performing the analysis on one subset (called the
 training set), and validating the analysis on the other subset (called
 the validation set or testing set).
 To reduce variability, in most methods multiple rounds of cross-validation
 are performed using different partitions, and the validation results are
 combined (e.g.
 averaged) over the rounds to give an estimate of the model’s predictive
 performance.
\end_layout

\begin_layout Itemize
Recent experimental results on artificial data and theoretical results in
 restricted settings have shown that for selecting a good classier from
 a set of classifiers (model selection), 10-fold cross-validation may be
 better than the more expensive leave-one-out cross-validation [6].
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection

\series bold
The whole process
\end_layout

\begin_layout Standard
(from
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets} and
\backslash

\backslash

\backslash
url{https://www.researchgate.net/post/what_is_the_difference_between_validation_se
t_and_test_set}}
\end_layout

\end_inset

)
\end_layout

\begin_layout Enumerate

\series bold
Training set
\series default
: 
\series bold
model is initially fit on this subset
\series default
, for fitting a model's parameters (e.g.
 weights of connections between neurons in artificial neural networks).
 
\end_layout

\begin_layout Enumerate

\series bold
Validation set
\series default
: 
\series bold
this subset is optional
\series default
, and it is aimed to
\series bold
 minimizing overfitting
\series default
 problem.
 Mostly used for tuning the hyper-parameters.
 This data is used during training to assess how well a model is currently
 performing - the performance of the ANN on this data may be used to guide
 the training in some way (e.g.
 controlling the learning rate, deciding when to stop training, number of
 hidden units in ANN, choosing between several trained networks - different
 architectures for instance), but you are not adjusting weights (parameters)
 of model (ANN for example) with this set.
 
\end_layout

\begin_deeper
\begin_layout Itemize
You're just verifying that any increase in accuracy over the training data
 set actually yields an increase in accuracy over a data set that has not
 been shown to a model before, or at least a model hasn't trained on it.
 If the accuracy over the training data set increases, but the accuracy
 over then validation data set stays the same or decreases, then you're
 overfitting your model and you should stop training.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-t
rain-validation-and-test-set-in-neural-netwo}}
\end_layout

\end_inset

 
\end_layout

\begin_layout Itemize

\series bold
The validation dataset functions as a hybrid: it is training data used by
 testing, but neither as part of the low-level training nor as part of the
 final testing.

\series default
 
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
Test set
\series default
: to assess the performance (i.e.
 generalization and predictive power).
 
\series bold
Ideally, should be used once only, after training is complete.
 
\end_layout

\begin_deeper
\begin_layout Itemize
When the data in the test dataset has never been used in training (for example
 in cross-validation), the test dataset is also called a holdout dataset.

\series bold
 
\end_layout

\begin_layout Itemize

\series bold
It is used to asset the performance (i.e.
 generalization) of a fully specified classifier.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection

\series bold
Types of CV
\end_layout

\begin_layout Itemize

\series bold
Leave-p-out cross-validation
\series default
 - it involves using 
\begin_inset Formula $p$
\end_inset

 observations as the validation set and the remaining observations as the
 training set.
 This is repeated on all ways to cut the original sample on a validation
 set of 
\begin_inset Formula $p$
\end_inset

 observations and a training set.
\end_layout

\begin_layout Itemize

\series bold
Leave-one-out cross-validation 
\series default
- this is a particular case of the previous one with 
\begin_inset Formula $p=1$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
It is k-fold cross validation taken to its logical extreme, with 
\begin_inset Formula $k=N$
\end_inset

, the number of data points in the set.
 That means that 
\begin_inset Formula $N$
\end_inset

 separate times, the function approximator is trained on all the data except
 for one point and a prediction is made for that point.
 As before the average error is computed and used to evaluate the model.
 
\end_layout

\begin_layout Itemize
Mainly used for rather small datasets.
\end_layout

\begin_layout Itemize
LOOCV sometimes useful, but typically doesn't shake up the data enough.
 The estimates from each fold are highly correlated and hence their average
 can have high variance (unreliable estimates), and the bias is minimized
 (in fact it is almost unbiased).
\end_layout

\begin_layout Itemize

\series bold
Pros
\series default
 - makes best use of the data for training.
 Increases the chance of building more accurate classifiers – involves no
 random sub-sampling.
\end_layout

\begin_layout Itemize

\series bold
Cons
\series default
 - very computationally expensive.
 Stratification is not possible
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{Ensures that each class is represented with approximately equal
 proportions in both subsets.}
\end_layout

\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
k-fold cross-validation
\series default
 - the original sample is randomly partitioned into 
\begin_inset Formula $k$
\end_inset

 mutually exclusive sub-samples, of approximately equal size.
 The inducer is trained and tested 
\begin_inset Formula $k$
\end_inset

 times (basically 
\begin_inset Formula $k$
\end_inset

 times repeated holdout method).
 
\end_layout

\begin_deeper
\begin_layout Itemize
Of the 
\begin_inset Formula $k$
\end_inset

 sub-samples (folds), a single sub-sample is retained as the validation
 data for testing the model, and the remaining 
\begin_inset Formula $k-1$
\end_inset

 sub-samples are used as training data (each fold contains equal number
 of data samples).
 The cross-validation process is then repeated 
\begin_inset Formula $k$
\end_inset

 times (so, you build 
\begin_inset Formula $k$
\end_inset

 models), with each of the 
\begin_inset Formula $k$
\end_inset

 sub-samples used exactly once as the validation data.
 The 
\begin_inset Formula $k$
\end_inset

 results can then be averaged to produce a single estimation (averaging
 results from predictions of 
\begin_inset Formula $k$
\end_inset

 models).
 All observations are used for both training and validation, and each observatio
n is used for validation exactly once.
 
\end_layout

\begin_layout Itemize
Results in [6] indicate that stratification is generally a better scheme,
 both in terms of bias and variance, when compared to a regular cross-validation.
\end_layout

\begin_layout Itemize
For model selection it is recommended to use stratified 10-fold CV [6].
\end_layout

\begin_layout Itemize
We expect an inducer stability to hold more in 20-fold cross-validation
 than in 10-fold cross-validation and both should be more stable than holdout
 of 1/3.
 In fact, the bias of 10 or 20 folds is (according to measurements) good
 enough [6].
 On the contrary, 2 or 3 folds are usually very biased.
 However, the variance is increased (instability of the training sets themselves
).
\end_layout

\begin_layout Itemize

\series bold
Pros
\series default
 - it matters less how the data gets divided.
 Every data point gets to be in a test set exactly once, and gets to be
 in a training set 
\begin_inset Formula $k-1$
\end_inset

 times.
 The variance of the resulting estimate is reduced as 
\begin_inset Formula $k$
\end_inset

 is increased.
 
\end_layout

\begin_layout Itemize

\series bold
Cons
\series default
 - a training algorithm has to be rerun from scratch 
\begin_inset Formula $k$
\end_inset

 times, which means it takes 
\begin_inset Formula $k$
\end_inset

 times as much computation to make an evaluation.
 
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Repeated random sub-sampling validation
\series default
 (also known as 
\series bold
Monte Carlo cross-validation
\series default
) - we randomly split the dataset into training and validation data.
 For each such split, the model is fit to the training data, and predictive
 accuracy is assessed using the validation data.
 The results are then averaged over the splits.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Repeating CV multiple times using different splits into folds.
\end_layout

\begin_layout Itemize

\series bold
Pros
\series default
 - advantage (over k-fold CV) is that the proportion of the training/validation
 split is not dependent on the number of iterations (folds).
\end_layout

\begin_layout Itemize

\series bold
Cons 
\series default
- some observations may never be selected in the validation sub-sample,
 whereas others may be selected more than once.
 In other words, validation subsets may overlap.
\end_layout

\end_deeper
\begin_layout Subsubsection

\series bold
Examples
\series default
 
\series bold
of usage
\end_layout

\begin_layout Itemize
CV can be used to compare the performances of different predictive modeling
 procedures.
 For example, suppose we are interested in optical character recognition,
 and we are considering using either support vector machines (SVM) or k
 nearest neighbors (k-NN) to predict the true character from an image of
 a handwritten character.
 Using cross-validation, we could objectively compare these two methods
 in terms of their respective fractions of misclassified characters.
 If we simply compared the methods based on their in-sample error rates,
 the k-NN method would likely appear to perform better, since it is more
 flexible and hence more prone to overfitting compared to the SVM method.
\end_layout

\begin_layout Itemize
CV can also be used in variable selection (also known as 
\series bold
feature selection
\series default
).
\end_layout

\begin_layout Itemize
In the context of linear regression, CV is also useful in that it can be
 used to select an optimally regularized cost function.
\end_layout

\begin_layout Itemize
Consider the following situation
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.
pdf}}
\end_layout

\end_inset

: 2 classes, 5000 predictors (input features for example), 50 samples:
\end_layout

\begin_deeper
\begin_layout Enumerate
Find that 100 predictors have the largest correlation with the class labels.
 
\end_layout

\begin_layout Enumerate
Then we apply a classifier for example logistic regression using only these
 100 predictors.
\end_layout

\begin_layout Standard
Estimation of test set performance of this classifier 
\series bold
cannot
\series default
 be done by applying CV in 2), forgetting about 1) - because 
\series bold
the procedure already seen the labels of the training data and made use
 of them
\series default
.
 This is a form of training and must be included in the validation process.
 
\series bold
The correct process
\series default
 is to apply CV in steps 1 and 2.
\end_layout

\end_deeper
\begin_layout Itemize
NOT TRUE, that training, validation, and test sets, must have the same distribut
ions of data [8].
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Example
\series default
: Users of your cat pictures app have uploaded 10,000 images, which you
 have manually labeled as containing cats or not.
 You also have a larger set of 200,000 images that you downloaded off the
 internet.
 How should you define train/dev/test sets? Since the 10,000 user images
 closely reflect the actual probability distribution of data you want to
 do well on, you might use that for your dev and test sets.
 If you are training a data-hungry deep learning algorithm, you might give
 it the additional 200,000 internet images for training.
 Thus, your training and dev/test sets come from different probability distribut
ions.
 How does this affect your work? Instead of partitioning our data into train/dev
/test sets, we could take all 210,000 images we have, and randomly shuffle
 them into train/dev/test sets.
 In this case, all the data comes from the same distribution.
 But I recommend against this method, because about 205,000/210,000 ≈ 97.6%
 of your dev/test data would come from internet images, which does not reflect
 the actual distribution you want to do well on.
 Most of the academic literature on machine learning assumes that the training
 set, dev set 11 and test set all come from the same distribution.
 In the early days of machine learning, data was scarce.
 We usually only had one dataset drawn from some probability distribution.
 So we would randomly split that data into train/dev/test sets, and the
 assumption that all the data was coming from the same source was usually
 satisfied.
 But in the era of big data, we now have access to huge training sets, such
 as cat internet images.
 Even if the training set comes from a different distribution than the dev/test
 set, we still want to use it for learning since it can provide a lot of
 information.
 Instead of putting all 10,000 user-uploaded images into the dev/test sets,
 we might instead put 5,000 into the dev/test sets.
 We can put the remaining 5,000 user-uploaded examples into the training
 set.
 This way, your training set of 205,000 examples contains some data that
 comes from your dev/test distribution along with the 200,000 internet images.
\end_layout

\begin_layout Itemize

\series bold
Usually they are all from the same distribution, especially in academic
 research, but it is important to understand that different training and
 dev/test distributions offer some special challenges 
\series default
(and perhaps also some good advantages, like in the previous example).
\end_layout

\end_deeper
\begin_layout Itemize
When to decide if use all your data?
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Example
\series default
: Suppose your cat detector's training set includes 10,000 user-uploaded
 images.
 This data comes from the same distribution as a separate dev/test set,
 and represents the distribution you care about doing well on.
 You also have an additional 20,000 images downloaded from the Internet.
 Should you provide all 20,000+10,000=30,000 images to your learning algorithm
 as its training set, or discard the 20,000 Internet images for fear of
 it biasing your learning algorithm? 
\end_layout

\begin_deeper
\begin_layout Itemize
When using earlier generations of learning algorithms (such as hand-designed
 computer vision features, followed by a simple linear classifier) there
 was a real risk that merging both types of data would cause you to perform
 worse.
 Thus, some engineers will warn you against including the 20,000 Internet
 images.
\end_layout

\begin_layout Itemize
But in the modern era of powerful, flexible learning algorithms - such as
 large neural networks - this risk has greatly diminished.
 If you can afford to build a neural network with a large enough number
 of hidden units/layers, you can safely add the 20,000 images to your training
 set.
 Adding the images is more likely to increase your performance.
\end_layout

\begin_layout Itemize
If you think you have data that has no benefit,you should just leave out
 that data for computational reasons.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Bootstrap
\end_layout

\begin_layout Itemize
In comparison to holdout and cross-validation, these methods use uses (uniformly
) sampling with replacement to form the training set.
 This means, that an instance may occur more than once.
\end_layout

\begin_layout Itemize
Usage is for estimation of properties of estimator
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{
\backslash
url{https://en.wikipedia.org/wiki/Bootstrapping_
\backslash
%28statistics
\backslash
%29}}
\end_layout

\end_inset

 (such as its variance) by measuring those properties when sampling from
 an approximating distribution.
\end_layout

\begin_layout Itemize
Bootstrap has low variance, but extremely large bias on some problems in
 comparison to cross-validation.
\end_layout

\begin_layout Itemize
Primarily used to obtain standard errors of an estimate and confidence intervals.
 Cross-validation provides a simpler, more attractive approach for estimating
 prediction error.
\end_layout

\begin_layout Itemize

\series bold
Probably the best way of estimating performance for very small data sets.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section
Derivations
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/derivatives_intuition.png
	scale 22

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of derivation of a simple function.
 We can see that the first derivation of some function is just a slope.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/derivatives_intuition2.png
	scale 22

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Another example of derivation of a simple function.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section
Eigenvectors, Eigenvalues, and PCA
\end_layout

\begin_layout Itemize
Let's consider a matrix to be a tool for making linear transformation and
 focus on a mapping of a vector.
 In other words, a matrix can transform a magnitude and direction of a vector
 sometimes also into a lower dimension.
\end_layout

\begin_layout Itemize
For example, 
\begin_inset Formula $\left(\begin{array}{cc}
3 & -2\\
1 & 0
\end{array}\right).\left(\begin{array}{c}
1\\
0
\end{array}\right)=\left(\begin{array}{c}
3\\
1
\end{array}\right)$
\end_inset

, or 
\begin_inset Formula $\left(\begin{array}{cc}
3 & -2\\
1 & 0
\end{array}\right).\left(\begin{array}{c}
0\\
1
\end{array}\right)=\left(\begin{array}{c}
-2\\
0
\end{array}\right)$
\end_inset

, or 
\begin_inset Formula $\left(\begin{array}{cc}
3 & -2\\
1 & 0
\end{array}\right).\left(\begin{array}{c}
-1\\
1
\end{array}\right)=\left(\begin{array}{c}
-5\\
-1
\end{array}\right)$
\end_inset

.
 Draw and see what happended.
 You have a vector 
\begin_inset Formula $\left(\begin{array}{c}
1\\
0
\end{array}\right)$
\end_inset

 and then you have 
\begin_inset Formula $\left(\begin{array}{c}
3\\
1
\end{array}\right)$
\end_inset

, so its magnitude and direction was changed.
\end_layout

\begin_layout Itemize
So basically, if you have matrix 
\begin_inset Formula $A=\left(\begin{array}{cc}
3 & -2\\
1 & 0
\end{array}\right)$
\end_inset

 and 
\begin_inset Formula $X=\left(\begin{array}{c}
1\\
0
\end{array}\right)$
\end_inset

, then linear transformation of 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula $\left(\begin{array}{c}
3\\
1
\end{array}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Eigenvector
\series default
 is a vector that when multiplied by a given transformation matrix, is a
\series bold
 scalar multiple
\series default
 of itself.
 
\series bold
Eigenvalue is this scalar multiple.
 In ML, transforming a vector only by some factoring scalar is very useful.
\end_layout

\begin_layout Itemize
For the example, 
\begin_inset Formula $A=\left(\begin{array}{cc}
3 & -2\\
1 & 0
\end{array}\right)$
\end_inset

 and 
\begin_inset Formula $X=\left(\begin{array}{c}
2\\
1
\end{array}\right)$
\end_inset

 vector 
\begin_inset Formula $X$
\end_inset

 is eigenvector of the matrix 
\begin_inset Formula $A$
\end_inset

.
 After linear transformation, we can see that resulting vector is 
\begin_inset Formula $\left(\begin{array}{c}
4\\
2
\end{array}\right)$
\end_inset

 and the direction was not changed; only magnitude.
 In this case, 
\begin_inset Formula $2$
\end_inset

 is eigenvalue and is usually notes as 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Itemize
How to find these numbers? The first step is always to look for eigenvalue.
 For example, let 
\begin_inset Formula $A$
\end_inset

 be transformation matrix, 
\begin_inset Formula $\bar{X}$
\end_inset

 to be eigenvector, and 
\begin_inset Formula $\lambda$
\end_inset

 to be eigenvalue.
 Then, 
\begin_inset Formula $A.\bar{X}=\lambda.\bar{X}$
\end_inset

.
 For finding out eigenvalue, we can use identity matrix 
\begin_inset Formula $I_{n}$
\end_inset

.
 The previous equation will be still the same, 
\begin_inset Formula $\lambda.\bar{X}=\lambda.I_{n}.\bar{X}$
\end_inset

.
 So, 
\begin_inset Formula $A.\bar{X}=\lambda.I_{n}.\bar{X}$
\end_inset

, and then 
\begin_inset Formula $A.\bar{X}-\lambda.I_{n}.\bar{X}=0$
\end_inset

, and 
\begin_inset Formula $(A-\lambda.I_{n}).\bar{X}=0$
\end_inset

.
 if 
\begin_inset Formula $A-\lambda.I_{n}$
\end_inset

 is invertible, we can divide this in both sides and end up with 
\begin_inset Formula $\bar{X}=\frac{0}{A-\lambda.I_{n}}=>\bar{X}=0$
\end_inset

.
 But this is not what we want.
 We want to find 
\begin_inset Formula $\lambda$
\end_inset

 such that the matrix 
\begin_inset Formula $A-\lambda.I_{n}$
\end_inset

 is not invertible.
 This is not invertible if its determinant is equal to 
\begin_inset Formula $0$
\end_inset

.
 This equation is also called characteristic equation of a matrix.
\end_layout

\begin_layout Itemize
BTW, 
\series bold
triangular matrix 
\series default
is such square matrix, where all the elements either above or below the
 diagonal are zero.
 Then, its determinant is calculated by simple multiplication of all the
 elements on its main diagonal and finding out solution to such polynomial
 function.
\end_layout

\begin_layout Itemize

\series bold
PCA
\series default
 can find linear transformation that will reduce dimensions (of data / vectors
 / matrices).
\end_layout

\begin_layout Itemize
Covariance matrix - to find out if components (from PCA) are independent
 or not, you can calculate this covariance matrix.
 It is a symmetric matrix, 
\series bold
for example 
\series default

\begin_inset Formula $\left(\begin{array}{cc}
V_{a} & C_{a,b}\\
C_{a,b} & V_{b}
\end{array}\right)$
\end_inset

 that expresses how each of the variables (in this example, two variables
 
\begin_inset Formula $V_{a}$
\end_inset

 and 
\begin_inset Formula $V_{b}$
\end_inset

 would be for data that has 2 variables for each data point, such as 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 coordinates) relate to each other.
 The goal is to find a new axis for our data (in this example each data
 point has 2 variables) such that we can represent each two dimensional
 points by using just one dimensional scalar value 
\begin_inset Formula $R$
\end_inset

, called projection of the datapoint onto the new axis.
 To achieve this, we consider covariance matrix to be our transformation
 matrix, and 
\series bold
we have to calculate eigenvalues and eigenvectors of the covariance matrix
\series default
.
 2x2 covariance matrix has 2 eigenvectors with 2 corresponding eigenvalues.
 Eigenvectors points to specific directions with relation to the data.
 The goal is to select one of these eigenvectors as a new axis.
 We will then take the projection of the original data point onto eigen
 vector and that we would be our reduced one dimensional data.
 And we will choose component (eigenvector) with the largest spread - largest
 variance, because that's where the most of the information is.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/eigenvectors.jpg
	scale 13

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
A simple example that calculates eigenvalues and eigenvectors.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section
Minimizing Cost Function
\begin_inset CommandInset label
LatexCommand label
name "sec:Minimizing-cost-function"

\end_inset


\end_layout

\begin_layout Subsection
Newton method 
\end_layout

\begin_layout Standard
Uses 2 derivations.
\end_layout

\begin_layout Subsection
Gradient descend 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Gradient-descend"

\end_inset


\end_layout

\begin_layout Itemize
Iterative optimization algorithm to finding a minimum of a function, in
 our case a cost function.
 Results in local minimum (if we want local maximum, we need to use gradient
 ascend algorithm).
 Local minimum is when partial derivative of cost function (see below) is
 0.
\end_layout

\begin_layout Itemize

\series bold
Idea
\end_layout

\begin_deeper
\begin_layout Enumerate
start at some initial parameters (e.g.
 
\begin_inset Formula $\text{Θ}_{0},\text{Θ}_{1}$
\end_inset

 for Univariate linear regression).
\end_layout

\begin_layout Enumerate
keep changing parameters (
\begin_inset Formula $\text{Θ}_{0},\text{Θ}_{1}$
\end_inset

) to reduce cost function until we hopefully end up at a minimum.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Practical tips and notes
\end_layout

\begin_deeper
\begin_layout Itemize
Gradient descend can be much more faster when using feature scaling (see
 the next section).
 This is because 
\begin_inset Formula $\theta$
\end_inset

 will descend quickly on small ranges and slowly on large ranges, and so
 will oscillate inefficiently down to the optimum when the variables are
 very uneven.
\end_layout

\begin_layout Itemize
To make sure that gradient descend is working correctly - plot on x-axis
 number of iterations, and on y-axis cost function 
\begin_inset Formula $J(\theta)$
\end_inset

 as gradient descent runs.
 
\series bold
Cost function
\series default
 in the graph 
\series bold
must decrease after every iteration
\series default
.
 If otherwise, learning rate 
\begin_inset Formula $\alpha$
\end_inset

 should be probably decreased (see a shape of a given cost function, maybe
 more scenarios can occur e.g.
 a bug in code).
 If 
\begin_inset Formula $\alpha$
\end_inset

 is small enough, it must decrease in every iteration, but the algorithm
 is probably very slow.
\end_layout

\begin_layout Itemize
How many iterations gradient descend needs to converge depends on the applicatio
n.
 It can be for example from 3,000 to 3,000,000 ...
 There are also automatic converge test - if cost function decreased by
 less than some 
\begin_inset Formula $\varepsilon$
\end_inset

, for example 
\begin_inset Formula $\varepsilon=10^{-3}$
\end_inset

 in one iteration.
 But it is also good enough to use only graph mentioned before (since it
 is also problem to estimate 
\begin_inset Formula $\varepsilon$
\end_inset

).
\end_layout

\begin_layout Itemize
Works pretty well even with a great number of features.
\end_layout

\begin_layout Itemize
Complexity 
\begin_inset Formula $O(kn^{2})$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is a number of features and 
\begin_inset Formula $k$
\end_inset

 is a number of iterations.
 
\end_layout

\begin_layout Itemize

\series bold
Initialization
\series default
 - the best is to use random initialization! For thetas in gradient descent
 and advanced optimization method.
\end_layout

\begin_layout Itemize
We can slowly decrease learning rate 
\begin_inset Formula $α$
\end_inset

 over time if we want θ to converge (e.g.
 
\begin_inset Formula $\alpha=\frac{const1}{iterationnumber+const2}$
\end_inset

, but some people are not doing this because you need to tune these 2 constants
 in the equation).
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Types
\end_layout

\begin_deeper
\begin_layout Standard

\bar under
Batch
\bar default
:
\end_layout

\begin_layout Itemize
Classical, basic one, where we iterate through all training samples and
 then perform update, so all training samples are used in each step.
\end_layout

\begin_layout Standard

\bar under
Stochastic
\bar default
 (also known as 
\bar under
online learning
\bar default
 or 
\bar under
incremental learning
\bar default
):
\end_layout

\begin_layout Enumerate
randomly shuffle dataset 
\end_layout

\begin_layout Enumerate
iterate through all training samples and perform update of parameters
\end_layout

\begin_layout Itemize
So stochastic gradient descent works with first sample, performs one step
 with gradient descent with cost function.
 In comparison to classic (batch) gradient descent, it does not wait to
 iterate through all training samples and calculates SUM, but the improvement
 starts from the beginning.
 OR it can work with just a subset (100 samples for instance) of training
 data, and then perform update, so not with all the samples.
\end_layout

\begin_layout Itemize
It is basically a heuristics, where a convergence to global minimum is slower
 and consequent, but sometimes we can go in wrong direction; however the
 algorithm is around global optima.
 
\end_layout

\begin_layout Itemize
So this method is a little less precise, but faster.
 However, 
\series bold
it is inefficient
\series default
, because we are working with just 1 sample and cannot make a benefit out
 of 
\series bold
vectorization
\series default
.
\end_layout

\begin_layout Itemize
To debug if it is working and converging - average the last 1,000 costs
 (iterations) that we computed and plot it (in BGD we plot 
\begin_inset Formula $J_{train}(θ)$
\end_inset

 as a function of number of iterations of gradient descent, so this is similar
 but not with all training samples and also not with just 1 sample).
 If not 1,000, but 5,000 example for instance, maybe we can get a smoother
 curve (again, number of iterations vs cost).
 Small number of examples can cause that the plot is very noisy (almost
 like ’oscillating’) which is normal.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/batch_gradient_vs_stochastic_gradient_descent.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
A difference between batch and stochastic gradient descent used in linear
 regression.
 In SGD, its cost function measures how well is a hypothesis doing on a
 single training example.
 Unlike Batch gra- dient descent, SGD does not end up in global minimum.
 Instead, the algorithm will end up in some close region of minima (this
 is for algorithms such as linear regression).
 But in practice this is not a problem, we are satisfied with such result.
 In SGD, the second step can be performed multiple times - if you need to
 still improve accurac even if you run a learning algorithm against al training
 samples.
 But you may end up in good accuracy even after the first pass, which can
 be true if you have a lot of data.
 If you have less data, maybe it is needed to run it more (like 1-10x) times.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\bar under
Mini-batch
\bar default
:
\end_layout

\begin_layout Itemize
Something between batch and stochastic versions.
 Mini-batch gradient descent uses only a small amount of samples, for example
 10 or 100.
 Why? Because 
\series bold
derivations are just an approximation of direction, no need to compute it
 on all dataset in each iteration
\series default
.
 Most optimization algorithms work with mini-batches (and with the first
 derivations).
 
\end_layout

\begin_layout Itemize
In practice, the most used variant.
 You use vectorization and a progress is made without a need to wait for
 processing the whole training set.
\end_layout

\begin_layout Itemize

\series bold
One pass through our training set (multiple batches of samples) is called
 1 epoch.
 
\series default
Let's say that we have 100 samples in a batch and 50,000 samples in our
 training set.
 So in mini-batch gradient descent, a single pass though the training set
 allows us to make 500 gradient descent steps.
 In comparison, in batch gradient descent, a single pass through training
 set allows us to make just 1 gradient descent step.
\end_layout

\begin_layout Itemize
Full batch gradient descent is 
\series bold
not guaranteed to find a better local minimum
\series default
 than mini-batch gradient descent.
 Mini-batch gradient descent can search through weight space due to noise
 and potentially escape a bad local minima.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/mini_batch_grad_descent_size.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A comparison of different mini-batch sizes.
 Basically this is a comparison of extremes as well, so stochastic, mini-batch
 and batch versions.
 If you have a small training set, use batch gradient descent.
 Otherwise, typicall size is a number from 64 to 128 samples.
 Sometimes it runs faster if it is a power of 2, and also using 256 or even
 512 samples is still common.
 What number, it depends also on CPU/GPU memory, if it fits - this depends
 on a data.
 Choosing a good mini-batch size is basically a hyperparameter.
 Andrew Ng recommends to try several values and then pick one that works
 the best.
 Choosing the best mini-batch size is about making a compromise.
 Too small, and you don't get to take full advantage of the benefits of
 good matrix libraries optimized for fast hardware.
 Too large and you're simply not updating your weights often enough.
 What you need is to choose a compromise value which maximizes the speed
 of learning.
 Fortunately, the choice of mini-batch size at which the speed is maximized
 is relatively independent of the other hyper-parameters (apart from the
 overall architecture), so you don't need to have optimized those hyper-paramete
rs in order to find a good mini-batch size.
 The way to go is therefore to use some acceptable (but not necessarily
 optimal) values for the other hyper-parameters, and then trial a number
 of different mini-batch sizes, and scaling learning rate.
 Plot the validation accuracy versus time (as in, real elapsed time, not
 epoch!), and choose whichever mini-batch size gives you the most rapid
 improvement in performance.
 With the mini-batch size chosen you can then proceed to optimize the other
 hyper-parameters.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
If it is decreasing, it is converging.
\end_layout

\begin_layout Itemize
If it is increasing, try to use smaller learning rate 
\begin_inset Formula $α$
\end_inset

.
\end_layout

\begin_layout Itemize
If it is not doing anything, then maybe try to use different model because
 it is for some reason not learning much.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/mini-batch_gradient_descent.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
In mini-batch gradient descent, learning error (cost function) may not decrease
 on every iteration as it should in batch gradient descent.
 A plot is showing learning with multiple epochs.
 The reason is that some mini-batches can be relatively easy and some may
 be harder.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Adagrad
\end_layout

\begin_deeper
\begin_layout Itemize
Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
 (from 2011).
\end_layout

\begin_layout Itemize
Version of SGD, that scales learning rate 
\begin_inset Formula $\alpha$
\end_inset

 for each parameter according to the history of gradients.
 As a result, 
\begin_inset Formula $\alpha$
\end_inset

 is reduced for very large gradients and vice-versa.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Gradient descent with Momentum
\end_layout

\begin_deeper
\begin_layout Itemize
Almost always works faster than the standard gradient descent algorithm.
 The basic idea is to calculate an 
\series bold
exponentially weighted average 
\series default
(see Definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:exp_weighted_avg"
plural "false"
caps "false"
noprefix "false"

\end_inset

)
\series bold
 of your gradients
\series default
, and then use that gradient to update your weights (and biases) instead.
 Considering exponentially weighted average and its bias correction, in
 practice not many people are applying bias correction.
 The reason is that after 10 iteration, the moving average is already 
\begin_inset Quotes eld
\end_inset

warmed-up
\begin_inset Quotes erd
\end_inset

 and is no longer biased.
\end_layout

\begin_layout Itemize
Because mini-batch gradient descent makes a parameter update after seeing
 just a subset of examples, the direction of the update has some variance,
 and so the path taken by mini-batch gradient descent will "oscillate" toward
 convergence.
 Using momentum can reduce these oscillations.
 Momentum takes into account the past gradients to smooth out the update.
\end_layout

\begin_layout Itemize
This result in smoothing the gradient steps (because we are using averaging).
 Steps are more direct and there is less oscillation.
 And the algorithm can take a more straightforward path into minimum.
\end_layout

\begin_layout Itemize
The larger the momentum 
\begin_inset Formula $β$
\end_inset

 is, the smoother the update because the more we take the past gradients
 into account.
 But if 
\begin_inset Formula $β$
\end_inset

 is too big, it could also smooth out the updates too much.
 Common values for 
\begin_inset Formula $β$
\end_inset

 range from 0.8 to 0.999.
 Tuning the optimal 
\begin_inset Formula $β$
\end_inset

 for your model might need trying several values to see what works best
 in term of reducing the value of the cost function 
\begin_inset Formula $J$
\end_inset

.
\end_layout

\begin_layout Itemize
It can be applied with batch gradient descent, mini-batch gradient descent
 or stochastic gradient descent.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/grad_descent_momentum_implementation.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Implementation of gradient descent with momentum.
 The crossed formula in the figure is bias correction, but it is not being
 used in practice because after for instance 10 iterations, the moving average
 is warmed up and there is no longer need for a bias estimate.
 
\begin_inset Formula $\beta=0.9$
\end_inset

 is the most common value, but it can be considered as hyperparameter and
 tuned out.
 Purple formula on the right side is an alternative that can be seen in
 literature.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/gradient_descent_momentum_plot.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of gradient descent with momentum learning.
 The correct answer is that (1) is gradient descent.
 (2) is gradient descent with momentum 
\begin_inset Formula $(small\,β)$
\end_inset

.
 (3) is gradient descent with momentum 
\begin_inset Formula $(large\,β)$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
So, basically we replace gradient descent update rule 
\begin_inset Formula $w\rightarrow w^{'}=w-\eta\nabla C$
\end_inset

, where 
\begin_inset Formula $\eta$
\end_inset

 is learning rate and 
\begin_inset Formula $\nabla C$
\end_inset

 is gradient vector, by:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
v\rightarrow v^{'}=\mu v-\eta\nabla C\label{eq:momentum1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
w\rightarrow w^{'}=w+v^{'}\label{eq:momentum2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mu$
\end_inset

 is a hyper-parameter controlling the amount of damping or friction in the
 system.
 
\begin_inset Formula $\mu=1$
\end_inset

 means no friction.
 So, the force of 
\begin_inset Formula $\nabla C$
\end_inset

 is modifying the velocity 
\begin_inset Formula $v$
\end_inset

, and the velocity is controlling the rate of change of 
\begin_inset Formula $w$
\end_inset

.
 Intuitively, we build up the velocity by repeatedly adding gradient terms
 to it.
 That means that if the gradient is in (roughly) the same direction through
 several rounds of learning, we can build up quite a bit of steam moving
 in that direction.
 So, if for example with each step the velocity gets larger down the slope,
 we move more and more quickly to the (local) minima.
 This can enable the momentum technique to work much faster than standard
 gradient descent.
 The problem is, that if gradient should change rapidly, then we could be
 moving in the wrong direction.
 For this reason, there is a hyper-parameter 
\begin_inset Formula $\mu$
\end_inset

, in a range between 0 and 1.
\end_layout

\end_deeper
\begin_layout Itemize
Momentum was probably inspired by 
\series bold
Hessian technique
\series default
, which is not used in practice.
 The reason is that it uses hessian matrix, which is extremely non-optimal
 (it has a lot of entries; for example, if a neural net has 
\begin_inset Formula $10^{7}$
\end_inset

biases and weights, then this matrix would have 
\begin_inset Formula $10^{14}$
\end_inset

entries, and having computation with such matrix is difficult in practice).
 But both these techniques uses the same idea - using information about
 how the gradient is changing.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
RMSprop
\end_layout

\begin_deeper
\begin_layout Itemize
Root mean square prop, is a method how to speed up gradient descent.
 So, it is similar to momentum, and the aim is to reduce oscillations in
 gradient descent.
\end_layout

\begin_layout Itemize
You can use a bigger learning rate and have faster learning without diverging
 in the vertical direction (see figure below).
 It is like Gradient descent with momentum, but we are squaring the derivatives,
 and then we take a square root at the end.
 
\end_layout

\begin_layout Itemize
Combination of RMSProp with momentum is called 
\series bold
Adam
\series default
.
\end_layout

\begin_layout Itemize
Fun fact is, that RMS Prop was first proposed by Geoffrey Hinton in a course
 on Coursera.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/rmsprop_intuition.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RMSprop intuition and formulas.
 In practice, to avoid a division by zero, we add a small number (here it
 is defined as 
\begin_inset Formula $\varepsilon=10^{-8}$
\end_inset

) in denominator for such numberical stability.
 In traditional gradient descent, you can have a big oscillation in for
 example vertical direction.
 To mitigate oscillation on vertical direction and still have progress in
 horizontal direction, RMSprop is helpful.
 In this example, there is 
\begin_inset Formula $b$
\end_inset

 on vertical axis and 
\begin_inset Formula $w$
\end_inset

 on horizontal axes.
 In real-case scenario, there would be much bigger number of dimensions
 of parameters, for example on horizontal axis 
\begin_inset Formula $w_{3},w_{4},...$
\end_inset

 but for the sake of intuition, this example is more simple.
 If 
\begin_inset Formula $db$
\end_inset

 is large, it will be even a bigger number and eventually, the final 
\begin_inset Formula $b$
\end_inset

 will have in its denominator large number (division by a big number is
 a small number).
 On the other hand, updates in 
\begin_inset Formula $w$
\end_inset

 (horizontal direction) will keep going (because we are still dividing by
 small number).
 So, the most importantly, in dimensions you got oscillations in your gradient
 descent, you end up computing a larger sum (these 
\begin_inset Formula $S_{d\,parameter}$
\end_inset

) of weighted averages of squares and derivatives, and so you end up dumping
 out directions in which there are these oscillations.
 So, there is no need to know a direction where the oscillation occurs.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Adam
\end_layout

\begin_deeper
\begin_layout Itemize
Adaptive moment estimation (from 2015).
\end_layout

\begin_layout Itemize
It is a combination of momentum and RMSprop algorithms.
 Most optimization algorithms that have been proposed don't generalize well
 to the wide range of deep neural networks.
 RMS prop and Adam do generalize well however.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/adam_algorithm.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Adam optimization algorithm for gradient descent.
 In Adam, we also compute biases corrections (because of weighted average
 means).
 The algorithm has a number of hyperparameters: learning rate 
\begin_inset Formula $\alpha$
\end_inset

, choice of moving weighted average 
\begin_inset Formula $\beta_{1}$
\end_inset

(called the first moment, default to 0.9) and 
\begin_inset Formula $\beta_{2}$
\end_inset

(called the second moment, for squared derivatives, default to 0.999), correction
 constant 
\begin_inset Formula $\varepsilon$
\end_inset

 (does not matter very much, but the authors proposed to be equal to 
\begin_inset Formula $10^{-8}$
\end_inset

), but in reality mostly learning rate is tuned.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
There are following hyperparameters (the last two values were recommended
 by the authors of Adam paper):
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\alpha$
\end_inset

: needs to be tuned
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta_{1}$
\end_inset

: 
\begin_inset Formula $0.9$
\end_inset

 (
\begin_inset Formula $dw$
\end_inset

 Momentum) - also know as The first moment
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta_{2}$
\end_inset

: 
\begin_inset Formula $0.999$
\end_inset

 (
\begin_inset Formula $dw^{2}$
\end_inset

RMS prop) - also know as The second moment
\end_layout

\begin_layout Itemize
\begin_inset Formula $\epsilon$
\end_inset

: 
\begin_inset Formula $10^{-8}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Some other very advanced alternatives
\end_layout

\begin_deeper
\begin_layout Itemize
BFGS 
\end_layout

\begin_layout Itemize
L-BFGS 
\end_layout

\begin_layout Itemize
Conjugate gradient
\end_layout

\begin_layout Standard
They are often faster than gradient descent, and there is no need to manually
 pick the learning rate 
\begin_inset Formula $\alpha$
\end_inset

.
 These algorithms automatically try and pick learning rate 
\begin_inset Formula $\alpha$
\end_inset

, which can be even different on every iteration.
 Andrew NG recommends, he uses them.
 They are very complex, even he did not know the details for over a decade.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Gradient checking
\series default
 - recommended by Andrew Ng - if you have even a bit more complex model
 which uses backpropagation or similar gradient descent algorithm, ALWAYS
 perform gradient checking, which checks a presence of almost any bug in
 the gradient descent implementation.
 This also solves problems with buggy implementation of backpropagation
 algorithms in ANN (see algorithm on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gradient_checking"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
an approximation of the derivative of cost function.
\end_layout

\begin_layout Itemize
verification that a computation of derivative of cost function is truly
 correct.
\end_layout

\begin_layout Itemize
however, after we use it and find out that our implementation of for example
 backpropagation is correct, we have to turn off gradient checking (for
 learning), since it is not optimal algorithm - forward propagation must
 be performed twice for every parameter in the network.
 Backprop is much faster.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/gradient_checking_visual.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A visual representation of Gradient Checking (numerical estimation of the
 derivative).
 On the graph, a slope of the line between those 2 red points is an approximatio
n to the derivative.
 Crossed formula (right side) is one-sided difference estimate and it is
 an alternative, but Andrew Ng uses two-sided difference estimate (on the
 left) since it is slightly more accurate.
 He also uses usually 
\begin_inset Formula $\varepsilon=10^{-4}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Learning rate decay
\end_layout

\begin_deeper
\begin_layout Itemize
Another way to speed up learning algorithm.
\end_layout

\begin_layout Itemize
Slowly decrease learning rate over time.
 
\end_layout

\begin_layout Itemize
For example, the following implementation could be used:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\alpha=\frac{1}{1+decay\,rate*epoch}*\alpha_{0}\label{eq:learning_rate_decay}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $decay\,rate$
\end_inset

 and initial learning rate 
\begin_inset Formula $\alpha_{0}$
\end_inset

 are hyperparameters and 
\begin_inset Formula $epoch$
\end_inset

 is a currently processed epoch of data.
 
\end_layout

\end_deeper
\begin_layout Itemize
Another approach is 
\series bold
exponential decay
\series default
.
 This will exponentially quickly degrade the learning rate.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\alpha=0.95^{epoch}*\alpha_{0}\label{eq:exponential_decay}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
But learning rate decay can be achieved also by manually changing learning
 rate during training, or automatically after a certain number of steps.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Normal Equation
\end_layout

\begin_layout Itemize
Another method for finding local optima of cost function.
 
\series bold
This method solves 
\begin_inset Formula $\theta$
\end_inset

 analytically
\series default
.
 
\end_layout

\begin_layout Itemize
TL;DR much faster alternative than gradient descent, not usable with all
 machine learning algorithms and not suitable every time.
 Gradient descent is more universal.
\end_layout

\begin_layout Itemize
In comparison to gradient descent, there is no need to perform multiple
 steps and to choose learning rate 
\begin_inset Formula $\alpha$
\end_inset

.
 But gradient descent scale better than normal equations method.
 Normal equations method is slow when there is a big amount (more than 
\begin_inset Formula $10^{3}$
\end_inset

or 
\begin_inset Formula $10^{4}$
\end_inset

) of features (invertion of a huge amount of matrices).
\end_layout

\begin_layout Itemize
Complexity is 
\begin_inset Formula $O(n^{3})$
\end_inset

 (matrix inversion), where 
\begin_inset Formula $n$
\end_inset

 is a number of features.
\end_layout

\begin_layout Itemize
We will minimize 
\begin_inset Formula $J$
\end_inset

 by explicitly taking its derivatives with respect to the 
\begin_inset Formula $\theta J's$
\end_inset

, and setting them to zero.
\end_layout

\begin_layout Itemize

\series bold
No need
\series default
 to do a 
\series bold
feature scaling
\series default
!
\end_layout

\begin_layout Itemize
Calculating a value of 
\begin_inset Formula $\theta$
\end_inset

 that minimizes a cost function: 
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\theta=(X^{T}*X)^{-1}*X^{T}*y\label{eq:normal_eq_method}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where:
\end_layout

\begin_layout Itemize
\begin_inset Formula $m$
\end_inset

 is # of examples.
\end_layout

\begin_layout Itemize
\begin_inset Formula $n$
\end_inset

 is # of features.
\end_layout

\begin_layout Itemize
\begin_inset Formula $X$
\end_inset

 is a matrix of features of dimension 
\begin_inset Formula $m*(n+1)$
\end_inset

 (+1 because of 
\begin_inset Formula $x_{0}=1$
\end_inset

 and it is another dimension for calculation purposes).
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $y$
\end_inset

 is vector of output values.
\end_layout

\end_deeper
\begin_layout Itemize
Or normal equation with regularization:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\theta=(X^{T}*X+\lambda*L)^{-1}*X^{T}*y
\]

\end_inset


\end_layout

\begin_layout Standard
where:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\lambda$
\end_inset

 is a regularization parameter.
\end_layout

\begin_layout Itemize
L is a matrix with 0 at the top left and 1's down the diagonal, with 0's
 everywhere else.
 It should have dimension (n+1)×(n+1).
 So this is the identity matrix (though we are not including 
\begin_inset Formula $x_{0}$
\end_inset

 ), multiplied with a single real number λ.
\end_layout

\end_deeper
\begin_layout Itemize
By the way, what if 
\begin_inset Formula $X^{T}X$
\end_inset

 matrix is non-invertible (a.k.a.
 singular / degenerate matrix)? This should happen only rarely.
 Then we probably have:
\end_layout

\begin_deeper
\begin_layout Itemize
redundant features (linearly dependent) = delete some features
\end_layout

\begin_layout Itemize
too many features (
\begin_inset Formula $m$
\end_inset


\begin_inset Formula $\leq n)$
\end_inset

 where 
\begin_inset Formula $m$
\end_inset

 is a number of samples and 
\begin_inset Formula $n$
\end_inset

 is number of features = delete some features or use regularization)
\end_layout

\begin_layout Itemize
however, when we add the regularization term 
\begin_inset Formula $\lambda*L$
\end_inset

, then 
\begin_inset Formula $X^{T}*X+\lambda*L$
\end_inset

 becomes invertible
\end_layout

\end_deeper
\begin_layout Subsection
Dimension hopping
\end_layout

\begin_layout Itemize
This occurs when one can take the information contained in the dimensions
 of some input, and move this between dimensions while not changing the
 target.
\end_layout

\begin_layout Itemize
The canonical example is taking an image of a handwritten digit and translating
 it within the image.
 The dimensions that contain "ink" are now different (they have been moved
 to other dimensions), however the label we assign to the digit has not
 changed.
\end_layout

\begin_deeper
\begin_layout Standard
Note that this is not something that happens consistently across the dataset,
 that is we may have a dataset containing two handwritten digits where one
 is a translated version of the other, however this still does not change
 the corresponding label of the digits.
\end_layout

\end_deeper
\begin_layout Itemize
Another example - determining whether a given image shows a bike or a car.
 The bike or car might appear anywhere in the image.
 The input is the whole set of pixels for the image.
\end_layout

\end_body
\end_document
